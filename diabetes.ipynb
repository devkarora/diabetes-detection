{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2gbXiQhudio",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function  # Python 2/3 compatibility\n",
        "import math\n",
        "from IPython import display\n",
        "from matplotlib import cm\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.data import Dataset\n",
        "import os\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_auc_score, roc_curve, accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import seaborn as sns\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwXghoWRum04",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "87a74931-6192-49ea-d115-3da697daecae"
      },
      "source": [
        "\n",
        "from keras.models  import Sequential, K\n",
        "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam, SGD, RMSprop"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oovFz9cu1Gw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
        "       'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
        "diabetes_df = pd.read_csv(\"diabetes.csv\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRq659wTvGft",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "9f950fba-078f-474a-a04f-a1f0b55618cb"
      },
      "source": [
        "print(diabetes_df.head())\n",
        "print(diabetes_df.columns)\n",
        "# Take a peek at the data -- if there are lots of \"NaN\" you may have internet connectivity issues\n",
        "print(diabetes_df.shape)\n",
        "diabetes_df.sample(5)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n",
            "0            6      148             72  ...                     0.627   50        1\n",
            "1            1       85             66  ...                     0.351   31        0\n",
            "2            8      183             64  ...                     0.672   32        1\n",
            "3            1       89             66  ...                     0.167   21        0\n",
            "4            0      137             40  ...                     2.288   33        1\n",
            "\n",
            "[5 rows x 9 columns]\n",
            "Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
            "       'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'],\n",
            "      dtype='object')\n",
            "(768, 9)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>636</th>\n",
              "      <td>5</td>\n",
              "      <td>104</td>\n",
              "      <td>74</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>28.8</td>\n",
              "      <td>0.153</td>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>744</th>\n",
              "      <td>13</td>\n",
              "      <td>153</td>\n",
              "      <td>88</td>\n",
              "      <td>37</td>\n",
              "      <td>140</td>\n",
              "      <td>40.6</td>\n",
              "      <td>1.174</td>\n",
              "      <td>39</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>307</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>68</td>\n",
              "      <td>14</td>\n",
              "      <td>148</td>\n",
              "      <td>24.8</td>\n",
              "      <td>0.143</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>686</th>\n",
              "      <td>3</td>\n",
              "      <td>130</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.1</td>\n",
              "      <td>0.314</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>361</th>\n",
              "      <td>5</td>\n",
              "      <td>158</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>29.8</td>\n",
              "      <td>0.207</td>\n",
              "      <td>63</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Pregnancies  Glucose  ...  Age  Outcome\n",
              "636            5      104  ...   48        0\n",
              "744           13      153  ...   39        0\n",
              "307            0      137  ...   21        0\n",
              "686            3      130  ...   22        0\n",
              "361            5      158  ...   63        0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23Z2A8dvvtii",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f7b54f7d-43c7-4a7b-afb1-766ae61d9a2d"
      },
      "source": [
        "\n",
        "X = diabetes_df.iloc[:, :-1].values\n",
        "y = diabetes_df[\"Outcome\"].values\n",
        "\n",
        "# Split the data to Train, and Test (75%, 25%)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=11111)\n",
        "\n",
        "print(np.mean(y), np.mean(1-y))\n",
        "#35% of the patients in this dataset have diabetes, while 65% do not\n",
        "\n",
        "## Train the RF Model\n",
        "rf_model = RandomForestClassifier(n_estimators=200)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set - both \"hard\" predictions, and the scores (percent of trees voting yes)\n",
        "y_pred_class_rf = rf_model.predict(X_test)\n",
        "y_pred_prob_rf = rf_model.predict_proba(X_test)\n",
        "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_rf)))\n",
        "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_rf[:,1])))\n",
        "\n",
        "#observe\n",
        "#accuracy is 0.776\n",
        "#roc-auc is 0.830\n",
        "\n",
        "def plot_roc(y_test, y_pred, model_name):\n",
        "    fpr, tpr, thr = roc_curve(y_test, y_pred)\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    ax.plot(fpr, tpr, 'k-')\n",
        "    ax.plot([0, 1], [0, 1], 'k--', linewidth=.5)  # roc curve for random model\n",
        "    ax.grid(True)\n",
        "    ax.set(title='ROC Curve for {} on PIMA diabetes problem'.format(model_name),\n",
        "           xlim=[-0.01, 1.01], ylim=[-0.01, 1.01])\n",
        "    ax.figure\n",
        "    plt.show()\n",
        "plot_roc(y_test, y_pred_prob_rf[:, 1], 'Random Forest')\n",
        "\n",
        "## First let's normalize the data\n",
        "## This aids the training of neural nets by providing numerical stability\n",
        "## Random Forest does not need this as it finds a split only, as opposed to performing matrix multiplications\n",
        "normalizer = StandardScaler()\n",
        "X_train_norm = normalizer.fit_transform(X_train)\n",
        "X_test_norm = normalizer.transform(X_test)\n",
        "\n",
        "# Define the Model\n",
        "# Input size is 8-dimensional\n",
        "# 1 hidden layer, 12 hidden nodes, sigmoid activation\n",
        "# Final layer has just one node with a sigmoid activation (standard for binary classification)\n",
        "model_1 = Sequential([\n",
        "    Dense(12, input_shape=(8,), activation=\"relu\"),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model_1.summary()\n",
        "\n",
        "# Fit(Train) the Model\n",
        "# Compile the model with Optimizer, Loss Function and Metrics\n",
        "# Roc-Auc is not available in Keras as an off the shelf metric yet, so we will skip it here.\n",
        "model_1.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "run_hist_1 = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=200)\n",
        "# the fit function returns the run history.\n",
        "# It is very convenient, as it contains information about the model fit, iterations etc.\n",
        "## Like we did for the Random Forest, we generate two kinds of predictions\n",
        "#  One is a hard decision, the other is a probabilitistic score.\n",
        "y_pred_class_nn_1 = model_1.predict_classes(X_test_norm)\n",
        "y_pred_prob_nn_1 = model_1.predict(X_test_norm)\n",
        "\n",
        "# Let's check out the outputs to get a feel for how keras apis work.\n",
        "y_pred_class_nn_1[:10]\n",
        "y_pred_prob_nn_1[:10]\n",
        "\n",
        "# Print model performance and plot the roc curve\n",
        "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_1)))\n",
        "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_1)))\n",
        "plot_roc(y_test, y_pred_prob_nn_1, 'Neural Network')\n",
        "\n",
        "#observe\n",
        "#accuracy is 0.734\n",
        "#roc-auc is 0.789\n",
        "\n",
        "run_hist_1.history.keys()\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
        "ax.plot(run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n",
        "## Note that when we call \"fit\" again, it picks up where it left off\n",
        "run_hist_1b = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=1000)\n",
        "\n",
        "n = len(run_hist_1.history[\"loss\"])\n",
        "m = len(run_hist_1b.history['loss'])\n",
        "fig, ax = plt.subplots(figsize=(16, 8))\n",
        "ax.plot(range(n), run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss - Run 1\")\n",
        "ax.plot(range(n, n+m), run_hist_1b.history[\"loss\"], 'hotpink', marker='.', label=\"Train Loss - Run 2\")\n",
        "ax.plot(range(n), run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss - Run 1\")\n",
        "ax.plot(range(n, n+m), run_hist_1b.history[\"val_loss\"], 'LightSkyBlue', marker='.',  label=\"Validation Loss - Run 2\")\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n",
        "#model 2\n",
        "model_2 = Sequential([\n",
        "    Dense(6, input_shape=(8,), activation=\"relu\"),\n",
        "    Dense(6,  activation=\"relu\"),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model_2.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "run_hist_2 = model_2.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=1500)\n",
        "\n",
        "#Plot and observe\n",
        "n = len(run_hist_2.history[\"loss\"])\n",
        "\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "ax = fig.add_subplot(1, 2, 1)\n",
        "ax.plot(range(n), (run_hist_2.history[\"loss\"]),'r.', label=\"Train Loss\")\n",
        "ax.plot(range(n), (run_hist_2.history[\"val_loss\"]),'b.', label=\"Validation Loss\")\n",
        "ax.legend()\n",
        "ax.set_title('Loss over iterations')\n",
        "\n",
        "ax = fig.add_subplot(1, 2, 2)\n",
        "ax.plot(range(n), (run_hist_2.history[\"acc\"]),'r.', label=\"Train Acc\")\n",
        "ax.plot(range(n), (run_hist_2.history[\"val_acc\"]),'b.', label=\"Validation Acc\")\n",
        "ax.legend(loc='lower right')\n",
        "ax.set_title('Accuracy over iterations')\n",
        "plt.show()\n",
        "\n",
        "#prediction\n",
        "y_pred_class_nn_2 = model_2.predict_classes(X_test_norm)\n",
        "y_pred_prob_nn_2 = model_2.predict(X_test_norm)\n",
        "print('')\n",
        "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_2)))\n",
        "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_2)))\n",
        "plot_roc(y_test, y_pred_prob_nn_2, 'Neural Network -2')\n",
        "\n",
        "#observe\n",
        "#accuracy is 0.740\n",
        "#roc-auc is 0.817"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3489583333333333 0.6510416666666666\n",
            "accuracy is 0.771\n",
            "roc-auc is 0.834\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAHiCAYAAADSwATnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xd4VGX6xvHvSwggVYqCFMEVUBFR\nkOqqIHZwdV3UnyhiWdsqK1ICiIAgSFCUZsfGoouKiorSLBARBOlSA4bepYYACWnv748zuENIIJCZ\nvFPuz3Xlgplz5sw975wzzzznnJkx1lpEREQkdBRxHUBERESOpeIsIiISYlScRUREQoyKs4iISIhR\ncRYREQkxKs4iIiIhRsVZgsoYc4Yx5htjTLIx5jPXeU6VMWaMMWaQ6xxSOIwx1hhT2/f/t4wxffN5\nuwRjzMPBTeeWMaa/MeajE0zfYIy5rjAzRTIV5wDyrZypxpiDxpgdvhf20jnmucIYM90Yk+IrWN8Y\nY+rlmKesMWaEMWaTb1lrfZcr5XG/xhjzlDFmuTHmkDFmizHmM2PMJcF8vPl0B1AZqGitvbOgCzPG\ntDLGZPvGJcUYs9oY82DBY7pljHnAGJPle1xH/14r5AyF9kbkRM+jMaaWr0gW9ctljTG35VjGcN/1\nD+SybGuM6VmQjNbax621AwuyjPyIhsIup07FOfD+Zq0tDVwGNASeOTrBGNMC+A74GqgKnAf8Bsw2\nxvzFN08x4EfgYuAmoCzQAtgDNM3jPkcCnYGngApAXeAroO2phj/6ghhANYE11trMAGbZ5hvjskAX\n4B1jzAUFyBgq5lhrS/v9dTrVBQTh+Qsm/+exJ97zWC+PedcAHY9e8D3Ou4C1ucx7P7DXf345Md8b\nfNWDUGKt1V+A/oANwHV+l18CJvld/hl4I5fbTQHG+v7/MLATKJ3P+6wDZAFNTzBPAvCw3+UHgFl+\nly3wJPA7sB54E3g5xzK+Brr6/l8V+ALY5Zv/qTzudwCQDmQAB4F/4r0h7ANsBP4AxgLlfPPX8mX5\nJ7AJmJnLMlsBW3Jc9wdwp9/lkcBm4ACwELjKb1p/YLzvflOAFUBjv+kNgUW+aZ8CnwCD/KY/AiTh\nvfhPBKrmGMcnfOOYAgwEzgd+8WUZDxTLY6yOeU5yTCvny7vLN259gCJ+t5sNDMd7AzfId/1DwCpg\nHzANqOm73vjm/cOXaRlQH3jU9zyl+56rb/LIcgUwH0j2/XtFjvVsoC9PCt4b0Up5LCe353EX3p6W\no+tBUd/1Y4CX8baL8r7rbsHbbmYBD/gto5Tvvu/2PZbGud2/3/xxwHZgm2/MLFDb736Pjmd54Ftf\nxn2+/1fP8djjgXm+cf0aqOA3vblvPdiP94a8le/6F/C23zTfuL/mu/5C4Hu89Ww1cJffstoAK32P\ncyvQ/QTr1GzgNd/zlQhcmyPzC755UoHaeNv2RN/9JgGP5Nh2PsfbLlLwtpNLc3v9w9vOe+G9edqD\nt+5XyLGdP4i3ne4DHgeaAEt9Y/Rafl93I/XPeYBI+suxclbHe+Eb6btc0rcRXpPL7R4Etvv+/wnw\nn1O4z8eBjSeZJ4GTF+fv8bruM4CrfRuN8U0v79t4q/o2uoVAP6AY8BdgHXBjHvfdH/jI7/JDvo3+\nL0BpYALwoW/a0Y12LN6L7Bm5LK8Vvhd1X5ZbgWygod88HYCKQFGgG7ADKOGXJw3vBS4G7wV1rm9a\nMbzi1wWIxSsUGfzvBbo1sBtoBBQHXsXvDYQv+9d4neDFwBG8vSB/wSuwK4H78xinY56THNPG+pZb\nxjdGa4B/+t0uE/i37/GeAdzmG+OLfNf1AX7xzX+j7/k7E69QXwSc45s2Br83IrnkqID3Qnqfb7nt\nfZcr+q1na/H23Jzhuzwkj2XlfB5v9431BeRenAcBo4F/+a4b77v/nMX5PrxiGwN8A7x6gsdzE17B\nr4+3vo0j7+JcEWiHtx2XAT4DvsqxjW31W9YX+NZ7oBpegWrje6zX+y6flcf2WQpv+3vQN84N8da7\ner7p2/G94cTbNhudYJ3K5H/r8//hFekKfve7CW9dLeqbZybwBlACb+/fLqC137aTgbddxALd8d6c\nx+by+tcZmIv3OlgceBv4OMd2/pbvfm7A2ya/As72jdcfQMtgvE6Hy5/zAJH051s5D+K9q7R4L8xn\n+qZV9113YS63uwnI8P3/e/J4QcvjPp/FV1xOME/Ojf8Bji/Orf0uG99Ge7Xv8iPAdN//mwGbciz/\nGeCDPO67P8cW5x+BJ/wuX+Db4Iv6bbR/OcFjaYVXjPfjFb8s4OmTPP59+N7h+/L84DetHpDq+//V\neB2U8Zv+C/97gX4PeMlvWmlf9lp+4/hXv+kLgZ5+l18BRuSR8QG8F9L9fn/N8YpMOr4XZt+8jwEJ\nfrfL+XxMwVe8fZeLAIfxDjG0xivuzfF1337zjeHExfk+YF6O6+bgK46+9ayP37QngKn5eB73AkuA\nu33Tjq4HOYvzlb77OxOvqJ7B8cX5h6NjjFe8d+ErHrlkeB+/bQ3vTUWuxTmX214G7Muxjfkvq57v\neYvB22X/YY7bT8P3Ro3jt8//A37OMf/bwHO+/2/yrQNlT7LeP8Dx6/M84D6/+33eb1oNvO2pjN91\n8cAYv21nrt+0Ihz7RmED/yvOqzi2Sz+H47fzan7T9wD/53f5C06yXUf6n44xBN7frbVl8F58LgSO\nnsS1D+/F6JxcbnMO3jtj8FbS3ObJy6nOn5fNR/9jva3jE7wXN4B7gP/6/l8TqGqM2X/0D+iNd9JX\nflTF606P2oi3wfrffjMnts1aeyZehzoKr+D8yRjT3RizynfC3X68rtX/ZLodfv8/DJTwHcOsCmz1\nPX7/fLlmt9YexBv/an7z7PT7f2oul485QTCHudbaM/3+5vpyx3L8mPnfZ87xqgmM9Ht+9uK94apm\nrZ2Ot5vzdeAPY8xoY0zZE2Tyl/O5yy1LzrE90ePd5nucFay1l1lrPznRnVtrZwFn4b0h/dZam+o/\n3RhTA7iG/62rX+N1Znmde1GVY8cu52PzX3ZJY8zbxpiNxpgDeB3mmcaYGL/Zci4rFu/5qwncmWOb\nuZK8t9uaQLMc898LVPFNb4fXhW80xvzkO5clL7mtz1XzyFwV2GutTckxf67rmrU2G9iSY3n+j+FL\nv/yr8Aq//3ZekG0l4qk4B4m19if+d6wMa+0hvHf9uZ2xfBdeRwneO/8bjTGl8nlXPwLVjTGNTzDP\nIbzdcUdVyWUem+Pyx8AdxpiaeN3yF77rNwPrcxSRMtbaNvnMuw1vwz3qXLyO0X/DzJklV9baI3hd\nySXGmL8DGGOuAnrgjWl5XxFPxitOJ7MdqGaM8Z/33Lyy+56jini7M4NlN17HkXPM/O8z53htBh7L\n8RydYa39BcBaO8paezled1cX77hrbsvJKedzl1uWYPsI71DF2Fym3Yf3mvaNMWYH3uGWEngniOVm\nO163eNS5ecyH7z4vAJpZa8vi7WWBY9ernMvKwHv+NuN1zv7PRylr7RDfvLk9fz/lmL+0tfZfANba\n+dba2/B2AX+Ft4s/L7mtz9v8Lvvf9zaggjGmTI75/Z/fPx+j7wSy6jmW5/8Ybs7xGEpYawtzXQlr\nKs7BNQK43hhzqe9yL+B+38eeyhhjyvs+utIC7+QpgA/xVuwvjDEXGmOKGGMqGmN6G2OOK4DW2t/x\njhF97PsISTFjTAljzN3GmF6+2ZYA//C9+6+Nd8LVCVlrF+O9sLwLTLPW7vdNmgekGGN6Gu8zzDHG\nmPrGmCb5HJOPgS7GmPOM9zGzwcCn9jTO5vblTMfbXdzPd1UZvGK/CyhqjOmH12HnxxzfbZ8yxsQa\nY/7BsWfIfww8aIy5zBhT3Jf9V2vthtPJnh/W2iy8F98XfOtMTaArXpHKy1vAM8aYiwGMMeWMMXf6\n/t/EGNPMGBOL96YtDW+PDnhvkP5yguVOBuoaY+4xxhQ1xvwfXoH/tgAP8VSNwjtmOzOXaffjbUeX\n+f21A9oYYyrmMv944AFjTD1jTEnguRPcbxm8bm6/MaZCHvN28FvW88DnvufvI+BvxpgbfdtLCd+2\nWt13u5zj/i3eON/nWw9jfc/bRb7t+15jTDlrbQbeyWfZ5O1s/rc+34l3jsHk3Ga01m7GO4wT78vY\nAO+1wn9du9wY8w/fnqan8Q4tzc1lcW/hrbM1AYwxZ5kcH4WTE1NxDiJr7S68d/j9fJdn4Z2Q8w+8\nd+0b8U72uNJXZI92g9fhnVn5Pd7GNw9v99ivedzVU/xvV+V+vJNybsc7IQa8s3PT8V4E/sP/dvud\nzDhflnF+jykL70zZy/BOBjlawMvlc5nv470Bmem7fRreyUwF8T5wrjHmb3jH8qbiHVfd6Fv+yXaT\nA38W+n/gHavbi3fsb4Lf9B+Avnh7EbbjnYl9dwGz58e/8QrpOrxjrOPwHnOurLVfAi8Cn/h2wS4H\nbvZNLgu8g3eYZSPebvmhvmnvAfV8uyK/ymW5e/Ce+26+2/UAbrHW7s45b7BYa/daa3/MsasWY0xz\nvK7+dWvtDr+/iXgnx7XPZVlT8N5AT/fNM/0Edz0C7xj3brxiNDWXeT7E21u2A69jf8p3P5vxTtLr\njfemcTPe3oqjr78j8fZS7TPGjPLtVr4Bb93a5lvei3gnVoG3h2CD77l9HG+Xd15+xftEx268M7Pv\n8D2PeWmPd0x4G/Al3nHuH/ymf423XRw9MfAfvjcJOY3EO+v7O2NMCt6YNTvB/UoOJsc6LiIiEcB4\nX87ysLX2StdZ5NSpcxYREQkxKs4iIiIhRru1RUREQow6ZxERkRCj4iwiIhJiTvoLNsaY9/E+PvGH\ntbZ+LtMN3mnzbfC+EegBa+2iky23UqVKtlatWn9ePnToEKVK5fd7N+RUaXyDS+MbPBrb4NL4Bk/O\nsV24cOFua+1Z+bltfn5ebgzeZ2hz+0Ye8D4/Wcf31wzvF41O+nm2WrVqsWDBgj8vJyQk0KpVq3zE\nkdOh8Q0ujW/waGyDS+MbPDnH1hiT51fE5nTS3drW2pl4X8iQl9vwfu7Q+r4L+ExjTCC+61lERCQq\nBeKH2atx7DcwbfFdtz0AyxYREQmazMxMRo0axcaN+W5q8+3QoUOnvVciEMU534wxj+L9qDuVK1cm\nISHhz2kHDx485rIElsY3uDS+waOxDa5oHt/Dhw8zYMAA5s2bR6lSpTj2N0JOn7WW9PR0qlevftpj\nG4jivJVjf42lOnn8So21djTeD6bTuHFj6/+OQsc9gkvjG1wa3+DR2AZXtI7v1q1badu2LcuXL2f0\n6NE88sgjAVludnY2q1atolixYmzduvW0xzYQH6WaCHQ0nuZAsrVWu7RFRCQkLV26lGbNmrF27Vom\nTZoUsMJsreWZZ57BWkudOnUKtKz8fJTqY6AVUMkYswXvp9JifUHewvv5sTZ4v+pyGHiwQIlERESC\nZNq0adx5552ULVuWWbNmcemll578RvmQkZHB7Nmz6dWrF+XLly/w8k5anK21x/3UWo7pFniywElE\nRESC6N133+Xxxx/n4osvZtKkSVSvXv3kN8qngQMH0rFjx4AUZijkE8JERCRwVq5cyZw5cwq0jMTE\nRNauXRugRKFryZIlvPbaa9x4442MHz+esmXLBmS5R44c4YsvvuC5554jJiYmIMsEFWcRkbC0Y8cO\nmjdvTkpKiusoYeORRx7h9ddfJzY2NmDLfOONN2jXrl1ACzOoOIuIhKVnn32W1NRU5syZQ7Vq1U57\nOXPmzKFFixYBTBaaihUrRuXKlQO2vEOHDvH222/TtWvXgC3Tn4qziEiYWbRoER988AFdu3alefPm\nBVrW2rVrqVGjxslnlGN89dVX3HPPPUFbvn6VSkQkjFhr6dy5M5UqVaJv376u40Sd5ORkevbsyT33\n3EOVKlWCdj/qnEVEwshnn33GrFmzGD16NOXKlXMdJ6qkp6czb948evbsGbBvE8uLirOIRIU1a9aQ\nnJzsOkaBZGdnExcXx2WXXcZDDz3kOk5U2b17N8899xzDhw+nWLFiQb8/FWcRiXhr1qzhggsucB0j\nYMaOHRvws4Mlb3v27GHjxo3Ex8cXSmEGFWcRiQJHO+Z+/frRtGlTx2kKpkaNGjRo0MB1jKixfft2\nBg0axEsvvUSpUqUK7X5VnEUkajRt2pS2bdu6jiFhYsuWLezbt4+hQ4dSsmTJQr1vna0tIiKSw/bt\n23nppZeoU6dOoRdmUOcsIiJyjLVr15KSksLQoUMpXry4kwwqziJy2rKzs0lPTw/qfaSnp5OWllag\nZRw5ciRAaSTSHThwgDfffJP4+PiAfs3nqVJxFpHTdvXVVzN79mzXMfKtaFG95EneVq5cyc6dOxk6\ndGjQP8d8MlpTReS0JSUl0bRpU26//fag3ce6dev4y1/+UuDllCpVipYtWwYgkUSizMxMvvjiC3r3\n7u28MIOKs4gUUMOGDenVq1fQlp+QkECrVq2CtnyRRYsWsW7dupD6OlSdrS0iIlHLWsv8+fNp166d\n6yjHUOcsIiJRafbs2SxfvpzHHnvMdZTjqHMWEZGoc+jQIfbt28ejjz7qOkqu1DmLyGkpjI9RiQTD\nDz/8wIoVK+jcubPrKHlS5ywip+XDDz9k3759XHXVVa6jiOTb+vXrqVixYkgXZlBxFpHTkJKSwjPP\nPEPTpk1p37696zgi+fLtt98yZcoUGjZs6DrKSWm3toicsvj4eLZv386ECRMoUkTv8SX0zZo1iyZN\nmnDLLbe4jpIv2qpE5JSsW7eOYcOG0aFDB5o3b+46jshJTZ48maSkJCpXruw6Sr6pcxaRUxIXF0dM\nTAxDhgxxHUXkpCZMmMANN9xA6dKlXUc5JSrOIlHujz/+oE+fPuzdu/ek82ZkZDBx4kQGDhxItWrV\nCiGdyOmbOXMm6enpYVeYQcVZJKolJibSpk0btm/fzvnnn5+v29x+++1069YtyMlECua9997j9ttv\n5+qrr3Yd5bSoOItEqZkzZ/L3v/+d2NhYZs6cSZMmTVxHEgmI5cuXU6lSJSpUqOA6ymnTCWEiUWjc\nuHFcf/31VKlShblz56owS8QYOXIkJUuW5LbbbnMdpUBUnEWiiLWWF154gXvvvZcWLVowe/Zszjvv\nPNexRAJi8+bN1KtXLyA/MeqairNIlMjIyODhhx+mT58+dOjQgWnTplG+fHnXsUQKzFrLkCFD2L17\nN9dff73rOAGhY84iIe7DDz9k/fr1BV7OjBkzSEhIoG/fvgwYMCAkflBepKCstWzZsoVrrrkmLL75\nK79UnEVCWGpqKh07dgzIskqWLMl7773HQw89FJDlibhmrWXAgAG0bduWZs2auY4TUCrOIiEsOzsb\ngCFDhhAXF1fg5emrNiVSZGdns2LFCjp06EDt2rVdxwk4bakiYaBIkSIB+ROJBNZa+vTpQ3Z2dkQW\nZlDnLCIiYSQzM5OEhAR69uxJuXLlXMcJGr2VFhGRsDF48GBq1KgR0YUZ1DmLhJyDBw/y448/Yq0l\nLS3NdRyRkJCens6nn35Knz59ouIQjYqzSAix1tK2bVtmzpx5zPX6PLJEu3feeYe2bdtGRWEGFWeR\nkPLZZ58xc+ZMBg8ezM033wxA0aJFqVevnuNkIm6kpqby2muvBeTTCuFExVkkRKSmphIXF0eDBg3o\n0aMHMTExriOJOGWt5ZtvvuHee+91HaXQRcf+AZEw8Morr7Bp0yZGjhypwixRLyUlhbi4OO644w6q\nVq3qOk6hU3EWCQFbt24lPj6edu3a0apVK9dxRJxKS0tj4cKF9OrVK2qOMeek3doihWT79u2kpqbm\nOu3ZZ58lKyuLoUOHFnIqkdCyd+9e+vTpw7BhwyhRooTrOM6oOIsUgpkzZ9KyZcsTztO7d2/9fKNE\ntT179rBp0ybi4+OjujCDirNIoZgwYQIlSpTgrbfeyvXXoEqXLs2tt97qIJlIaNi5cyfPP/88Q4YM\noUyZMq7jOKfiLFIIJk+ezDXXXMP999/vOopIyNm2bRu7d+/mpZdeolSpUq7jhIToPNIuUoiSkpL4\n/fffadOmjesoIiFn165dDBkyhDp16qgw+1HnLBJkU6ZMAfjzS0VExLNhwwb27NnD0KFDKV68uOs4\nIUWds0iQTZ48mbp163L++ee7jiISMg4fPsyrr77KJZdcosKcC3XOIkF0+PBhZsyYwb/+9S/XUURC\nxurVq9mwYQMvv/xyridIijpnkaBKSEjgyJEjOt4s4pOVlcXnn3/Otddeq8J8AuqcRYJo8uTJlCxZ\nkquvvtp1FBHnfvvtN5YvX86zzz7rOkrIU+csEiTWWiZNmsS1116rY2oS9bKzs5k/fz7t27d3HSUs\nqHMWCZKjx9V69uzpOoqIU3PnzmX+/Pn8+9//dh0lbKhzFgkSfYRKxPt1qX379tGpUyfXUcKKOmeR\nAvj3v//NggULOHDgAGXLlj1mWlJSEvXq1aNmzZqO0om4lZCQwIIFC+jevbvrKGFHxVmkAD744AMq\nVarEWWeddVxxbtSoEQ899JCjZCJuJSUlUaFCBRXm06TiLFJAd9xxB7fccot+h1nEZ+rUqaxZs4an\nnnrKdZSwpeIsIiIBM3PmTBo1asRNN93kOkpY0wlhIiISEN999x2rV6/m7LPPdh0l7KlzFhGRApsw\nYQLXXXcdN9xwg+soEUHFWaLC/v37GTBgAIcPHw7octPS0gK6PJFw9Ouvv5KamnrcSZFy+lScJSrM\nmjWLESNGULFiRWJjYwO23MqVK9OkSZOALU8k3HzwwQe0adOGZs2auY4SUVScJSpYawGYNm0al19+\necCXn5CQEPBlioS633//nbJly1K5cmXXUSKOTggTEZFT9vrrr5OVlUW7du1cR4lIKs4iInJKduzY\nQe3atbnwwgtdR4lYKs4iIpIv1lpefvllNm3axI033ug6TkRTcRYRkZOy1rJ161auvPJKmjZt6jpO\nxFNxFhGRE7LWMmjQIDZv3kzz5s1dx4kKOltbRETyZK1l2bJl3HPPPZx//vmu40QNdc4iIpKn/v37\nk5mZqcJcyNQ5i4jIcbKysvjhhx/o3r07ZcqUcR0n6qhzFhGR47z00kvUqFFDhdkRdc4SsbZv305S\nUhIAK1ascJxGJDxkZGTw0Ucf0bNnT4oUUf/mioqzRKw2bdqwZMmSY64rXbq0ozQi4WHMmDG0bt1a\nhdkxFWeJWCkpKbRu3ZrevXsDcOaZZ3LBBRc4TiUSmtLS0njllVfo3bs3xhjXcaJevoqzMeYmYCQQ\nA7xrrR2SY/q5wH+AM33z9LLWTg5wVpFTds4553Dttde6jiES0qy1TJkyhfvvv1+FOUScdL+FMSYG\neB24GagHtDfG1MsxWx9gvLW2IXA38Eagg4qISOClpqbStWtX/va3v1G9enXXccQnPwcVmgJJ1tp1\n1tp04BPgthzzWODor2yXA7YFLqKIiARDamoqSUlJPPPMMxQtqqOcoSQ/z0Y1YLPf5S1Azl/V7g98\nZ4z5N1AKuC63BRljHgUeBe9H6v1/A/fgwYP6TdwgisbxTU1NZefOnYXyuKNxfAuLxjY4Dh48yDvv\nvEOHDh1YuXIlK1eudB0p4hRk3Q3UW6X2wBhr7SvGmBbAh8aY+tbabP+ZrLWjgdEAjRs3tq1atfpz\nWkJCAv6XJbCicXzPOOMMKleuXCiPOxrHt7BobANv7969bN68mTFjxvDbb79pfIOkIOtufnZrbwVq\n+F2u7rvO3z+B8QDW2jlACaDSaSUSEZGg2b17N3379qVWrVqUL1/edRzJQ36K83ygjjHmPGNMMbwT\nvibmmGcTcC2AMeYivOK8K5BBRUSkYHbs2MHWrVsZMmQI5cqVcx1HTuCkxdlamwl0AqYBq/DOyl5h\njHneGHOrb7ZuwCPGmN+Aj4EHrLU2WKFFROTU7Nu3j4EDB1K7dm19JWcYyNcxZ99nlifnuK6f3/9X\nAn8NbDQREQmETZs2sW3bNoYNG0bx4sVdx5F80PeziYhEsCNHjjBy5EgaNmyowhxG9ME2cW7Xrl20\nbt2a5OTkgC5327ZtNG/ePKDLFAknv//+O6tXr+bll1/WN3+FGRVncW7Dhg0sX76c6667jho1apz8\nBqfgwQcfDOjyRMKFtZbPP/+cuLg4FeYwpOIsIePpp5+mbdu2rmOIhL3ly5ezYMECnnnmGddR5DTp\nmLOISATJzs5mwYIFdOzY0XUUKQB1ziIiEWLBggXMnDmTrl27uo4iBaTOWUQkAiQnJ7N37166dOni\nOooEgDpnKRQbN26kV69epKSkHDdt//79DhKJRI6ff/6Z2bNn06tXL9dRJEBUnKVQPPnkk0yfPp16\n9XL+FLjn6quvpkGDBoWcSiT8rV69mgoVKtCzZ0/XUSSAVJwl6KZOncqkSZMYOnQo3bt3dx1HJGL8\n8MMPLF26VMeYI5CKswRVRkYGXbp0oXbt2jz11FOu44hEjJkzZ9KgQQOuu+4611EkCHRCmATVm2++\nSWJiIsOGDaNYsWKu44hEhISEBFauXMnZZ5/tOooEiTpnCZrdu3fz3HPPcf3113PLLbe4jiMSEb78\n8ktatWpFq1atXEeRIFLnLEHz3HPPkZKSwvDhw/X1gSIBsGTJEg4cOED58uVdR5EgU3GWoFi+fDlv\nvfUW//rXv7j44otdxxEJex9++CEVK1bk/vvvdx1FCoGKswSctZann36acuXK0b9/f9dxRMLepk2b\nKF68eMB/GEZCl4qzBNzEiRP58ccfGTBgABUrVnQdRySsvf322+zbt4+77rrLdRQpRCrOElBHjhyh\nW7du1KtXj8cff9x1HJGwtmvXLs4991wuvfRS11GkkOlsbQmoUaNGsXbtWqZNm0ZsbKzrOCJha/jw\n4TRp0oSbb77ZdRRxQMVZAmbbtm0MHDiQW265hRtuuMF1HJGwZK1l69atXHHFFTRr1sx1HHFEu7Wl\nwDIzM3njjTe45JJLSE9P55VXXnEdSSQsWWuJj49n/fr1KsxRTsVZCmT69Ok0bNiQJ598kksvvZT5\n8+dTt25d17FEwo61liVLltC+fXuuuuoq13HEMRVnOS3r16+nXbt2XHvttRw6dIgJEybw448/cskl\nl7iOJhKWBg0aRGZmJuedd55ZWl2AAAAgAElEQVTrKBICdMxZTsnBgweJj4/nlVdeoWjRorzwwgt0\n7dqVEiVKuI4mEpays7OZPHkyXbt2pVSpUq7jSIhQ5yz5kp2dzYcffkjdunUZPHgwd911F6tXr6Z3\n794qzCIFMGzYMGrWrKnCLMdQ5ywnNX/+fJ566inmzp1L06ZNmTBhAs2bN3cdSySsZWZm8sEHH9Ct\nWzd997wcR8VZTig5OZlWrVpRtmxZ/vOf/9ChQweKFNEOF5GC+uijj2jZsqUKs+RKxVlO6MCBAxw+\nfJiRI0fSsWNH13FEwt6RI0d48cUX6du3rwqz5EktkOSLXkRECs5ayw8//MD999+vbUpOSMVZRKQQ\nHD58mC5dunD99ddTs2ZN13EkxKk4i4gEWWpqKsuWLaNXr14UK1bMdRwJAyrOIiJBdODAAbp3786F\nF15IlSpVXMeRMKETwkREgmTfvn1s2rSJ559/nnLlyrmOI2FEnbOISBDs3buXPn36ULNmTSpWrOg6\njoQZdc4iIgG2a9cutm7dSnx8PGXLlnUdR8KQOmcRkQBKSUlhwIAB1K5dW4VZTps6ZxGRANm6dSvr\n169n2LBhOitbCkSds4hIAGRmZjJy5EgaN26swiwFps5ZTig9Pd11BJGQt27dOn777Tdeeukl11Ek\nQqhzlhN6/fXXMcbQtGlT11FEQpK1li+++IJbbrnFdRSJIOqcJU+rV6/m1Vdf5eGHH+aSSy5xHUck\n5KxatYqff/6ZuLg411Ekwqhzljx169aNkiVLMmjQINdRREJOVlYWCxcu5J///KfrKBKB1DlLrqZO\nncqkSZN4+eWXOfvss13HEQkpixcv5rvvvqNnz56uo0iEUucsx8nIyKBLly7UqVOHf//7367jiISU\nffv2sW/fPu3KlqBS5xwl9u7dy2OPPUZaWtpJ592xYweJiYlMnDhRHwkR8fPLL78wffp0+vTp4zqK\nRDgV5yixePFiRo8eTdWqVfNVcJ944gmdfSriZ9WqVZQvX55nn33WdRSJAirOUWb69OlccMEFrmOI\nhJWffvqJefPm0b17d4wxruNIFFBxFhE5gZ9++okLL7yQli1buo4iUUQnhImI5OGXX35h2bJlVK5c\n2XUUiTLqnEVEcvH1119zxRVXcMUVV7iOIlFInXOUsNa6jiASNlauXMnu3bs566yzXEeRKKXiHAXW\nrVvH2LFjKVGiBJUqVXIdRySk/fe//6V48eL65i9xSsU5wv366680b96c/fv3891331GxYkXXkURC\n1o4dOyhSpAjnn3++6ygS5VScI9iXX37JNddcQ5kyZXjttde46qqrXEcSCVnvvvsumzdvpn379q6j\niKg4R6oRI0bQrl07GjRowJw5czj33HNdRxIJWXv37uWcc86hSZMmrqOIACrOEScrK4vOnTvTpUsX\nbr/9dmbMmKEfrhA5gVGjRvHbb7/Rtm1b11FE/qSPUoWZgwcPMnfu3DzPvn7ttdeYOHEiXbt25aWX\nXiImJqaQE4qEjy1bttCsWTOaNWvmOorIMVScw8ygQYN48cUX85xepEgRXn31VTp16lSIqUTCz5Ah\nQ2jWrBnXXHON6ygix1FxDjMHDx6kbNmyTJ48OdfpVapU0ZmmIidgrWXhwoXcc889OhdDQpaKcxiK\njY3lr3/9q+sYImHpxRdfpGXLlirMEtJUnEUkKmRnZ/PNN9/QuXNnzjjjDNdxRE5IZ2uLSFR4/fXX\nqVmzpgqzhAV1ziIS0bKysnjnnXfo1KmTfotZwoY6ZxGJaJ9++imtWrVSYZawos5ZRCJSeno6gwcP\npl+/fhQpoj5EwovWWBGJONnZ2fz000/cf//9KswSlrTWikhESU1NpUuXLlx55ZWcd955ruOInBbt\n1haRiHH48GFWrVpFjx49dFa2hDV1ziISEVJSUoiLi6NWrVpUq1bNdRyRAlHnHAI2bdrENddcw8GD\nB08674EDByhdunQhpBIJH8nJyWzYsIH+/ftTsWJF13FECkzFOQSsXbuWdevWceutt1K1atWTzt+4\nceNCSCUSHvbv30/v3r0ZNGgQFSpUcB1HJCBUnENI165dadmypesYImFj9+7dbNq0ifj4eMqVK+c6\njkjA6JiziISl1NRU+vfvT506dVSYJeKocxaRsLN9+3ZWrVrF8OHDiY2NdR1HJODUOYtIWMnOzmbE\niBE0b95chVkiljrnQpKYmEiPHj04fPjwcdP27t3rIJFI+NmwYQNz587lxRdfdB1FJKjy1TkbY24y\nxqw2xiQZY3rlMc9dxpiVxpgVxphxgY0Z3qy1PPzww/z000+kpaUd91eyZEluuukmLr74YtdRRULa\nhAkT+Mc//uE6hkjQnbRzNsbEAK8D1wNbgPnGmInW2pV+89QBngH+aq3dZ4w5O1iBw9H48eOZPXs2\no0eP5pFHHnEdRyTsrF69mu+//56uXbu6jiJSKPLTOTcFkqy166y16cAnwG055nkEeN1auw/AWvtH\nYGOGr8OHDxMXF8dll13GQw895DqOSNjJyspi0aJFPP74466jiBSa/BTnasBmv8tbfNf5qwvUNcbM\nNsbMNcbcFKiA4e7ll19m8+bNjBw5kpiYGNdxRMLK0qVLGTduHO3bt6doUZ0iI9EjUGt7UaAO0Aqo\nDsw0xlxird3vP5Mx5lHgUYDKlSuTkJDw57SDBw8eczkS/PHHHwwePJiWLVuSnZ3t9PFF4viGEo1v\n4CUnJ7N+/Xpuu+02jW0Qad0NnoKMbX6K81aght/l6r7r/G0BfrXWZgDrjTFr8Ir1fP+ZrLWjgdEA\njRs3tq1atfpzWkJCAv6XI8G9994LwJgxY6hVq5bTLJE4vqFE4xtY8+bNY8aMGQwYMEBjG2Qa3+Ap\nyNjmZ7f2fKCOMeY8Y0wx4G5gYo55vsLrmjHGVMLbzb3utBJFiH379vHpp5/yxBNPOC/MIuFkxYoV\nlCtXjv79+7uOIuLMSYuztTYT6ARMA1YB4621K4wxzxtjbvXNNg3YY4xZCcwA4qy1e4IVOhx8//33\nZGVlcccdd7iOIhI2Zs+ezcSJE6lbty7GGNdxRJzJ1zFna+1kYHKO6/r5/d8CXX1/AkyePJny5cvT\nrFkz11FEwsLMmTOpW7cuV1xxhQqzRD19fWcQZGdnM3XqVG688UadoS2SDwsWLGDRokVUqVJFhVkE\nFeegWLx4MTt37qRNmzauo4iEvG+++YaqVavy9NNPu44iEjJUnINg8uTJGGO48cYbXUcRCWlr165l\n+/btVK1a1XUUkZCi4hwEU6ZMoUmTJpx9tr7FVCQvn376KUeOHOHRRx91HUUk5Kg4B9ju3buZO3cu\nN998s+soIiFrz549ZGZmUq9ePddRREKSvg8vwL777justTreLJKHMWPGULt27T+/pEdEjqfOOcCm\nTJnCWWedRePGjV1HEQk5ycnJnHXWWVx55ZWuo4iENHXOAZSVlcXUqVO5+eabKVJE73tE/L3xxhvU\nrl2btm3buo4iEvJUnANowYIF7N69W8ebRXLYvHkzTZo0oUmTJq6jiIQFtXcBNHbsWGJiYrjhhhtc\nRxEJGa+88gqJiYkqzCKnQJ1zgKxYsYK3336bRx99lIoVK7qOI+KctZZ58+Zx9913U61azp+AF5ET\nUeccANZaunTpQpkyZXj++eddxxEJCcOGDSMzM1OFWeQ0qHMOgG+//Zbvv/+eESNGUKlSJddxRJyy\n1vLll1/y5JNPUqJECddxRMKSOucCSk9Pp1u3blx44YU88cQTruOIODd69Ghq1qypwixSAOqcC+jV\nV1/l999/Z8qUKcTGxrqOI+JMVlYWb7zxBp06ddIvS4kUkDrnApg5cyb9+/enTZs23HTTTa7jiDg1\nYcIEWrdurcIsEgAqzqdp3LhxXH/99VSvXp233nrLdRwRZzIyMujbty+33347F198ses4IhFBxfkU\nWWt54YUXuPfee2nRogW//PILNWrUcB1LxIns7Gxmz57N/fffT9GiOkomEigqzqcgIyODRx55hD59\n+nDvvfcybdo0ypcv7zqWiBNpaWl06dKFyy+/nNq1a7uOIxJRVJzz6cCBA7Rt25b33nuPPn368OGH\nH1K8eHHXsUScSE1NJTExke7du1OmTBnXcUQijopzPmzevJkrr7ySGTNm8N577zFw4ECd9CJR69Ch\nQ8TFxVG1alUd0hEJEh0kysWIESMYPXr0n5e3bduGtZbJkydz/fXXO0wm4lZKSgrr16+nb9++nH32\n2a7jiEQsdc65mDp1Kjt27KB+/frUr1+fW2+9lVmzZqkwS1RLSUmhV69eVK1alcqVK7uOIxLR1Dnn\noW7duowfP951DJGQsHfvXtatW8fgwYMpV66c6zgiEU+ds4icUHp6Ov369aNOnToqzCKFRJ2ziORp\n586dLFmyhBEjRuhzzCKFSJ2ziOTKWsuoUaO48sorVZhFCpm2OBE5zubNm0lISOCFF15wHUUkKqlz\nFpHjfPXVV9x5552uY4hELXXOIvKntWvXMnHiRLp06eI6ikhUU+csIoD33fGLFi2iU6dOrqOIRD11\nziLCihUrGD9+PAMGDHAdRURQ5ywS9f744w/2799Pv379XEcRER8VZ5EotnDhQkaNGsUVV1xBTEyM\n6zgi4qPiLBKlli9fTpkyZfQrayIhSMVZJArNmzePr776ijp16qgwi4QgFWeRKPPzzz9TvXp1nn32\nWRVmkRCl4iwSRZYuXcq8efOoWrWqCrNICFNxFokSkydPply5cnTr1s11FBE5CRVnkSiwefNmNmzY\nQM2aNV1HEZF8UHEWiXCff/45e/bs4YknnnAdRUTyScVZJIIlJyeTmprKZZdd5jqKiJwCfX2nSIT6\n8MMPqVatGvfdd5/rKCJyitQ5i0SgAwcOULFiRVq3bu06ioicBnXOIhHm7bffpnr16rRt29Z1FBE5\nTSrOPjt37iQtLQ2A1NRUx2lETs/GjRtp3Lgxl19+uesoIlIAKs7A9OnTufbaa4+57uqrr3aURuT0\njBw5krp163LzzTe7jiIiBaTiDLz55ptUqlSJl1566c/rmjdv7jCRSP5Za/nll1+46667OOecc1zH\nEZEAiPrivHv3br7++ms6derEgw8+6DqOyCkbNWoUl112mQqzSASJ+uL83//+l4yMDB566CHXUURO\nibWWzz77jMcff5zixYu7jiMiARTVH6Wy1vLee+/RpEkT6tev7zqOyCn54IMPqFmzpgqzSASK6s55\n4cKFLFu2jDfffNN1FJF8y87OZtSoUXTu3Fm/LCUSoaK6c37//fcpUaIEd999t+soIvn27bff0rp1\naxVmkQgWtcU5NTWVcePGcccdd3DmmWe6jiNyUpmZmfTt25cbb7yRBg0auI4jIkEUtcV5woQJJCcn\n60QwCQtZWVnMmzeP++67T8eYRaJA1Bbn999/n/POO4+WLVu6jiJyQunp6XTv3p2LLrqIunXruo4j\nIoUgKovz+vXrmT59Og8++CBFikTlEEiYSEtLIzExkaeffpry5cu7jiMihSQqK9MHH3yAMYYHHnjA\ndRSRPB0+fJi4uDjOOussatas6TqOiBSiqPsoVVZWFmPGjOGGG26gRo0aruOI5OrQoUOsXbuW3r17\n65u/RKJQ1HXOP/74I5s3b9aJYBKyDh06RI8ePahSpYoKs0iUirrO+f3336dChQrcdtttrqOIHGf/\n/v2sXr2awYMHU65cOddxRMSRqOqc9+zZw5dffkmHDh30cRQJOZmZmfTr14+6deuqMItEuajqnMeN\nG0d6erp2aUvI2bVrF7/++ivDhw8nJibGdRwRcSyqOuf333+fRo0acemll7qOIvInay2vvfYarVq1\nUmEWESCKOufFixezZMkSXnvtNddRRP60detWpk2bxoABA1xHEZEQEjWd88KFCwFo06aN4yQiHmst\nEydOpH379q6jiEiIiZrO+ajY2FjXEURYv349n376Kb169XIdRURCUNR0ziKh4siRIyxZsoSuXbu6\njiIiIUrFWaQQrVq1igEDBnD77bdTrFgx13FEJESpOIsUkh07dpCcnMzAgQNdRxGREKfiLFIIlixZ\nwsiRI2natKk+LiUiJ6XiLBJky5cvp1SpUrzwwgv6iVIRyRe9UogE0aJFi/j888+pXbu2CrOI5Jte\nLUSCZPbs2VSqVInnnnsOY4zrOCISRlScRYIgMTGRWbNmUaNGDRVmETllKs4iAfbdd99RpEgRevbs\nqcIsIqclX8XZGHOTMWa1MSbJGJPnVxoZY9oZY6wxpnHgIoqEj507d5KYmEjdunVdRxGRMHbS4myM\niQFeB24G6gHtjTH1cpmvDNAZ+DXQIUXCwVdffcWGDRt46qmnXEcRkTCXn865KZBkrV1nrU0HPgFu\ny2W+gcCLQFoA84mEhdTUVA4cOECzZs1cRxGRCJCf4lwN2Ox3eYvvuj8ZYxoBNay1kwKYTSQsfPzx\nxyxbtoyOHTu6jiIiEaLAv0pljCkCDAMeyMe8jwKPAlSuXJmEhIQ/px08ePCYy4G2evVqAObMmcNZ\nZ50VtPsJVcEe32h16NAhNm7cSP369TW+QaJ1N7g0vsFTkLHNT3HeCtTwu1zdd91RZYD6QILvzNQq\nwERjzK3W2gX+C7LWjgZGAzRu3Ni2atXqz2kJCQn4Xw60pKQkAFq0aEH16tWDdj+hKtjjG43ef/99\nKlSoQK9evTS+QaSxDS6Nb/AUZGzzU5znA3WMMefhFeW7gXuOTrTWJgOVjl42xiQA3XMWZheysrJI\nTU0FIC1Nh8IlcNatW0ejRo247LLLXEcRkQh00uJsrc00xnQCpgExwPvW2hXGmOeBBdbaicEOeTpS\nUlJo2rQpiYmJx1xftGiB9+RLlHv99dc599xz+dvf/uY6iohEqHxVKmvtZGByjuv65TFvq4LHKrjB\ngweTmJhIv379KFOmDABVqlShSpUqjpNJOPv555+58847Ofvss11HEZEIFpFt5Nq1axk2bBgdO3Zk\nwIABruNIhHjzzTe54IILVJhFJOgisjjHxcURGxtLfHy86ygSAay1fPLJJzz88MPExsa6jiMiUSDi\nvlt7+vTpfPnll/Tu3ZuqVau6jiMRYNy4cdSqVUuFWUQKTUR1zpmZmTz99NPUqlWLrl27uo4jYS47\nO5sRI0bQuXNnYmJiXMcRkSgS1sU5MzOT6667jk2bNgGQnp7O1q1b+eyzzyhRooTjdBLuvvvuO665\n5hoVZhEpdGFdnFNSUvjpp59o3LgxF110EQD169enXbt2jpNJOMvKyuK5556jd+/elCxZ0nUcEYlC\nYV2cj+rQoQOdO3d2HUMiQFZWFosWLeLee+9VYRYRZyLuhDCR05WRkUFcXBw1a9b8c0+MiIgLEdE5\nixTUkSNH+P333+nUqZM+xywizqlzlqiXlpZGXFwcZ555Jn/5y19cxxERCf3OOTk5mR49erBz587j\npqWnpztIJJHk8OHDJCUl0atXL30uXkRCRsgX5wEDBvDOO+/QoEGDXKc3btyYFi1aFHIqiQRpaWn0\n6NGDPn366DvXRSSkhHRxXr16Na+++ioPP/wwo0ePdh1HIsiBAwdYtmwZgwcPpmzZsq7jiIgcI6SP\nOXfr1o2SJUsyaNAg11EkgmRnZ9O3b18uvPBCFWYRCUkh2zlPnTqVSZMmMXToUJ09KwGzZ88eZs6c\nyfDhwylSJKTfm4pIFAvJV6eMjAy6dOlCnTp1eOqpp1zHkQjyxhtvcO2116owi0hIC8nO+c033yQx\nMZGJEydSrFgx13EkAuzYsYOvv/6avn37uo4iInJSIdk+jBkzhhYtWnDLLbe4jiIRwFrLN998w333\n3ec6iohIvoRccU5PT2f58uVcffXVGGNcx5Ewt3HjRgYNGsQjjzyi78oWkbARcsV5xYoVZGRk0LBh\nQ9dRJMylpaWxdOlSevTo4TqKiMgpCbnivHjxYgAVZymQNWvW0K9fP2655RaKFy/uOo6IyCkJyeJc\nunRpateu7TqKhKlt27aRnJzM4MGDdWhERMJSSBbnSy+9VB91kdOybNkyRo4cSaNGjShaNCQ/jCAi\nclIhVQGzsrJYsmQJjRo1ch1FwtDy5cspUaIE8fHxxMTEuI4jInLaQqo4JyUlcejQIR1vllO2fPly\nxo8fz/nnn6+9LiIS9kLqVUwng8npmDNnDqVKlWLAgAEqzCISEULqlWzx4sXExsZSr14911EkTKxb\nt44ZM2ZQq1YtnfwlIhEj5Ipz/fr19ZWdki8//vgjhw8f5plnnlFhFpGIEjLF2VrL4sWLtUtb8mXv\n3r0sX76c+vXrqzCLSMQJmc+a7N69m927d6s4y0l9++23lCtXjs6dO7uOIiISFCHTOa9ZswbQyWBy\nYmlpaezdu5errrrKdRQRkaAJmc45KSkJYwyXXnqp6ygSosaPH0+JEiXo2LGj6ygiIkEVMsX5999/\np27dupQuXdp1FAlBBw4coGzZstx0002uo4iIBF3IFOekpCSuueYa1zEkBP3nP/+hZMmS3Hnnna6j\niIgUipAoznv27GHnzp063izH+f3332nUqBGXXHKJ6ygiIoUmJE4IW7p0KQCXXXaZ4yQSSt5++21W\nrlypwiwiUSckOudDhw4BUKFCBcdJJFTMmDGDdu3aUalSJddRREQKXUh0ziL+3n33XTIyMlSYRSRq\nhUTnLALet8R99NFHPPDAA/otZhGJauqcJWR8/vnn1KpVS4VZRKKeXgXFOWstw4YN46mnniI2NtZ1\nHBER59Q5i3MzZsygZcuWKswiIj4qzuJMdnY2ffr0oXHjxjRu3Nh1HBGRkKHd2uJEVlYWy5Yt4+67\n76Zs2bKu44iIhBR1zlLoMjIy6NmzJ2eddRb169d3HUdEJOSoc5ZClZ6eTlJSEo899hjVqlVzHUdE\nJCSpc5ZCc+TIEXr06EHJkiWpU6eO6zgiIiFLnbMUitTUVNasWUNcXJw6ZhGRk1DnLEGXkZFBXFwc\nlSpVUmEWEckHdc4SVCkpKSxatIj4+HjKlCnjOo6ISFhQ5yxBY62lf//+1KtXT4VZROQUqHOWoNi3\nbx/ff/89Q4cOpUgRvQcUETkVetWUoBg9ejQ33HCDCrOIyGlQ5ywB9ccffzB+/Hh69uzpOoqISNhS\nWyMBY61l0qRJPPjgg66jiIiENXXOEhBbtmxh9OjRPP/8866jiIiEPXXOUmCpqaksX76c3r17u44i\nIhIRVJylQNauXcuzzz7LjTfeSIkSJVzHERGJCCrOctq2bNlCcnIyL774IsYY13FERCKGirOcllWr\nVjFq1CgaNGhAbGys6zgiIhFFxVlO2YoVKyhatCjx8fEULapzCkVEAk3FWU5JYmIi48aN4/zzzycm\nJsZ1HBGRiKTiLPk2b948YmJiGDRokL75S0QkiPQKK/myZcsWpk6dSu3atXXyl4hIkOmAoZzUTz/9\nRJkyZejbt68Ks4hIIVDnLCeUkpLC4sWLadiwoQqziEghUecseZoyZQqxsbE8/fTTrqOIiEQVdc6S\nq/T0dHbt2sV1113nOoqISNRR5yzHmTBhAtnZ2XTs2NF1FBGRqKTiLMdITk6mdOnS3HDDDa6jiIhE\nLRVn+dNHH31EkSJFuOeee1xHERGJairOAnjf/NWoUSPq1avnOoqISNTTCWHCe++9x4oVK1SYRURC\nhDrnKPfjjz9y++23U6FCBddRRETER51zFBs7dixHjhxRYRYRCTHqnKPU2LFjueeee/STjyIiIUid\ncxSaOHEi5557rgqziEiIyldxNsbcZIxZbYxJMsb0ymV6V2PMSmPMUmPMj8aYmoGPKgVlreWVV17h\nxhtvpFWrVq7jiIhIHk5anI0xMcDrwM1APaC9MSbnab2LgcbW2gbA58BLgQ4qBTd79myuvPJKihcv\n7jqKiIicQH4656ZAkrV2nbU2HfgEuM1/BmvtDGvtYd/FuUD1wMaUgsjOzub999/noosuolmzZq7j\niIjISeTnoGM1YLPf5S3AiV7h/wlMyW2CMeZR4FGAypUrk5CQAMCyZcsAWLhwIQcPHsxHJMmvrKws\nNm3aRJMmTf4cZwm8gwcP/rk+S2BpbINL4xs8BRnbgJ4RZIzpADQGWuY23Vo7GhgN0LhxY3v0uOfR\ngnz55ZfTuHHjQEaKapmZmfTu3Zsnn3yS9evX6zhzECUkJGh8g0RjG1wa3+ApyNjmZ7f2VqCG3+Xq\nvuuOYYy5DngWuNVae+S00kjAZGRkkJSUxD//+U9q1tT5eSIi4SQ/xXk+UMcYc54xphhwNzDRfwZj\nTEPgbbzC/EfgY8qpSE9Pp0ePHsTGxnLBBRe4jiMiIqfopLu1rbWZxphOwDQgBnjfWrvCGPM8sMBa\nOxEYCpQGPjPGAGyy1t4axNySh7S0NBITE+nevTvVqlVzHUdERE5Dvo45W2snA5NzXNfP7//XBTiX\nnIasrCx69OhBXFycCrOISBjTV0RFiEOHDjF37lzi4+MpVaqU6zgiIlIA+vrOCPH8889Tv359FWYR\nkQigzjnM7d+/n0mTJjFkyBB8x/tFRCTMqXMOc++99x4333yzCrOISARR5xymdu/ezdixY+nWrZvr\nKCIiEmDqnMOQtZapU6fyyCOPuI4iIiJBoOIcZrZt20bv3r3p0KEDZcqUcR1HRESCQMU5jBw6dIiV\nK1fSr1+/k88sIiJhS8U5TGzYsIHevXvTunVrzjjjDNdxREQkiFScw8CWLVvYv38/Q4cOpUgRPWUi\nIpFOr/Qhbs2aNQwfPpyLL76YYsWKuY4jIiKFQMU5hK1cuRKAF198kdjYWMdpRESksKg4h6i1a9cy\nduxYzj//fIoW1cfRRUSiiYpzCFq4cCFHjhxh8ODBxMTEuI4jIiKFTMU5xPzxxx988803XHTRRTr5\nS0QkSml/aQiZNWsWRYsWpX///q6jiIiIQ2rNQkRqairz58+nWbNmrqOIiIhj6pxDwPfff096ejpd\nunRxHUVEREKAOmfHMmoCoH0AAAn7SURBVDIy2LlzJ23btnUdRUREQoQ6Z4cmTpzIwYMH6dChg+so\nIiISQlScHdm3bx+lSpXi1ltvdR1FRERCjIqzA5988gnp6el07NjRdRQREQlBKs6FbMWKFTRs2JAL\nLrjAdRQREQlROiGsEI0dO5YVK1aoMIuIyAmpcy4k3333HbfddhvlypVzHUVEREKcOudC8Mknn3Dk\nyBEVZhERyRd1zkE2ZswY7r33Xv3ko4iI5Js65yCaOnUq1atXV2EWEZFTos45CKy1vPLKK/zrX/+i\nVKlSruOIiEiYUeccYNZa5s+fT4sWLVSYRUTktKg4B1B2djbPPfcc5557Ln/9619dxxERkTCl4hwg\n2dnZrFmzhr///e9UqVLFdRwREQljKs4BkJWVxTPPPEPRokVp1KiR6zgiIhLmdEJYAWVmZrJ27Voe\nfPBBateu7TqOiIhEAHXOBZCRkUGPHj0wxnDhhRe6jiMiIhFCnfNpOnLkCCtWrKBbt25Uq1bNdRwR\nEYkg6pxPQ3Z2Nj179qRixYoqzCIiEnDqnE/R4cOHmTlzJvHx8Zxxxhmu44iISARS53yKXnjhBS69\n9FIVZhERCRp1zvl04MABvvzySwYNGoQxxnUcERGJYOqc8+mDDz6gbdu2KswiIhJ06pxPYu/evbz7\n7rv06NHDdRQREYkS6pxPIDs7m++//57HHnvMdRQREYkiKs552LFjBz179uSuu+6iXLlyruOIiEgU\nUXHORUpKComJifTv31/HmEVEpNCpOOewadMmevfuzZVXXqnfYxYRESdUnP1s3ryZ/fv38/LLL1O0\nqM6VExERN1ScfdauXcvw4cO58MILKV68uOs4IiISxdQeAomJiQC8+OKLxMbGOk4jIiLRLuo7502b\nNvHBBx9Qp04dFWYREQkJUd05L1myhCJFihAfH0+RIlH/PkVEREJE1Fak/fv38+WXX1K/fn0VZhER\nCSlR2TnPnTuX9PR0BgwY4DqKiIjIcaKuZUxPT2fOnDlcddVVrqOIiIjkKqo65+nTp7N//366dOni\nOoqIiEieoqZzzsjIYPv27fzjH/9wHUVEROSEoqJznjRpErt27eKBBx5wHUVEROSkIr447969m1Kl\nStG2bVvXUURERPIloovzZ599RkpKCg899JDrKCIiIvkWscV56dKlNGzYkNq1a7uOIiIickoi8oSw\njz/+mGXLlqkwi4hIWIq4znnKlCm0bduWsmXLuo4iIiJyWiKqOH/xxRcUKVJEhVlERMJaxBTnMWPG\n0L59e/0Ws4iIhL2IOOY8ffp0qlSposIsIiIRIaw7Z2stw4YN4+GHH6ZcuXKu44iIiARE2HbO1lqW\nLl1KkyZNVJhFRCSihGVxttYycOBAypcvz9VXX+06joiISED9f3v3F6JVncdx/P3pr0T/NE3EMosK\nHNoLZYy6KaOI9GK8KKNg6A9SMLFd7MqisIpL4UVGBUFQhkMl9P8iBioCq0GIlIQwKih0+rPWgu5u\nK4zi7vz57sU5xDCoz28ePf+e5/OCA+c8z3kOXz4cznd+55w5p3GntScnJxkZGWHlypUsWrSo6nLM\nzMzOuEaNnCcnJ9m4cSNjY2MsX7686nLMzMwK0ZiR88TEBAcOHKC/v58lS5ZUXY6ZmVlhGjFyHh8f\nZ/369UxMTNDT01N1OWZmZoWq/ch5bGyMffv2sW7dOhYsWFB1OWZmZoWr9cg5ItiwYQNz5sxxYzYz\ns65R25Hz8ePH2blzJ1u2bGHWrFlVl2NmZlaa2o6ct27dytKlS92Yzcys6yQ1Z0l3SfpO0n5JG07w\n/fmS3sq/3yNpcbsFjY6Osn37djZt2sTChQvb3YyZmVljtWzOks4GXgBWAj3A/ZKm3zK9FvgtIq4F\nngOearegHTt20NfXh6R2N2FmZtZoKSPnG4H9ETESEf8D3gRWT1tnNfBqPv8ucLva6K6Dg4MMDAww\nb968mf7UzMysY6Q054XA36csH8w/O+E6ETEOHAEum2kxa9asmelPzMzMOk6pd2tLehR4FGD+/PkM\nDw8D2f8yb968maNHj/7+mZ1Zo6OjzrZAzrc4zrZYzrc4p5NtSnP+BbhyyvIV+WcnWuegpHOAS4B/\nTd9QRGwDtgH09vbGihUrfv9u9uzZTF22M2t4eNj5Fsj5FsfZFsv5Fud0sk05rf0FcJ2kqyWdB9wH\nDE1bZwh4MJ+/B/gkIqKtiszMzLpcy5FzRIxL+iPwEXA2MBgR30h6AtgbEUPAdmCHpP3Av8kauJmZ\nmbVBVQ1wJR0Gfpry0Vzgn5UU0x2cb7Gcb3GcbbGcb3GmZ3tVRCT9O1JlzXk6SXsjorfqOjqV8y2W\n8y2Osy2W8y3O6WRb28d3mpmZdSs3ZzMzs5qpU3PeVnUBHc75Fsv5FsfZFsv5FqftbGtzzdnMzMwy\ndRo5m5mZGRU05zJfP9mNEvL9s6RvJX0l6WNJV1VRZxO1ynbKendLCkm+A3YGUvKVdG++/34j6fWy\na2yqhOPCIkmfSvoyPzasqqLOJpI0KOmQpK9P8r0kPZ9n/5WkZUkbjojSJrKHmBwArgHOA/YBPdPW\neQx4MZ+/D3irzBqbPCXmextwQT4/4HzPXLb5ehcBu4DdQG/VdTdlStx3rwO+BGbny5dXXXcTpsRs\ntwED+XwP8GPVdTdlAm4BlgFfn+T7VcCHgICbgD0p2y175Fza6ye7VMt8I+LTiDiWL+4me1a6tZay\n7wI8SfY+8+NlFtcBUvJ9BHghIn4DiIhDJdfYVCnZBnBxPn8J8GuJ9TVaROwiezLmyawGXovMbuBS\nSQtabbfs5lza6ye7VEq+U60l+4vOWmuZbX666sqIeL/MwjpEyr57PXC9pM8k7ZZ0V2nVNVtKtn8D\n+iUdBD4AHi+ntK4w0+MyUPIrI60+JPUDvcCtVdfSCSSdBTwLPFRxKZ3sHLJT2yvIzvjskvSHiPhP\npVV1hvuBVyLiGUk3k70r4YaImKy6sG5V9sh5Jq+f5FSvn7QTSskXSXcAfwX6IuK/JdXWdK2yvQi4\nARiW9CPZtaUh3xSWLGXfPQgMRcRYRPwAfE/WrO3UUrJdC7wNEBGfA7PIngttpy/puDxd2c3Zr58s\nVst8JS0FXiJrzL5ml+6U2UbEkYiYGxGLI2Ix2fX8vojYW025jZNybHiPbNSMpLlkp7lHyiyyoVKy\n/Rm4HUDSErLmfLjUKjvXEPBAftf2TcCRiPhHqx+Velo7/PrJQiXm+zRwIfBOfp/dzxHRV1nRDZGY\nrbUpMd+PgDslfQtMAH+JCJ9VayEx23XAy5L+RHZz2EMeFKWR9AbZH41z82v2m4FzASLiRbJr+KuA\n/cAx4OGk7Tp/MzOzevETwszMzGrGzdnMzKxm3JzNzMxqxs3ZzMysZtyczczMasbN2czMrGbcnM3M\nzGrGzdnMzKxm/g9/7o7L09gPEwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 12)                108       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 13        \n",
            "=================================================================\n",
            "Total params: 121\n",
            "Trainable params: 121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "Train on 576 samples, validate on 192 samples\n",
            "Epoch 1/200\n",
            "576/576 [==============================] - 1s 1ms/step - loss: 0.7252 - acc: 0.5972 - val_loss: 0.7614 - val_acc: 0.5677\n",
            "Epoch 2/200\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.7211 - acc: 0.6024 - val_loss: 0.7565 - val_acc: 0.5729\n",
            "Epoch 3/200\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.7170 - acc: 0.6024 - val_loss: 0.7517 - val_acc: 0.5781\n",
            "Epoch 4/200\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.7131 - acc: 0.6059 - val_loss: 0.7470 - val_acc: 0.5781\n",
            "Epoch 5/200\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.7093 - acc: 0.6076 - val_loss: 0.7426 - val_acc: 0.5833\n",
            "Epoch 6/200\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.7056 - acc: 0.6094 - val_loss: 0.7382 - val_acc: 0.5885\n",
            "Epoch 7/200\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.7020 - acc: 0.6163 - val_loss: 0.7339 - val_acc: 0.5833\n",
            "Epoch 8/200\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.6985 - acc: 0.6198 - val_loss: 0.7297 - val_acc: 0.5885\n",
            "Epoch 9/200\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.6951 - acc: 0.6285 - val_loss: 0.7257 - val_acc: 0.5990\n",
            "Epoch 10/200\n",
            "576/576 [==============================] - 0s 63us/step - loss: 0.6917 - acc: 0.6337 - val_loss: 0.7217 - val_acc: 0.6094\n",
            "Epoch 11/200\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.6884 - acc: 0.6441 - val_loss: 0.7178 - val_acc: 0.6094\n",
            "Epoch 12/200\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.6852 - acc: 0.6476 - val_loss: 0.7141 - val_acc: 0.6146\n",
            "Epoch 13/200\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.6821 - acc: 0.6510 - val_loss: 0.7104 - val_acc: 0.6198\n",
            "Epoch 14/200\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.6791 - acc: 0.6510 - val_loss: 0.7068 - val_acc: 0.6146\n",
            "Epoch 15/200\n",
            "576/576 [==============================] - 0s 65us/step - loss: 0.6761 - acc: 0.6562 - val_loss: 0.7032 - val_acc: 0.6250\n",
            "Epoch 16/200\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.6732 - acc: 0.6562 - val_loss: 0.6997 - val_acc: 0.6406\n",
            "Epoch 17/200\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.6703 - acc: 0.6632 - val_loss: 0.6963 - val_acc: 0.6406\n",
            "Epoch 18/200\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.6675 - acc: 0.6615 - val_loss: 0.6929 - val_acc: 0.6302\n",
            "Epoch 19/200\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.6647 - acc: 0.6632 - val_loss: 0.6896 - val_acc: 0.6354\n",
            "Epoch 20/200\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.6619 - acc: 0.6632 - val_loss: 0.6865 - val_acc: 0.6406\n",
            "Epoch 21/200\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.6593 - acc: 0.6684 - val_loss: 0.6833 - val_acc: 0.6458\n",
            "Epoch 22/200\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.6566 - acc: 0.6701 - val_loss: 0.6803 - val_acc: 0.6458\n",
            "Epoch 23/200\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.6541 - acc: 0.6667 - val_loss: 0.6772 - val_acc: 0.6510\n",
            "Epoch 24/200\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.6515 - acc: 0.6719 - val_loss: 0.6743 - val_acc: 0.6615\n",
            "Epoch 25/200\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.6490 - acc: 0.6684 - val_loss: 0.6714 - val_acc: 0.6667\n",
            "Epoch 26/200\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.6465 - acc: 0.6667 - val_loss: 0.6685 - val_acc: 0.6667\n",
            "Epoch 27/200\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.6441 - acc: 0.6667 - val_loss: 0.6657 - val_acc: 0.6823\n",
            "Epoch 28/200\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.6417 - acc: 0.6719 - val_loss: 0.6630 - val_acc: 0.6823\n",
            "Epoch 29/200\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.6393 - acc: 0.6719 - val_loss: 0.6603 - val_acc: 0.6875\n",
            "Epoch 30/200\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.6370 - acc: 0.6736 - val_loss: 0.6576 - val_acc: 0.6875\n",
            "Epoch 31/200\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.6347 - acc: 0.6736 - val_loss: 0.6551 - val_acc: 0.6979\n",
            "Epoch 32/200\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.6325 - acc: 0.6719 - val_loss: 0.6525 - val_acc: 0.6979\n",
            "Epoch 33/200\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.6302 - acc: 0.6719 - val_loss: 0.6501 - val_acc: 0.6979\n",
            "Epoch 34/200\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.6280 - acc: 0.6701 - val_loss: 0.6476 - val_acc: 0.6979\n",
            "Epoch 35/200\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.6259 - acc: 0.6701 - val_loss: 0.6452 - val_acc: 0.6979\n",
            "Epoch 36/200\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.6237 - acc: 0.6736 - val_loss: 0.6429 - val_acc: 0.6875\n",
            "Epoch 37/200\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.6217 - acc: 0.6736 - val_loss: 0.6406 - val_acc: 0.6875\n",
            "Epoch 38/200\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.6196 - acc: 0.6788 - val_loss: 0.6383 - val_acc: 0.6875\n",
            "Epoch 39/200\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.6176 - acc: 0.6788 - val_loss: 0.6361 - val_acc: 0.6875\n",
            "Epoch 40/200\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.6156 - acc: 0.6806 - val_loss: 0.6339 - val_acc: 0.6927\n",
            "Epoch 41/200\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.6136 - acc: 0.6806 - val_loss: 0.6317 - val_acc: 0.6979\n",
            "Epoch 42/200\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.6116 - acc: 0.6806 - val_loss: 0.6296 - val_acc: 0.6979\n",
            "Epoch 43/200\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.6097 - acc: 0.6806 - val_loss: 0.6274 - val_acc: 0.6979\n",
            "Epoch 44/200\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.6078 - acc: 0.6823 - val_loss: 0.6253 - val_acc: 0.6979\n",
            "Epoch 45/200\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.6059 - acc: 0.6823 - val_loss: 0.6233 - val_acc: 0.7031\n",
            "Epoch 46/200\n",
            "576/576 [==============================] - 0s 60us/step - loss: 0.6041 - acc: 0.6858 - val_loss: 0.6212 - val_acc: 0.6979\n",
            "Epoch 47/200\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.6023 - acc: 0.6858 - val_loss: 0.6192 - val_acc: 0.7031\n",
            "Epoch 48/200\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.6005 - acc: 0.6840 - val_loss: 0.6172 - val_acc: 0.7031\n",
            "Epoch 49/200\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.5987 - acc: 0.6823 - val_loss: 0.6152 - val_acc: 0.7031\n",
            "Epoch 50/200\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.5969 - acc: 0.6823 - val_loss: 0.6133 - val_acc: 0.7083\n",
            "Epoch 51/200\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.5952 - acc: 0.6858 - val_loss: 0.6114 - val_acc: 0.7083\n",
            "Epoch 52/200\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.5934 - acc: 0.6875 - val_loss: 0.6095 - val_acc: 0.7031\n",
            "Epoch 53/200\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.5918 - acc: 0.6875 - val_loss: 0.6077 - val_acc: 0.7031\n",
            "Epoch 54/200\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.5901 - acc: 0.6910 - val_loss: 0.6058 - val_acc: 0.7031\n",
            "Epoch 55/200\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.5884 - acc: 0.6927 - val_loss: 0.6040 - val_acc: 0.7135\n",
            "Epoch 56/200\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.5867 - acc: 0.6910 - val_loss: 0.6023 - val_acc: 0.7083\n",
            "Epoch 57/200\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.5851 - acc: 0.6979 - val_loss: 0.6005 - val_acc: 0.7083\n",
            "Epoch 58/200\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.5835 - acc: 0.6979 - val_loss: 0.5988 - val_acc: 0.7188\n",
            "Epoch 59/200\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.5819 - acc: 0.7014 - val_loss: 0.5971 - val_acc: 0.7188\n",
            "Epoch 60/200\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.5803 - acc: 0.6997 - val_loss: 0.5955 - val_acc: 0.7188\n",
            "Epoch 61/200\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.5788 - acc: 0.7031 - val_loss: 0.5938 - val_acc: 0.7240\n",
            "Epoch 62/200\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.5772 - acc: 0.7066 - val_loss: 0.5922 - val_acc: 0.7240\n",
            "Epoch 63/200\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.5757 - acc: 0.7066 - val_loss: 0.5906 - val_acc: 0.7240\n",
            "Epoch 64/200\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.5741 - acc: 0.7066 - val_loss: 0.5890 - val_acc: 0.7188\n",
            "Epoch 65/200\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.5726 - acc: 0.7083 - val_loss: 0.5874 - val_acc: 0.7188\n",
            "Epoch 66/200\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.5711 - acc: 0.7118 - val_loss: 0.5859 - val_acc: 0.7188\n",
            "Epoch 67/200\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.5697 - acc: 0.7153 - val_loss: 0.5844 - val_acc: 0.7188\n",
            "Epoch 68/200\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.5682 - acc: 0.7188 - val_loss: 0.5829 - val_acc: 0.7188\n",
            "Epoch 69/200\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.5668 - acc: 0.7188 - val_loss: 0.5814 - val_acc: 0.7188\n",
            "Epoch 70/200\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.5654 - acc: 0.7170 - val_loss: 0.5799 - val_acc: 0.7188\n",
            "Epoch 71/200\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.5640 - acc: 0.7205 - val_loss: 0.5785 - val_acc: 0.7188\n",
            "Epoch 72/200\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.5626 - acc: 0.7222 - val_loss: 0.5771 - val_acc: 0.7188\n",
            "Epoch 73/200\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.5612 - acc: 0.7205 - val_loss: 0.5756 - val_acc: 0.7240\n",
            "Epoch 74/200\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.5599 - acc: 0.7222 - val_loss: 0.5743 - val_acc: 0.7240\n",
            "Epoch 75/200\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.5586 - acc: 0.7240 - val_loss: 0.5729 - val_acc: 0.7240\n",
            "Epoch 76/200\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.5573 - acc: 0.7257 - val_loss: 0.5716 - val_acc: 0.7240\n",
            "Epoch 77/200\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.5560 - acc: 0.7292 - val_loss: 0.5702 - val_acc: 0.7240\n",
            "Epoch 78/200\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.5547 - acc: 0.7292 - val_loss: 0.5689 - val_acc: 0.7240\n",
            "Epoch 79/200\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.5535 - acc: 0.7309 - val_loss: 0.5677 - val_acc: 0.7188\n",
            "Epoch 80/200\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.5522 - acc: 0.7309 - val_loss: 0.5664 - val_acc: 0.7188\n",
            "Epoch 81/200\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.5510 - acc: 0.7309 - val_loss: 0.5652 - val_acc: 0.7188\n",
            "Epoch 82/200\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.5498 - acc: 0.7309 - val_loss: 0.5640 - val_acc: 0.7188\n",
            "Epoch 83/200\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.5486 - acc: 0.7309 - val_loss: 0.5628 - val_acc: 0.7188\n",
            "Epoch 84/200\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.5474 - acc: 0.7326 - val_loss: 0.5616 - val_acc: 0.7188\n",
            "Epoch 85/200\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.5462 - acc: 0.7344 - val_loss: 0.5605 - val_acc: 0.7188\n",
            "Epoch 86/200\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.5451 - acc: 0.7361 - val_loss: 0.5593 - val_acc: 0.7188\n",
            "Epoch 87/200\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.5439 - acc: 0.7344 - val_loss: 0.5582 - val_acc: 0.7188\n",
            "Epoch 88/200\n",
            "576/576 [==============================] - 0s 70us/step - loss: 0.5428 - acc: 0.7344 - val_loss: 0.5571 - val_acc: 0.7188\n",
            "Epoch 89/200\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.5416 - acc: 0.7378 - val_loss: 0.5560 - val_acc: 0.7188\n",
            "Epoch 90/200\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.5405 - acc: 0.7378 - val_loss: 0.5549 - val_acc: 0.7188\n",
            "Epoch 91/200\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.5394 - acc: 0.7378 - val_loss: 0.5539 - val_acc: 0.7188\n",
            "Epoch 92/200\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.5383 - acc: 0.7378 - val_loss: 0.5529 - val_acc: 0.7188\n",
            "Epoch 93/200\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.5373 - acc: 0.7396 - val_loss: 0.5519 - val_acc: 0.7188\n",
            "Epoch 94/200\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.5362 - acc: 0.7396 - val_loss: 0.5509 - val_acc: 0.7188\n",
            "Epoch 95/200\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.5352 - acc: 0.7378 - val_loss: 0.5499 - val_acc: 0.7240\n",
            "Epoch 96/200\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.5342 - acc: 0.7378 - val_loss: 0.5489 - val_acc: 0.7240\n",
            "Epoch 97/200\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.5332 - acc: 0.7413 - val_loss: 0.5480 - val_acc: 0.7240\n",
            "Epoch 98/200\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.5322 - acc: 0.7431 - val_loss: 0.5471 - val_acc: 0.7240\n",
            "Epoch 99/200\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.5313 - acc: 0.7413 - val_loss: 0.5461 - val_acc: 0.7240\n",
            "Epoch 100/200\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.5303 - acc: 0.7413 - val_loss: 0.5452 - val_acc: 0.7240\n",
            "Epoch 101/200\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.5293 - acc: 0.7413 - val_loss: 0.5444 - val_acc: 0.7240\n",
            "Epoch 102/200\n",
            "576/576 [==============================] - 0s 59us/step - loss: 0.5284 - acc: 0.7413 - val_loss: 0.5435 - val_acc: 0.7240\n",
            "Epoch 103/200\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.5275 - acc: 0.7413 - val_loss: 0.5426 - val_acc: 0.7188\n",
            "Epoch 104/200\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.5266 - acc: 0.7413 - val_loss: 0.5418 - val_acc: 0.7188\n",
            "Epoch 105/200\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.5257 - acc: 0.7413 - val_loss: 0.5410 - val_acc: 0.7188\n",
            "Epoch 106/200\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.5248 - acc: 0.7448 - val_loss: 0.5402 - val_acc: 0.7188\n",
            "Epoch 107/200\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.5239 - acc: 0.7465 - val_loss: 0.5394 - val_acc: 0.7188\n",
            "Epoch 108/200\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.5230 - acc: 0.7448 - val_loss: 0.5386 - val_acc: 0.7188\n",
            "Epoch 109/200\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.5222 - acc: 0.7465 - val_loss: 0.5378 - val_acc: 0.7188\n",
            "Epoch 110/200\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.5213 - acc: 0.7465 - val_loss: 0.5371 - val_acc: 0.7188\n",
            "Epoch 111/200\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.5205 - acc: 0.7465 - val_loss: 0.5363 - val_acc: 0.7135\n",
            "Epoch 112/200\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.5197 - acc: 0.7431 - val_loss: 0.5356 - val_acc: 0.7135\n",
            "Epoch 113/200\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.5189 - acc: 0.7483 - val_loss: 0.5349 - val_acc: 0.7135\n",
            "Epoch 114/200\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.5181 - acc: 0.7500 - val_loss: 0.5342 - val_acc: 0.7135\n",
            "Epoch 115/200\n",
            "576/576 [==============================] - 0s 59us/step - loss: 0.5173 - acc: 0.7483 - val_loss: 0.5335 - val_acc: 0.7188\n",
            "Epoch 116/200\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.5165 - acc: 0.7483 - val_loss: 0.5328 - val_acc: 0.7188\n",
            "Epoch 117/200\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.5157 - acc: 0.7465 - val_loss: 0.5322 - val_acc: 0.7188\n",
            "Epoch 118/200\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.5149 - acc: 0.7517 - val_loss: 0.5315 - val_acc: 0.7188\n",
            "Epoch 119/200\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.5142 - acc: 0.7535 - val_loss: 0.5309 - val_acc: 0.7188\n",
            "Epoch 120/200\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.5134 - acc: 0.7500 - val_loss: 0.5303 - val_acc: 0.7188\n",
            "Epoch 121/200\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.5127 - acc: 0.7500 - val_loss: 0.5296 - val_acc: 0.7240\n",
            "Epoch 122/200\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.5119 - acc: 0.7500 - val_loss: 0.5290 - val_acc: 0.7240\n",
            "Epoch 123/200\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.5112 - acc: 0.7517 - val_loss: 0.5284 - val_acc: 0.7240\n",
            "Epoch 124/200\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.5105 - acc: 0.7500 - val_loss: 0.5278 - val_acc: 0.7240\n",
            "Epoch 125/200\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.5098 - acc: 0.7483 - val_loss: 0.5272 - val_acc: 0.7240\n",
            "Epoch 126/200\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.5091 - acc: 0.7517 - val_loss: 0.5267 - val_acc: 0.7240\n",
            "Epoch 127/200\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.5084 - acc: 0.7500 - val_loss: 0.5261 - val_acc: 0.7240\n",
            "Epoch 128/200\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.5077 - acc: 0.7500 - val_loss: 0.5255 - val_acc: 0.7240\n",
            "Epoch 129/200\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.5070 - acc: 0.7517 - val_loss: 0.5250 - val_acc: 0.7240\n",
            "Epoch 130/200\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.5064 - acc: 0.7500 - val_loss: 0.5245 - val_acc: 0.7240\n",
            "Epoch 131/200\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.5057 - acc: 0.7500 - val_loss: 0.5240 - val_acc: 0.7240\n",
            "Epoch 132/200\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.5051 - acc: 0.7517 - val_loss: 0.5235 - val_acc: 0.7240\n",
            "Epoch 133/200\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.5045 - acc: 0.7500 - val_loss: 0.5230 - val_acc: 0.7240\n",
            "Epoch 134/200\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.5038 - acc: 0.7517 - val_loss: 0.5225 - val_acc: 0.7240\n",
            "Epoch 135/200\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.5033 - acc: 0.7500 - val_loss: 0.5220 - val_acc: 0.7240\n",
            "Epoch 136/200\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.5027 - acc: 0.7500 - val_loss: 0.5215 - val_acc: 0.7240\n",
            "Epoch 137/200\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.5021 - acc: 0.7517 - val_loss: 0.5211 - val_acc: 0.7240\n",
            "Epoch 138/200\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.5015 - acc: 0.7517 - val_loss: 0.5206 - val_acc: 0.7240\n",
            "Epoch 139/200\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.5009 - acc: 0.7517 - val_loss: 0.5202 - val_acc: 0.7188\n",
            "Epoch 140/200\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.5003 - acc: 0.7500 - val_loss: 0.5197 - val_acc: 0.7188\n",
            "Epoch 141/200\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4997 - acc: 0.7500 - val_loss: 0.5193 - val_acc: 0.7188\n",
            "Epoch 142/200\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4991 - acc: 0.7500 - val_loss: 0.5189 - val_acc: 0.7188\n",
            "Epoch 143/200\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4986 - acc: 0.7500 - val_loss: 0.5184 - val_acc: 0.7188\n",
            "Epoch 144/200\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4980 - acc: 0.7500 - val_loss: 0.5180 - val_acc: 0.7188\n",
            "Epoch 145/200\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4975 - acc: 0.7517 - val_loss: 0.5176 - val_acc: 0.7188\n",
            "Epoch 146/200\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4970 - acc: 0.7535 - val_loss: 0.5172 - val_acc: 0.7240\n",
            "Epoch 147/200\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4964 - acc: 0.7517 - val_loss: 0.5168 - val_acc: 0.7240\n",
            "Epoch 148/200\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4959 - acc: 0.7517 - val_loss: 0.5164 - val_acc: 0.7240\n",
            "Epoch 149/200\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4953 - acc: 0.7517 - val_loss: 0.5160 - val_acc: 0.7240\n",
            "Epoch 150/200\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4948 - acc: 0.7535 - val_loss: 0.5157 - val_acc: 0.7240\n",
            "Epoch 151/200\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4943 - acc: 0.7535 - val_loss: 0.5153 - val_acc: 0.7240\n",
            "Epoch 152/200\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4938 - acc: 0.7552 - val_loss: 0.5149 - val_acc: 0.7240\n",
            "Epoch 153/200\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4933 - acc: 0.7587 - val_loss: 0.5146 - val_acc: 0.7240\n",
            "Epoch 154/200\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4928 - acc: 0.7604 - val_loss: 0.5142 - val_acc: 0.7240\n",
            "Epoch 155/200\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4923 - acc: 0.7622 - val_loss: 0.5139 - val_acc: 0.7240\n",
            "Epoch 156/200\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4919 - acc: 0.7587 - val_loss: 0.5136 - val_acc: 0.7240\n",
            "Epoch 157/200\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4914 - acc: 0.7622 - val_loss: 0.5133 - val_acc: 0.7240\n",
            "Epoch 158/200\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4909 - acc: 0.7604 - val_loss: 0.5129 - val_acc: 0.7344\n",
            "Epoch 159/200\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4904 - acc: 0.7622 - val_loss: 0.5126 - val_acc: 0.7344\n",
            "Epoch 160/200\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4900 - acc: 0.7604 - val_loss: 0.5123 - val_acc: 0.7344\n",
            "Epoch 161/200\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4895 - acc: 0.7622 - val_loss: 0.5120 - val_acc: 0.7396\n",
            "Epoch 162/200\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4891 - acc: 0.7604 - val_loss: 0.5117 - val_acc: 0.7396\n",
            "Epoch 163/200\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4886 - acc: 0.7622 - val_loss: 0.5114 - val_acc: 0.7396\n",
            "Epoch 164/200\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4882 - acc: 0.7604 - val_loss: 0.5111 - val_acc: 0.7396\n",
            "Epoch 165/200\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4877 - acc: 0.7622 - val_loss: 0.5108 - val_acc: 0.7396\n",
            "Epoch 166/200\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4873 - acc: 0.7622 - val_loss: 0.5105 - val_acc: 0.7396\n",
            "Epoch 167/200\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4869 - acc: 0.7622 - val_loss: 0.5103 - val_acc: 0.7396\n",
            "Epoch 168/200\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4864 - acc: 0.7622 - val_loss: 0.5100 - val_acc: 0.7396\n",
            "Epoch 169/200\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4860 - acc: 0.7622 - val_loss: 0.5097 - val_acc: 0.7396\n",
            "Epoch 170/200\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4856 - acc: 0.7622 - val_loss: 0.5095 - val_acc: 0.7396\n",
            "Epoch 171/200\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4852 - acc: 0.7639 - val_loss: 0.5092 - val_acc: 0.7396\n",
            "Epoch 172/200\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4848 - acc: 0.7639 - val_loss: 0.5089 - val_acc: 0.7396\n",
            "Epoch 173/200\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4844 - acc: 0.7639 - val_loss: 0.5087 - val_acc: 0.7448\n",
            "Epoch 174/200\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4840 - acc: 0.7639 - val_loss: 0.5084 - val_acc: 0.7448\n",
            "Epoch 175/200\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4836 - acc: 0.7639 - val_loss: 0.5082 - val_acc: 0.7448\n",
            "Epoch 176/200\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4832 - acc: 0.7639 - val_loss: 0.5079 - val_acc: 0.7448\n",
            "Epoch 177/200\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4828 - acc: 0.7622 - val_loss: 0.5077 - val_acc: 0.7448\n",
            "Epoch 178/200\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4824 - acc: 0.7622 - val_loss: 0.5075 - val_acc: 0.7448\n",
            "Epoch 179/200\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4821 - acc: 0.7587 - val_loss: 0.5073 - val_acc: 0.7448\n",
            "Epoch 180/200\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4817 - acc: 0.7604 - val_loss: 0.5070 - val_acc: 0.7448\n",
            "Epoch 181/200\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4813 - acc: 0.7604 - val_loss: 0.5068 - val_acc: 0.7448\n",
            "Epoch 182/200\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4809 - acc: 0.7604 - val_loss: 0.5066 - val_acc: 0.7448\n",
            "Epoch 183/200\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4806 - acc: 0.7604 - val_loss: 0.5064 - val_acc: 0.7448\n",
            "Epoch 184/200\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4802 - acc: 0.7622 - val_loss: 0.5062 - val_acc: 0.7448\n",
            "Epoch 185/200\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4798 - acc: 0.7622 - val_loss: 0.5060 - val_acc: 0.7448\n",
            "Epoch 186/200\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4795 - acc: 0.7622 - val_loss: 0.5058 - val_acc: 0.7448\n",
            "Epoch 187/200\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4791 - acc: 0.7622 - val_loss: 0.5056 - val_acc: 0.7448\n",
            "Epoch 188/200\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4788 - acc: 0.7639 - val_loss: 0.5054 - val_acc: 0.7448\n",
            "Epoch 189/200\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4784 - acc: 0.7639 - val_loss: 0.5052 - val_acc: 0.7448\n",
            "Epoch 190/200\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4781 - acc: 0.7639 - val_loss: 0.5050 - val_acc: 0.7500\n",
            "Epoch 191/200\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4778 - acc: 0.7639 - val_loss: 0.5048 - val_acc: 0.7500\n",
            "Epoch 192/200\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4774 - acc: 0.7622 - val_loss: 0.5046 - val_acc: 0.7500\n",
            "Epoch 193/200\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4771 - acc: 0.7656 - val_loss: 0.5044 - val_acc: 0.7500\n",
            "Epoch 194/200\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4768 - acc: 0.7639 - val_loss: 0.5043 - val_acc: 0.7500\n",
            "Epoch 195/200\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4764 - acc: 0.7639 - val_loss: 0.5041 - val_acc: 0.7500\n",
            "Epoch 196/200\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4761 - acc: 0.7656 - val_loss: 0.5039 - val_acc: 0.7500\n",
            "Epoch 197/200\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4758 - acc: 0.7656 - val_loss: 0.5037 - val_acc: 0.7500\n",
            "Epoch 198/200\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4755 - acc: 0.7656 - val_loss: 0.5036 - val_acc: 0.7500\n",
            "Epoch 199/200\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4752 - acc: 0.7656 - val_loss: 0.5034 - val_acc: 0.7500\n",
            "Epoch 200/200\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4749 - acc: 0.7656 - val_loss: 0.5032 - val_acc: 0.7500\n",
            "accuracy is 0.750\n",
            "roc-auc is 0.811\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAHiCAYAAADSwATnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREQkzqg4i4iIxJn/D17EE6iSj/grAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 576 samples, validate on 192 samples\n",
            "Epoch 1/1000\n",
            "576/576 [==============================] - 0s 61us/step - loss: 0.4746 - acc: 0.7656 - val_loss: 0.5031 - val_acc: 0.7552\n",
            "Epoch 2/1000\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4743 - acc: 0.7656 - val_loss: 0.5029 - val_acc: 0.7552\n",
            "Epoch 3/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4739 - acc: 0.7656 - val_loss: 0.5028 - val_acc: 0.7552\n",
            "Epoch 4/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4737 - acc: 0.7656 - val_loss: 0.5026 - val_acc: 0.7604\n",
            "Epoch 5/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4734 - acc: 0.7656 - val_loss: 0.5025 - val_acc: 0.7604\n",
            "Epoch 6/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4731 - acc: 0.7656 - val_loss: 0.5024 - val_acc: 0.7604\n",
            "Epoch 7/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4728 - acc: 0.7656 - val_loss: 0.5022 - val_acc: 0.7604\n",
            "Epoch 8/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4725 - acc: 0.7674 - val_loss: 0.5021 - val_acc: 0.7604\n",
            "Epoch 9/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4722 - acc: 0.7674 - val_loss: 0.5020 - val_acc: 0.7604\n",
            "Epoch 10/1000\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4720 - acc: 0.7674 - val_loss: 0.5018 - val_acc: 0.7604\n",
            "Epoch 11/1000\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4716 - acc: 0.7674 - val_loss: 0.5017 - val_acc: 0.7604\n",
            "Epoch 12/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4714 - acc: 0.7674 - val_loss: 0.5016 - val_acc: 0.7604\n",
            "Epoch 13/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4711 - acc: 0.7674 - val_loss: 0.5015 - val_acc: 0.7604\n",
            "Epoch 14/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4708 - acc: 0.7674 - val_loss: 0.5013 - val_acc: 0.7604\n",
            "Epoch 15/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4706 - acc: 0.7674 - val_loss: 0.5012 - val_acc: 0.7604\n",
            "Epoch 16/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4703 - acc: 0.7674 - val_loss: 0.5011 - val_acc: 0.7604\n",
            "Epoch 17/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4701 - acc: 0.7674 - val_loss: 0.5010 - val_acc: 0.7604\n",
            "Epoch 18/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4698 - acc: 0.7691 - val_loss: 0.5009 - val_acc: 0.7604\n",
            "Epoch 19/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4696 - acc: 0.7691 - val_loss: 0.5008 - val_acc: 0.7604\n",
            "Epoch 20/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4693 - acc: 0.7691 - val_loss: 0.5006 - val_acc: 0.7604\n",
            "Epoch 21/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4691 - acc: 0.7726 - val_loss: 0.5005 - val_acc: 0.7604\n",
            "Epoch 22/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4688 - acc: 0.7726 - val_loss: 0.5004 - val_acc: 0.7604\n",
            "Epoch 23/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4686 - acc: 0.7726 - val_loss: 0.5003 - val_acc: 0.7604\n",
            "Epoch 24/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4684 - acc: 0.7726 - val_loss: 0.5002 - val_acc: 0.7604\n",
            "Epoch 25/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4681 - acc: 0.7726 - val_loss: 0.5001 - val_acc: 0.7604\n",
            "Epoch 26/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4679 - acc: 0.7726 - val_loss: 0.5000 - val_acc: 0.7604\n",
            "Epoch 27/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4676 - acc: 0.7726 - val_loss: 0.4999 - val_acc: 0.7604\n",
            "Epoch 28/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4674 - acc: 0.7726 - val_loss: 0.4998 - val_acc: 0.7604\n",
            "Epoch 29/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4672 - acc: 0.7726 - val_loss: 0.4996 - val_acc: 0.7604\n",
            "Epoch 30/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4670 - acc: 0.7726 - val_loss: 0.4995 - val_acc: 0.7604\n",
            "Epoch 31/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4668 - acc: 0.7726 - val_loss: 0.4994 - val_acc: 0.7604\n",
            "Epoch 32/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4665 - acc: 0.7726 - val_loss: 0.4993 - val_acc: 0.7604\n",
            "Epoch 33/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4663 - acc: 0.7726 - val_loss: 0.4992 - val_acc: 0.7604\n",
            "Epoch 34/1000\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4661 - acc: 0.7726 - val_loss: 0.4991 - val_acc: 0.7604\n",
            "Epoch 35/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4659 - acc: 0.7726 - val_loss: 0.4990 - val_acc: 0.7604\n",
            "Epoch 36/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4657 - acc: 0.7743 - val_loss: 0.4990 - val_acc: 0.7604\n",
            "Epoch 37/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4656 - acc: 0.7743 - val_loss: 0.4989 - val_acc: 0.7604\n",
            "Epoch 38/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4653 - acc: 0.7760 - val_loss: 0.4988 - val_acc: 0.7604\n",
            "Epoch 39/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4652 - acc: 0.7760 - val_loss: 0.4987 - val_acc: 0.7604\n",
            "Epoch 40/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4649 - acc: 0.7760 - val_loss: 0.4986 - val_acc: 0.7604\n",
            "Epoch 41/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4647 - acc: 0.7760 - val_loss: 0.4985 - val_acc: 0.7604\n",
            "Epoch 42/1000\n",
            "576/576 [==============================] - 0s 60us/step - loss: 0.4645 - acc: 0.7743 - val_loss: 0.4985 - val_acc: 0.7604\n",
            "Epoch 43/1000\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4643 - acc: 0.7726 - val_loss: 0.4984 - val_acc: 0.7604\n",
            "Epoch 44/1000\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4642 - acc: 0.7778 - val_loss: 0.4983 - val_acc: 0.7604\n",
            "Epoch 45/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4640 - acc: 0.7760 - val_loss: 0.4982 - val_acc: 0.7604\n",
            "Epoch 46/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4638 - acc: 0.7743 - val_loss: 0.4982 - val_acc: 0.7604\n",
            "Epoch 47/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4636 - acc: 0.7778 - val_loss: 0.4981 - val_acc: 0.7604\n",
            "Epoch 48/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4634 - acc: 0.7760 - val_loss: 0.4980 - val_acc: 0.7604\n",
            "Epoch 49/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4632 - acc: 0.7778 - val_loss: 0.4980 - val_acc: 0.7604\n",
            "Epoch 50/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4630 - acc: 0.7778 - val_loss: 0.4979 - val_acc: 0.7604\n",
            "Epoch 51/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4628 - acc: 0.7778 - val_loss: 0.4978 - val_acc: 0.7604\n",
            "Epoch 52/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4627 - acc: 0.7778 - val_loss: 0.4978 - val_acc: 0.7604\n",
            "Epoch 53/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4625 - acc: 0.7778 - val_loss: 0.4977 - val_acc: 0.7604\n",
            "Epoch 54/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4623 - acc: 0.7778 - val_loss: 0.4977 - val_acc: 0.7604\n",
            "Epoch 55/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4621 - acc: 0.7795 - val_loss: 0.4976 - val_acc: 0.7604\n",
            "Epoch 56/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4620 - acc: 0.7795 - val_loss: 0.4975 - val_acc: 0.7604\n",
            "Epoch 57/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4618 - acc: 0.7795 - val_loss: 0.4975 - val_acc: 0.7604\n",
            "Epoch 58/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4616 - acc: 0.7795 - val_loss: 0.4974 - val_acc: 0.7604\n",
            "Epoch 59/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4614 - acc: 0.7795 - val_loss: 0.4974 - val_acc: 0.7604\n",
            "Epoch 60/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4613 - acc: 0.7795 - val_loss: 0.4973 - val_acc: 0.7604\n",
            "Epoch 61/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4611 - acc: 0.7795 - val_loss: 0.4973 - val_acc: 0.7604\n",
            "Epoch 62/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4610 - acc: 0.7795 - val_loss: 0.4972 - val_acc: 0.7604\n",
            "Epoch 63/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4608 - acc: 0.7795 - val_loss: 0.4972 - val_acc: 0.7604\n",
            "Epoch 64/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4606 - acc: 0.7812 - val_loss: 0.4971 - val_acc: 0.7604\n",
            "Epoch 65/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4604 - acc: 0.7812 - val_loss: 0.4971 - val_acc: 0.7604\n",
            "Epoch 66/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4603 - acc: 0.7812 - val_loss: 0.4970 - val_acc: 0.7604\n",
            "Epoch 67/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4601 - acc: 0.7812 - val_loss: 0.4970 - val_acc: 0.7604\n",
            "Epoch 68/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4599 - acc: 0.7812 - val_loss: 0.4969 - val_acc: 0.7604\n",
            "Epoch 69/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4598 - acc: 0.7812 - val_loss: 0.4968 - val_acc: 0.7604\n",
            "Epoch 70/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4596 - acc: 0.7812 - val_loss: 0.4968 - val_acc: 0.7604\n",
            "Epoch 71/1000\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4594 - acc: 0.7812 - val_loss: 0.4967 - val_acc: 0.7656\n",
            "Epoch 72/1000\n",
            "576/576 [==============================] - 0s 61us/step - loss: 0.4593 - acc: 0.7812 - val_loss: 0.4967 - val_acc: 0.7656\n",
            "Epoch 73/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4591 - acc: 0.7812 - val_loss: 0.4966 - val_acc: 0.7656\n",
            "Epoch 74/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4589 - acc: 0.7812 - val_loss: 0.4965 - val_acc: 0.7656\n",
            "Epoch 75/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4588 - acc: 0.7812 - val_loss: 0.4965 - val_acc: 0.7656\n",
            "Epoch 76/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4586 - acc: 0.7812 - val_loss: 0.4964 - val_acc: 0.7656\n",
            "Epoch 77/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4584 - acc: 0.7812 - val_loss: 0.4964 - val_acc: 0.7604\n",
            "Epoch 78/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4583 - acc: 0.7812 - val_loss: 0.4963 - val_acc: 0.7604\n",
            "Epoch 79/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4581 - acc: 0.7812 - val_loss: 0.4963 - val_acc: 0.7604\n",
            "Epoch 80/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4580 - acc: 0.7812 - val_loss: 0.4962 - val_acc: 0.7604\n",
            "Epoch 81/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4578 - acc: 0.7812 - val_loss: 0.4962 - val_acc: 0.7604\n",
            "Epoch 82/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4576 - acc: 0.7830 - val_loss: 0.4961 - val_acc: 0.7604\n",
            "Epoch 83/1000\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4575 - acc: 0.7812 - val_loss: 0.4961 - val_acc: 0.7604\n",
            "Epoch 84/1000\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4573 - acc: 0.7812 - val_loss: 0.4960 - val_acc: 0.7604\n",
            "Epoch 85/1000\n",
            "576/576 [==============================] - 0s 60us/step - loss: 0.4572 - acc: 0.7847 - val_loss: 0.4960 - val_acc: 0.7604\n",
            "Epoch 86/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4571 - acc: 0.7830 - val_loss: 0.4959 - val_acc: 0.7604\n",
            "Epoch 87/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4569 - acc: 0.7830 - val_loss: 0.4959 - val_acc: 0.7604\n",
            "Epoch 88/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4567 - acc: 0.7847 - val_loss: 0.4958 - val_acc: 0.7604\n",
            "Epoch 89/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4566 - acc: 0.7865 - val_loss: 0.4958 - val_acc: 0.7604\n",
            "Epoch 90/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4565 - acc: 0.7847 - val_loss: 0.4957 - val_acc: 0.7604\n",
            "Epoch 91/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4563 - acc: 0.7865 - val_loss: 0.4957 - val_acc: 0.7604\n",
            "Epoch 92/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4562 - acc: 0.7847 - val_loss: 0.4956 - val_acc: 0.7604\n",
            "Epoch 93/1000\n",
            "576/576 [==============================] - 0s 40us/step - loss: 0.4560 - acc: 0.7865 - val_loss: 0.4956 - val_acc: 0.7604\n",
            "Epoch 94/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4559 - acc: 0.7865 - val_loss: 0.4955 - val_acc: 0.7604\n",
            "Epoch 95/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4557 - acc: 0.7847 - val_loss: 0.4955 - val_acc: 0.7604\n",
            "Epoch 96/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4556 - acc: 0.7847 - val_loss: 0.4954 - val_acc: 0.7604\n",
            "Epoch 97/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4555 - acc: 0.7865 - val_loss: 0.4954 - val_acc: 0.7604\n",
            "Epoch 98/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4553 - acc: 0.7865 - val_loss: 0.4953 - val_acc: 0.7604\n",
            "Epoch 99/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4552 - acc: 0.7865 - val_loss: 0.4953 - val_acc: 0.7604\n",
            "Epoch 100/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4551 - acc: 0.7865 - val_loss: 0.4953 - val_acc: 0.7604\n",
            "Epoch 101/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4550 - acc: 0.7865 - val_loss: 0.4952 - val_acc: 0.7604\n",
            "Epoch 102/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4548 - acc: 0.7865 - val_loss: 0.4952 - val_acc: 0.7604\n",
            "Epoch 103/1000\n",
            "576/576 [==============================] - 0s 62us/step - loss: 0.4547 - acc: 0.7865 - val_loss: 0.4951 - val_acc: 0.7604\n",
            "Epoch 104/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4546 - acc: 0.7865 - val_loss: 0.4951 - val_acc: 0.7604\n",
            "Epoch 105/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4544 - acc: 0.7865 - val_loss: 0.4950 - val_acc: 0.7604\n",
            "Epoch 106/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4543 - acc: 0.7847 - val_loss: 0.4950 - val_acc: 0.7552\n",
            "Epoch 107/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4542 - acc: 0.7847 - val_loss: 0.4949 - val_acc: 0.7552\n",
            "Epoch 108/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4541 - acc: 0.7847 - val_loss: 0.4949 - val_acc: 0.7552\n",
            "Epoch 109/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4540 - acc: 0.7847 - val_loss: 0.4949 - val_acc: 0.7552\n",
            "Epoch 110/1000\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4538 - acc: 0.7847 - val_loss: 0.4948 - val_acc: 0.7604\n",
            "Epoch 111/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4537 - acc: 0.7847 - val_loss: 0.4948 - val_acc: 0.7604\n",
            "Epoch 112/1000\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4536 - acc: 0.7847 - val_loss: 0.4947 - val_acc: 0.7604\n",
            "Epoch 113/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4535 - acc: 0.7847 - val_loss: 0.4947 - val_acc: 0.7604\n",
            "Epoch 114/1000\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4534 - acc: 0.7847 - val_loss: 0.4947 - val_acc: 0.7604\n",
            "Epoch 115/1000\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4532 - acc: 0.7865 - val_loss: 0.4946 - val_acc: 0.7604\n",
            "Epoch 116/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4531 - acc: 0.7847 - val_loss: 0.4946 - val_acc: 0.7604\n",
            "Epoch 117/1000\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4530 - acc: 0.7865 - val_loss: 0.4945 - val_acc: 0.7604\n",
            "Epoch 118/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4529 - acc: 0.7865 - val_loss: 0.4945 - val_acc: 0.7604\n",
            "Epoch 119/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4528 - acc: 0.7865 - val_loss: 0.4945 - val_acc: 0.7604\n",
            "Epoch 120/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4527 - acc: 0.7865 - val_loss: 0.4944 - val_acc: 0.7604\n",
            "Epoch 121/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4526 - acc: 0.7865 - val_loss: 0.4944 - val_acc: 0.7604\n",
            "Epoch 122/1000\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.4525 - acc: 0.7865 - val_loss: 0.4944 - val_acc: 0.7604\n",
            "Epoch 123/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4524 - acc: 0.7865 - val_loss: 0.4943 - val_acc: 0.7604\n",
            "Epoch 124/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4522 - acc: 0.7865 - val_loss: 0.4943 - val_acc: 0.7604\n",
            "Epoch 125/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4521 - acc: 0.7865 - val_loss: 0.4943 - val_acc: 0.7604\n",
            "Epoch 126/1000\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4521 - acc: 0.7865 - val_loss: 0.4942 - val_acc: 0.7604\n",
            "Epoch 127/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4519 - acc: 0.7865 - val_loss: 0.4942 - val_acc: 0.7604\n",
            "Epoch 128/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4518 - acc: 0.7865 - val_loss: 0.4942 - val_acc: 0.7604\n",
            "Epoch 129/1000\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.4517 - acc: 0.7865 - val_loss: 0.4941 - val_acc: 0.7604\n",
            "Epoch 130/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4516 - acc: 0.7865 - val_loss: 0.4941 - val_acc: 0.7604\n",
            "Epoch 131/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4515 - acc: 0.7865 - val_loss: 0.4941 - val_acc: 0.7604\n",
            "Epoch 132/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4514 - acc: 0.7865 - val_loss: 0.4941 - val_acc: 0.7604\n",
            "Epoch 133/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4513 - acc: 0.7865 - val_loss: 0.4940 - val_acc: 0.7604\n",
            "Epoch 134/1000\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4512 - acc: 0.7865 - val_loss: 0.4940 - val_acc: 0.7604\n",
            "Epoch 135/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4511 - acc: 0.7865 - val_loss: 0.4940 - val_acc: 0.7604\n",
            "Epoch 136/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4510 - acc: 0.7865 - val_loss: 0.4939 - val_acc: 0.7604\n",
            "Epoch 137/1000\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4509 - acc: 0.7865 - val_loss: 0.4939 - val_acc: 0.7604\n",
            "Epoch 138/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4508 - acc: 0.7865 - val_loss: 0.4939 - val_acc: 0.7604\n",
            "Epoch 139/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4507 - acc: 0.7865 - val_loss: 0.4939 - val_acc: 0.7604\n",
            "Epoch 140/1000\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4506 - acc: 0.7847 - val_loss: 0.4938 - val_acc: 0.7604\n",
            "Epoch 141/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4505 - acc: 0.7865 - val_loss: 0.4938 - val_acc: 0.7604\n",
            "Epoch 142/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4504 - acc: 0.7865 - val_loss: 0.4938 - val_acc: 0.7604\n",
            "Epoch 143/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4503 - acc: 0.7847 - val_loss: 0.4937 - val_acc: 0.7604\n",
            "Epoch 144/1000\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4502 - acc: 0.7847 - val_loss: 0.4937 - val_acc: 0.7604\n",
            "Epoch 145/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4501 - acc: 0.7847 - val_loss: 0.4937 - val_acc: 0.7604\n",
            "Epoch 146/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4500 - acc: 0.7847 - val_loss: 0.4937 - val_acc: 0.7604\n",
            "Epoch 147/1000\n",
            "576/576 [==============================] - 0s 69us/step - loss: 0.4499 - acc: 0.7847 - val_loss: 0.4936 - val_acc: 0.7604\n",
            "Epoch 148/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4498 - acc: 0.7847 - val_loss: 0.4936 - val_acc: 0.7604\n",
            "Epoch 149/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4497 - acc: 0.7847 - val_loss: 0.4936 - val_acc: 0.7604\n",
            "Epoch 150/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4496 - acc: 0.7865 - val_loss: 0.4936 - val_acc: 0.7604\n",
            "Epoch 151/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4495 - acc: 0.7847 - val_loss: 0.4935 - val_acc: 0.7604\n",
            "Epoch 152/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4494 - acc: 0.7847 - val_loss: 0.4935 - val_acc: 0.7604\n",
            "Epoch 153/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4493 - acc: 0.7865 - val_loss: 0.4935 - val_acc: 0.7604\n",
            "Epoch 154/1000\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4493 - acc: 0.7865 - val_loss: 0.4934 - val_acc: 0.7604\n",
            "Epoch 155/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4491 - acc: 0.7865 - val_loss: 0.4934 - val_acc: 0.7604\n",
            "Epoch 156/1000\n",
            "576/576 [==============================] - 0s 63us/step - loss: 0.4491 - acc: 0.7847 - val_loss: 0.4934 - val_acc: 0.7604\n",
            "Epoch 157/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4490 - acc: 0.7847 - val_loss: 0.4934 - val_acc: 0.7604\n",
            "Epoch 158/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4489 - acc: 0.7865 - val_loss: 0.4934 - val_acc: 0.7604\n",
            "Epoch 159/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4488 - acc: 0.7847 - val_loss: 0.4933 - val_acc: 0.7604\n",
            "Epoch 160/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4487 - acc: 0.7865 - val_loss: 0.4933 - val_acc: 0.7604\n",
            "Epoch 161/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4486 - acc: 0.7865 - val_loss: 0.4933 - val_acc: 0.7604\n",
            "Epoch 162/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4485 - acc: 0.7865 - val_loss: 0.4932 - val_acc: 0.7604\n",
            "Epoch 163/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4484 - acc: 0.7865 - val_loss: 0.4932 - val_acc: 0.7604\n",
            "Epoch 164/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4483 - acc: 0.7865 - val_loss: 0.4932 - val_acc: 0.7604\n",
            "Epoch 165/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4482 - acc: 0.7865 - val_loss: 0.4932 - val_acc: 0.7604\n",
            "Epoch 166/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4482 - acc: 0.7865 - val_loss: 0.4931 - val_acc: 0.7604\n",
            "Epoch 167/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4481 - acc: 0.7865 - val_loss: 0.4931 - val_acc: 0.7604\n",
            "Epoch 168/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4480 - acc: 0.7865 - val_loss: 0.4931 - val_acc: 0.7604\n",
            "Epoch 169/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4479 - acc: 0.7865 - val_loss: 0.4930 - val_acc: 0.7604\n",
            "Epoch 170/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4478 - acc: 0.7865 - val_loss: 0.4930 - val_acc: 0.7604\n",
            "Epoch 171/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4477 - acc: 0.7882 - val_loss: 0.4930 - val_acc: 0.7604\n",
            "Epoch 172/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4476 - acc: 0.7882 - val_loss: 0.4930 - val_acc: 0.7604\n",
            "Epoch 173/1000\n",
            "576/576 [==============================] - 0s 40us/step - loss: 0.4475 - acc: 0.7882 - val_loss: 0.4929 - val_acc: 0.7604\n",
            "Epoch 174/1000\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4474 - acc: 0.7882 - val_loss: 0.4929 - val_acc: 0.7604\n",
            "Epoch 175/1000\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4474 - acc: 0.7882 - val_loss: 0.4929 - val_acc: 0.7604\n",
            "Epoch 176/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4473 - acc: 0.7882 - val_loss: 0.4929 - val_acc: 0.7604\n",
            "Epoch 177/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4472 - acc: 0.7882 - val_loss: 0.4929 - val_acc: 0.7604\n",
            "Epoch 178/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4471 - acc: 0.7882 - val_loss: 0.4928 - val_acc: 0.7604\n",
            "Epoch 179/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4470 - acc: 0.7882 - val_loss: 0.4928 - val_acc: 0.7604\n",
            "Epoch 180/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4470 - acc: 0.7882 - val_loss: 0.4928 - val_acc: 0.7604\n",
            "Epoch 181/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4469 - acc: 0.7882 - val_loss: 0.4928 - val_acc: 0.7604\n",
            "Epoch 182/1000\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4468 - acc: 0.7882 - val_loss: 0.4928 - val_acc: 0.7604\n",
            "Epoch 183/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4467 - acc: 0.7882 - val_loss: 0.4928 - val_acc: 0.7604\n",
            "Epoch 184/1000\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4466 - acc: 0.7882 - val_loss: 0.4928 - val_acc: 0.7604\n",
            "Epoch 185/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4466 - acc: 0.7882 - val_loss: 0.4927 - val_acc: 0.7604\n",
            "Epoch 186/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4465 - acc: 0.7882 - val_loss: 0.4927 - val_acc: 0.7604\n",
            "Epoch 187/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4464 - acc: 0.7882 - val_loss: 0.4927 - val_acc: 0.7604\n",
            "Epoch 188/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4463 - acc: 0.7882 - val_loss: 0.4927 - val_acc: 0.7604\n",
            "Epoch 189/1000\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4462 - acc: 0.7882 - val_loss: 0.4927 - val_acc: 0.7604\n",
            "Epoch 190/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4462 - acc: 0.7882 - val_loss: 0.4927 - val_acc: 0.7604\n",
            "Epoch 191/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4461 - acc: 0.7882 - val_loss: 0.4927 - val_acc: 0.7604\n",
            "Epoch 192/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4460 - acc: 0.7882 - val_loss: 0.4927 - val_acc: 0.7604\n",
            "Epoch 193/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4459 - acc: 0.7882 - val_loss: 0.4926 - val_acc: 0.7604\n",
            "Epoch 194/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4458 - acc: 0.7882 - val_loss: 0.4926 - val_acc: 0.7604\n",
            "Epoch 195/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4457 - acc: 0.7882 - val_loss: 0.4926 - val_acc: 0.7552\n",
            "Epoch 196/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4457 - acc: 0.7882 - val_loss: 0.4926 - val_acc: 0.7552\n",
            "Epoch 197/1000\n",
            "576/576 [==============================] - 0s 63us/step - loss: 0.4456 - acc: 0.7882 - val_loss: 0.4926 - val_acc: 0.7552\n",
            "Epoch 198/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4455 - acc: 0.7882 - val_loss: 0.4926 - val_acc: 0.7552\n",
            "Epoch 199/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4454 - acc: 0.7882 - val_loss: 0.4926 - val_acc: 0.7552\n",
            "Epoch 200/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4454 - acc: 0.7882 - val_loss: 0.4926 - val_acc: 0.7552\n",
            "Epoch 201/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4453 - acc: 0.7882 - val_loss: 0.4926 - val_acc: 0.7552\n",
            "Epoch 202/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4452 - acc: 0.7882 - val_loss: 0.4926 - val_acc: 0.7552\n",
            "Epoch 203/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4451 - acc: 0.7882 - val_loss: 0.4925 - val_acc: 0.7552\n",
            "Epoch 204/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4450 - acc: 0.7882 - val_loss: 0.4925 - val_acc: 0.7552\n",
            "Epoch 205/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4450 - acc: 0.7882 - val_loss: 0.4925 - val_acc: 0.7552\n",
            "Epoch 206/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4449 - acc: 0.7882 - val_loss: 0.4925 - val_acc: 0.7552\n",
            "Epoch 207/1000\n",
            "576/576 [==============================] - 0s 61us/step - loss: 0.4448 - acc: 0.7882 - val_loss: 0.4925 - val_acc: 0.7552\n",
            "Epoch 208/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4448 - acc: 0.7882 - val_loss: 0.4925 - val_acc: 0.7552\n",
            "Epoch 209/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4447 - acc: 0.7882 - val_loss: 0.4925 - val_acc: 0.7552\n",
            "Epoch 210/1000\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4446 - acc: 0.7882 - val_loss: 0.4925 - val_acc: 0.7552\n",
            "Epoch 211/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4445 - acc: 0.7882 - val_loss: 0.4925 - val_acc: 0.7552\n",
            "Epoch 212/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4445 - acc: 0.7882 - val_loss: 0.4925 - val_acc: 0.7552\n",
            "Epoch 213/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4444 - acc: 0.7882 - val_loss: 0.4925 - val_acc: 0.7552\n",
            "Epoch 214/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4443 - acc: 0.7882 - val_loss: 0.4925 - val_acc: 0.7552\n",
            "Epoch 215/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4442 - acc: 0.7882 - val_loss: 0.4925 - val_acc: 0.7552\n",
            "Epoch 216/1000\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4442 - acc: 0.7882 - val_loss: 0.4925 - val_acc: 0.7552\n",
            "Epoch 217/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4441 - acc: 0.7882 - val_loss: 0.4925 - val_acc: 0.7552\n",
            "Epoch 218/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4440 - acc: 0.7882 - val_loss: 0.4925 - val_acc: 0.7552\n",
            "Epoch 219/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4440 - acc: 0.7882 - val_loss: 0.4925 - val_acc: 0.7552\n",
            "Epoch 220/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4439 - acc: 0.7865 - val_loss: 0.4925 - val_acc: 0.7552\n",
            "Epoch 221/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4438 - acc: 0.7865 - val_loss: 0.4924 - val_acc: 0.7552\n",
            "Epoch 222/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4437 - acc: 0.7865 - val_loss: 0.4924 - val_acc: 0.7552\n",
            "Epoch 223/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4437 - acc: 0.7865 - val_loss: 0.4924 - val_acc: 0.7552\n",
            "Epoch 224/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4436 - acc: 0.7865 - val_loss: 0.4924 - val_acc: 0.7552\n",
            "Epoch 225/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4436 - acc: 0.7865 - val_loss: 0.4924 - val_acc: 0.7552\n",
            "Epoch 226/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4435 - acc: 0.7865 - val_loss: 0.4924 - val_acc: 0.7552\n",
            "Epoch 227/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4434 - acc: 0.7865 - val_loss: 0.4924 - val_acc: 0.7552\n",
            "Epoch 228/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4433 - acc: 0.7865 - val_loss: 0.4924 - val_acc: 0.7552\n",
            "Epoch 229/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4433 - acc: 0.7865 - val_loss: 0.4924 - val_acc: 0.7552\n",
            "Epoch 230/1000\n",
            "576/576 [==============================] - 0s 40us/step - loss: 0.4432 - acc: 0.7865 - val_loss: 0.4924 - val_acc: 0.7552\n",
            "Epoch 231/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4431 - acc: 0.7865 - val_loss: 0.4924 - val_acc: 0.7552\n",
            "Epoch 232/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4431 - acc: 0.7865 - val_loss: 0.4924 - val_acc: 0.7552\n",
            "Epoch 233/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4430 - acc: 0.7865 - val_loss: 0.4924 - val_acc: 0.7552\n",
            "Epoch 234/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4429 - acc: 0.7865 - val_loss: 0.4924 - val_acc: 0.7552\n",
            "Epoch 235/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4429 - acc: 0.7865 - val_loss: 0.4924 - val_acc: 0.7552\n",
            "Epoch 236/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4428 - acc: 0.7865 - val_loss: 0.4924 - val_acc: 0.7552\n",
            "Epoch 237/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4427 - acc: 0.7865 - val_loss: 0.4924 - val_acc: 0.7552\n",
            "Epoch 238/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4427 - acc: 0.7865 - val_loss: 0.4924 - val_acc: 0.7552\n",
            "Epoch 239/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4426 - acc: 0.7865 - val_loss: 0.4924 - val_acc: 0.7552\n",
            "Epoch 240/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4425 - acc: 0.7865 - val_loss: 0.4924 - val_acc: 0.7552\n",
            "Epoch 241/1000\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.4424 - acc: 0.7865 - val_loss: 0.4924 - val_acc: 0.7552\n",
            "Epoch 242/1000\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4424 - acc: 0.7865 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 243/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4423 - acc: 0.7865 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 244/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4423 - acc: 0.7865 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 245/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4422 - acc: 0.7847 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 246/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4421 - acc: 0.7847 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 247/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4421 - acc: 0.7847 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 248/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4420 - acc: 0.7847 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 249/1000\n",
            "576/576 [==============================] - 0s 59us/step - loss: 0.4420 - acc: 0.7847 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 250/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4419 - acc: 0.7847 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 251/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4418 - acc: 0.7847 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 252/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4417 - acc: 0.7847 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 253/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4417 - acc: 0.7847 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 254/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4416 - acc: 0.7847 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 255/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4415 - acc: 0.7847 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 256/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4415 - acc: 0.7847 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 257/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4414 - acc: 0.7847 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 258/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4414 - acc: 0.7847 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 259/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4413 - acc: 0.7847 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 260/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4412 - acc: 0.7847 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 261/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4412 - acc: 0.7847 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 262/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4411 - acc: 0.7847 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 263/1000\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4410 - acc: 0.7847 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 264/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4410 - acc: 0.7847 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 265/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4409 - acc: 0.7847 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 266/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4408 - acc: 0.7847 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 267/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4408 - acc: 0.7847 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 268/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4407 - acc: 0.7847 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 269/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4407 - acc: 0.7847 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 270/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4406 - acc: 0.7847 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 271/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4406 - acc: 0.7847 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 272/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4405 - acc: 0.7847 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 273/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4404 - acc: 0.7847 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 274/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4404 - acc: 0.7865 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 275/1000\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4403 - acc: 0.7865 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 276/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4402 - acc: 0.7865 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 277/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4402 - acc: 0.7865 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 278/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4401 - acc: 0.7847 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 279/1000\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4401 - acc: 0.7865 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 280/1000\n",
            "576/576 [==============================] - 0s 63us/step - loss: 0.4400 - acc: 0.7865 - val_loss: 0.4924 - val_acc: 0.7604\n",
            "Epoch 281/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4400 - acc: 0.7865 - val_loss: 0.4925 - val_acc: 0.7604\n",
            "Epoch 282/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4399 - acc: 0.7865 - val_loss: 0.4925 - val_acc: 0.7604\n",
            "Epoch 283/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4399 - acc: 0.7865 - val_loss: 0.4925 - val_acc: 0.7604\n",
            "Epoch 284/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4398 - acc: 0.7865 - val_loss: 0.4925 - val_acc: 0.7604\n",
            "Epoch 285/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4397 - acc: 0.7865 - val_loss: 0.4925 - val_acc: 0.7604\n",
            "Epoch 286/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4397 - acc: 0.7847 - val_loss: 0.4925 - val_acc: 0.7604\n",
            "Epoch 287/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4396 - acc: 0.7865 - val_loss: 0.4925 - val_acc: 0.7604\n",
            "Epoch 288/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4395 - acc: 0.7865 - val_loss: 0.4925 - val_acc: 0.7604\n",
            "Epoch 289/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4395 - acc: 0.7865 - val_loss: 0.4925 - val_acc: 0.7604\n",
            "Epoch 290/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4394 - acc: 0.7865 - val_loss: 0.4925 - val_acc: 0.7604\n",
            "Epoch 291/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4394 - acc: 0.7865 - val_loss: 0.4925 - val_acc: 0.7604\n",
            "Epoch 292/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4393 - acc: 0.7865 - val_loss: 0.4925 - val_acc: 0.7604\n",
            "Epoch 293/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4392 - acc: 0.7865 - val_loss: 0.4925 - val_acc: 0.7604\n",
            "Epoch 294/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4392 - acc: 0.7847 - val_loss: 0.4926 - val_acc: 0.7604\n",
            "Epoch 295/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4391 - acc: 0.7865 - val_loss: 0.4926 - val_acc: 0.7604\n",
            "Epoch 296/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4391 - acc: 0.7865 - val_loss: 0.4926 - val_acc: 0.7604\n",
            "Epoch 297/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4390 - acc: 0.7865 - val_loss: 0.4926 - val_acc: 0.7604\n",
            "Epoch 298/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4390 - acc: 0.7865 - val_loss: 0.4926 - val_acc: 0.7604\n",
            "Epoch 299/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4389 - acc: 0.7847 - val_loss: 0.4926 - val_acc: 0.7604\n",
            "Epoch 300/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4389 - acc: 0.7865 - val_loss: 0.4926 - val_acc: 0.7604\n",
            "Epoch 301/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4388 - acc: 0.7865 - val_loss: 0.4926 - val_acc: 0.7604\n",
            "Epoch 302/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4387 - acc: 0.7865 - val_loss: 0.4926 - val_acc: 0.7604\n",
            "Epoch 303/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4387 - acc: 0.7865 - val_loss: 0.4926 - val_acc: 0.7604\n",
            "Epoch 304/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4386 - acc: 0.7847 - val_loss: 0.4927 - val_acc: 0.7604\n",
            "Epoch 305/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4385 - acc: 0.7865 - val_loss: 0.4927 - val_acc: 0.7604\n",
            "Epoch 306/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4385 - acc: 0.7847 - val_loss: 0.4927 - val_acc: 0.7604\n",
            "Epoch 307/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4385 - acc: 0.7847 - val_loss: 0.4927 - val_acc: 0.7604\n",
            "Epoch 308/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4384 - acc: 0.7865 - val_loss: 0.4927 - val_acc: 0.7604\n",
            "Epoch 309/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4383 - acc: 0.7847 - val_loss: 0.4927 - val_acc: 0.7604\n",
            "Epoch 310/1000\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4383 - acc: 0.7847 - val_loss: 0.4927 - val_acc: 0.7604\n",
            "Epoch 311/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4382 - acc: 0.7865 - val_loss: 0.4927 - val_acc: 0.7604\n",
            "Epoch 312/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4382 - acc: 0.7847 - val_loss: 0.4927 - val_acc: 0.7604\n",
            "Epoch 313/1000\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4381 - acc: 0.7847 - val_loss: 0.4928 - val_acc: 0.7604\n",
            "Epoch 314/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4380 - acc: 0.7865 - val_loss: 0.4928 - val_acc: 0.7604\n",
            "Epoch 315/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4380 - acc: 0.7847 - val_loss: 0.4928 - val_acc: 0.7604\n",
            "Epoch 316/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4380 - acc: 0.7865 - val_loss: 0.4928 - val_acc: 0.7604\n",
            "Epoch 317/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4379 - acc: 0.7847 - val_loss: 0.4928 - val_acc: 0.7604\n",
            "Epoch 318/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4378 - acc: 0.7865 - val_loss: 0.4928 - val_acc: 0.7604\n",
            "Epoch 319/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4378 - acc: 0.7865 - val_loss: 0.4929 - val_acc: 0.7604\n",
            "Epoch 320/1000\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.4377 - acc: 0.7865 - val_loss: 0.4929 - val_acc: 0.7604\n",
            "Epoch 321/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4377 - acc: 0.7865 - val_loss: 0.4929 - val_acc: 0.7604\n",
            "Epoch 322/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4376 - acc: 0.7865 - val_loss: 0.4929 - val_acc: 0.7604\n",
            "Epoch 323/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4375 - acc: 0.7865 - val_loss: 0.4929 - val_acc: 0.7604\n",
            "Epoch 324/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4375 - acc: 0.7865 - val_loss: 0.4929 - val_acc: 0.7656\n",
            "Epoch 325/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4375 - acc: 0.7865 - val_loss: 0.4929 - val_acc: 0.7656\n",
            "Epoch 326/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4374 - acc: 0.7847 - val_loss: 0.4930 - val_acc: 0.7656\n",
            "Epoch 327/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4374 - acc: 0.7865 - val_loss: 0.4930 - val_acc: 0.7656\n",
            "Epoch 328/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4373 - acc: 0.7865 - val_loss: 0.4930 - val_acc: 0.7656\n",
            "Epoch 329/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4372 - acc: 0.7865 - val_loss: 0.4930 - val_acc: 0.7656\n",
            "Epoch 330/1000\n",
            "576/576 [==============================] - 0s 39us/step - loss: 0.4372 - acc: 0.7865 - val_loss: 0.4930 - val_acc: 0.7656\n",
            "Epoch 331/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4371 - acc: 0.7865 - val_loss: 0.4930 - val_acc: 0.7656\n",
            "Epoch 332/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4371 - acc: 0.7865 - val_loss: 0.4930 - val_acc: 0.7656\n",
            "Epoch 333/1000\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4370 - acc: 0.7865 - val_loss: 0.4931 - val_acc: 0.7656\n",
            "Epoch 334/1000\n",
            "576/576 [==============================] - 0s 62us/step - loss: 0.4369 - acc: 0.7865 - val_loss: 0.4931 - val_acc: 0.7656\n",
            "Epoch 335/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4369 - acc: 0.7847 - val_loss: 0.4931 - val_acc: 0.7656\n",
            "Epoch 336/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4369 - acc: 0.7865 - val_loss: 0.4931 - val_acc: 0.7656\n",
            "Epoch 337/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4368 - acc: 0.7865 - val_loss: 0.4931 - val_acc: 0.7656\n",
            "Epoch 338/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4368 - acc: 0.7847 - val_loss: 0.4931 - val_acc: 0.7656\n",
            "Epoch 339/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4367 - acc: 0.7847 - val_loss: 0.4931 - val_acc: 0.7656\n",
            "Epoch 340/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4367 - acc: 0.7865 - val_loss: 0.4932 - val_acc: 0.7656\n",
            "Epoch 341/1000\n",
            "576/576 [==============================] - 0s 63us/step - loss: 0.4366 - acc: 0.7847 - val_loss: 0.4932 - val_acc: 0.7656\n",
            "Epoch 342/1000\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4365 - acc: 0.7865 - val_loss: 0.4932 - val_acc: 0.7656\n",
            "Epoch 343/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4365 - acc: 0.7865 - val_loss: 0.4932 - val_acc: 0.7656\n",
            "Epoch 344/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4364 - acc: 0.7865 - val_loss: 0.4932 - val_acc: 0.7656\n",
            "Epoch 345/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4364 - acc: 0.7830 - val_loss: 0.4932 - val_acc: 0.7656\n",
            "Epoch 346/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4363 - acc: 0.7847 - val_loss: 0.4932 - val_acc: 0.7656\n",
            "Epoch 347/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4363 - acc: 0.7865 - val_loss: 0.4933 - val_acc: 0.7656\n",
            "Epoch 348/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4362 - acc: 0.7830 - val_loss: 0.4933 - val_acc: 0.7656\n",
            "Epoch 349/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4362 - acc: 0.7830 - val_loss: 0.4933 - val_acc: 0.7656\n",
            "Epoch 350/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4361 - acc: 0.7865 - val_loss: 0.4933 - val_acc: 0.7656\n",
            "Epoch 351/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4361 - acc: 0.7847 - val_loss: 0.4933 - val_acc: 0.7656\n",
            "Epoch 352/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4360 - acc: 0.7830 - val_loss: 0.4933 - val_acc: 0.7656\n",
            "Epoch 353/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4360 - acc: 0.7830 - val_loss: 0.4933 - val_acc: 0.7656\n",
            "Epoch 354/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4359 - acc: 0.7830 - val_loss: 0.4934 - val_acc: 0.7656\n",
            "Epoch 355/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4359 - acc: 0.7847 - val_loss: 0.4934 - val_acc: 0.7656\n",
            "Epoch 356/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4359 - acc: 0.7812 - val_loss: 0.4934 - val_acc: 0.7656\n",
            "Epoch 357/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4358 - acc: 0.7865 - val_loss: 0.4934 - val_acc: 0.7656\n",
            "Epoch 358/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4358 - acc: 0.7865 - val_loss: 0.4934 - val_acc: 0.7656\n",
            "Epoch 359/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4357 - acc: 0.7830 - val_loss: 0.4935 - val_acc: 0.7656\n",
            "Epoch 360/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4356 - acc: 0.7812 - val_loss: 0.4935 - val_acc: 0.7656\n",
            "Epoch 361/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4356 - acc: 0.7830 - val_loss: 0.4935 - val_acc: 0.7656\n",
            "Epoch 362/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4355 - acc: 0.7830 - val_loss: 0.4935 - val_acc: 0.7656\n",
            "Epoch 363/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4355 - acc: 0.7847 - val_loss: 0.4935 - val_acc: 0.7656\n",
            "Epoch 364/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4354 - acc: 0.7830 - val_loss: 0.4935 - val_acc: 0.7656\n",
            "Epoch 365/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4354 - acc: 0.7830 - val_loss: 0.4936 - val_acc: 0.7656\n",
            "Epoch 366/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4353 - acc: 0.7830 - val_loss: 0.4936 - val_acc: 0.7656\n",
            "Epoch 367/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4353 - acc: 0.7847 - val_loss: 0.4936 - val_acc: 0.7656\n",
            "Epoch 368/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4352 - acc: 0.7812 - val_loss: 0.4936 - val_acc: 0.7656\n",
            "Epoch 369/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4352 - acc: 0.7830 - val_loss: 0.4936 - val_acc: 0.7656\n",
            "Epoch 370/1000\n",
            "576/576 [==============================] - 0s 61us/step - loss: 0.4351 - acc: 0.7830 - val_loss: 0.4936 - val_acc: 0.7656\n",
            "Epoch 371/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4351 - acc: 0.7830 - val_loss: 0.4936 - val_acc: 0.7656\n",
            "Epoch 372/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4350 - acc: 0.7830 - val_loss: 0.4937 - val_acc: 0.7656\n",
            "Epoch 373/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4350 - acc: 0.7830 - val_loss: 0.4937 - val_acc: 0.7656\n",
            "Epoch 374/1000\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4349 - acc: 0.7865 - val_loss: 0.4937 - val_acc: 0.7656\n",
            "Epoch 375/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4349 - acc: 0.7847 - val_loss: 0.4937 - val_acc: 0.7656\n",
            "Epoch 376/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4348 - acc: 0.7847 - val_loss: 0.4937 - val_acc: 0.7656\n",
            "Epoch 377/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4348 - acc: 0.7847 - val_loss: 0.4938 - val_acc: 0.7604\n",
            "Epoch 378/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4347 - acc: 0.7847 - val_loss: 0.4938 - val_acc: 0.7604\n",
            "Epoch 379/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4347 - acc: 0.7847 - val_loss: 0.4938 - val_acc: 0.7604\n",
            "Epoch 380/1000\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4346 - acc: 0.7847 - val_loss: 0.4938 - val_acc: 0.7604\n",
            "Epoch 381/1000\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4346 - acc: 0.7847 - val_loss: 0.4938 - val_acc: 0.7604\n",
            "Epoch 382/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4345 - acc: 0.7847 - val_loss: 0.4939 - val_acc: 0.7604\n",
            "Epoch 383/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4345 - acc: 0.7830 - val_loss: 0.4939 - val_acc: 0.7604\n",
            "Epoch 384/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4345 - acc: 0.7847 - val_loss: 0.4939 - val_acc: 0.7604\n",
            "Epoch 385/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4344 - acc: 0.7847 - val_loss: 0.4939 - val_acc: 0.7656\n",
            "Epoch 386/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4344 - acc: 0.7847 - val_loss: 0.4939 - val_acc: 0.7656\n",
            "Epoch 387/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4343 - acc: 0.7847 - val_loss: 0.4940 - val_acc: 0.7656\n",
            "Epoch 388/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4342 - acc: 0.7847 - val_loss: 0.4940 - val_acc: 0.7656\n",
            "Epoch 389/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4342 - acc: 0.7847 - val_loss: 0.4940 - val_acc: 0.7656\n",
            "Epoch 390/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4341 - acc: 0.7847 - val_loss: 0.4940 - val_acc: 0.7656\n",
            "Epoch 391/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4341 - acc: 0.7847 - val_loss: 0.4940 - val_acc: 0.7656\n",
            "Epoch 392/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4341 - acc: 0.7847 - val_loss: 0.4940 - val_acc: 0.7656\n",
            "Epoch 393/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4340 - acc: 0.7847 - val_loss: 0.4941 - val_acc: 0.7656\n",
            "Epoch 394/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4339 - acc: 0.7847 - val_loss: 0.4941 - val_acc: 0.7656\n",
            "Epoch 395/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4339 - acc: 0.7847 - val_loss: 0.4941 - val_acc: 0.7656\n",
            "Epoch 396/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4339 - acc: 0.7865 - val_loss: 0.4941 - val_acc: 0.7656\n",
            "Epoch 397/1000\n",
            "576/576 [==============================] - 0s 60us/step - loss: 0.4338 - acc: 0.7847 - val_loss: 0.4941 - val_acc: 0.7656\n",
            "Epoch 398/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4338 - acc: 0.7847 - val_loss: 0.4942 - val_acc: 0.7656\n",
            "Epoch 399/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4337 - acc: 0.7865 - val_loss: 0.4942 - val_acc: 0.7656\n",
            "Epoch 400/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4337 - acc: 0.7847 - val_loss: 0.4942 - val_acc: 0.7656\n",
            "Epoch 401/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4336 - acc: 0.7847 - val_loss: 0.4942 - val_acc: 0.7656\n",
            "Epoch 402/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4336 - acc: 0.7847 - val_loss: 0.4942 - val_acc: 0.7656\n",
            "Epoch 403/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4335 - acc: 0.7865 - val_loss: 0.4943 - val_acc: 0.7656\n",
            "Epoch 404/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4335 - acc: 0.7865 - val_loss: 0.4943 - val_acc: 0.7656\n",
            "Epoch 405/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4335 - acc: 0.7847 - val_loss: 0.4943 - val_acc: 0.7656\n",
            "Epoch 406/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4334 - acc: 0.7847 - val_loss: 0.4943 - val_acc: 0.7656\n",
            "Epoch 407/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4334 - acc: 0.7847 - val_loss: 0.4943 - val_acc: 0.7708\n",
            "Epoch 408/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4334 - acc: 0.7899 - val_loss: 0.4944 - val_acc: 0.7708\n",
            "Epoch 409/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4333 - acc: 0.7865 - val_loss: 0.4944 - val_acc: 0.7708\n",
            "Epoch 410/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4332 - acc: 0.7865 - val_loss: 0.4944 - val_acc: 0.7708\n",
            "Epoch 411/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4332 - acc: 0.7882 - val_loss: 0.4944 - val_acc: 0.7708\n",
            "Epoch 412/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4332 - acc: 0.7865 - val_loss: 0.4944 - val_acc: 0.7708\n",
            "Epoch 413/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4331 - acc: 0.7882 - val_loss: 0.4944 - val_acc: 0.7708\n",
            "Epoch 414/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4331 - acc: 0.7865 - val_loss: 0.4944 - val_acc: 0.7708\n",
            "Epoch 415/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4330 - acc: 0.7882 - val_loss: 0.4945 - val_acc: 0.7708\n",
            "Epoch 416/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4330 - acc: 0.7882 - val_loss: 0.4945 - val_acc: 0.7708\n",
            "Epoch 417/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4330 - acc: 0.7882 - val_loss: 0.4945 - val_acc: 0.7708\n",
            "Epoch 418/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4329 - acc: 0.7865 - val_loss: 0.4945 - val_acc: 0.7708\n",
            "Epoch 419/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4329 - acc: 0.7882 - val_loss: 0.4945 - val_acc: 0.7708\n",
            "Epoch 420/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4328 - acc: 0.7882 - val_loss: 0.4945 - val_acc: 0.7708\n",
            "Epoch 421/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4328 - acc: 0.7865 - val_loss: 0.4945 - val_acc: 0.7708\n",
            "Epoch 422/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4327 - acc: 0.7882 - val_loss: 0.4946 - val_acc: 0.7708\n",
            "Epoch 423/1000\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4327 - acc: 0.7865 - val_loss: 0.4946 - val_acc: 0.7708\n",
            "Epoch 424/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4326 - acc: 0.7882 - val_loss: 0.4946 - val_acc: 0.7708\n",
            "Epoch 425/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4326 - acc: 0.7865 - val_loss: 0.4946 - val_acc: 0.7708\n",
            "Epoch 426/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4326 - acc: 0.7865 - val_loss: 0.4946 - val_acc: 0.7708\n",
            "Epoch 427/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4325 - acc: 0.7865 - val_loss: 0.4946 - val_acc: 0.7708\n",
            "Epoch 428/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4325 - acc: 0.7865 - val_loss: 0.4946 - val_acc: 0.7708\n",
            "Epoch 429/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4325 - acc: 0.7865 - val_loss: 0.4946 - val_acc: 0.7708\n",
            "Epoch 430/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4324 - acc: 0.7865 - val_loss: 0.4946 - val_acc: 0.7708\n",
            "Epoch 431/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4324 - acc: 0.7847 - val_loss: 0.4946 - val_acc: 0.7708\n",
            "Epoch 432/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4323 - acc: 0.7847 - val_loss: 0.4946 - val_acc: 0.7708\n",
            "Epoch 433/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4322 - acc: 0.7865 - val_loss: 0.4947 - val_acc: 0.7708\n",
            "Epoch 434/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4322 - acc: 0.7865 - val_loss: 0.4947 - val_acc: 0.7708\n",
            "Epoch 435/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4322 - acc: 0.7882 - val_loss: 0.4947 - val_acc: 0.7708\n",
            "Epoch 436/1000\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4321 - acc: 0.7847 - val_loss: 0.4947 - val_acc: 0.7708\n",
            "Epoch 437/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4321 - acc: 0.7882 - val_loss: 0.4947 - val_acc: 0.7708\n",
            "Epoch 438/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4320 - acc: 0.7865 - val_loss: 0.4947 - val_acc: 0.7708\n",
            "Epoch 439/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4320 - acc: 0.7882 - val_loss: 0.4947 - val_acc: 0.7708\n",
            "Epoch 440/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4320 - acc: 0.7882 - val_loss: 0.4947 - val_acc: 0.7708\n",
            "Epoch 441/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4319 - acc: 0.7899 - val_loss: 0.4947 - val_acc: 0.7708\n",
            "Epoch 442/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4319 - acc: 0.7882 - val_loss: 0.4947 - val_acc: 0.7708\n",
            "Epoch 443/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4318 - acc: 0.7882 - val_loss: 0.4947 - val_acc: 0.7708\n",
            "Epoch 444/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4318 - acc: 0.7899 - val_loss: 0.4947 - val_acc: 0.7708\n",
            "Epoch 445/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4317 - acc: 0.7882 - val_loss: 0.4948 - val_acc: 0.7708\n",
            "Epoch 446/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4317 - acc: 0.7882 - val_loss: 0.4948 - val_acc: 0.7708\n",
            "Epoch 447/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4316 - acc: 0.7882 - val_loss: 0.4948 - val_acc: 0.7708\n",
            "Epoch 448/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4316 - acc: 0.7882 - val_loss: 0.4948 - val_acc: 0.7708\n",
            "Epoch 449/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4316 - acc: 0.7865 - val_loss: 0.4948 - val_acc: 0.7708\n",
            "Epoch 450/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4315 - acc: 0.7882 - val_loss: 0.4948 - val_acc: 0.7708\n",
            "Epoch 451/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4315 - acc: 0.7882 - val_loss: 0.4948 - val_acc: 0.7708\n",
            "Epoch 452/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4314 - acc: 0.7882 - val_loss: 0.4948 - val_acc: 0.7708\n",
            "Epoch 453/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4314 - acc: 0.7882 - val_loss: 0.4948 - val_acc: 0.7708\n",
            "Epoch 454/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4313 - acc: 0.7882 - val_loss: 0.4948 - val_acc: 0.7708\n",
            "Epoch 455/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4313 - acc: 0.7882 - val_loss: 0.4948 - val_acc: 0.7708\n",
            "Epoch 456/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4313 - acc: 0.7882 - val_loss: 0.4949 - val_acc: 0.7708\n",
            "Epoch 457/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4312 - acc: 0.7882 - val_loss: 0.4949 - val_acc: 0.7708\n",
            "Epoch 458/1000\n",
            "576/576 [==============================] - 0s 40us/step - loss: 0.4311 - acc: 0.7882 - val_loss: 0.4949 - val_acc: 0.7708\n",
            "Epoch 459/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4311 - acc: 0.7882 - val_loss: 0.4949 - val_acc: 0.7708\n",
            "Epoch 460/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4311 - acc: 0.7882 - val_loss: 0.4949 - val_acc: 0.7708\n",
            "Epoch 461/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4310 - acc: 0.7882 - val_loss: 0.4949 - val_acc: 0.7708\n",
            "Epoch 462/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4310 - acc: 0.7865 - val_loss: 0.4949 - val_acc: 0.7708\n",
            "Epoch 463/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4310 - acc: 0.7865 - val_loss: 0.4949 - val_acc: 0.7708\n",
            "Epoch 464/1000\n",
            "576/576 [==============================] - 0s 39us/step - loss: 0.4309 - acc: 0.7882 - val_loss: 0.4950 - val_acc: 0.7708\n",
            "Epoch 465/1000\n",
            "576/576 [==============================] - 0s 38us/step - loss: 0.4309 - acc: 0.7882 - val_loss: 0.4950 - val_acc: 0.7656\n",
            "Epoch 466/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4308 - acc: 0.7882 - val_loss: 0.4950 - val_acc: 0.7656\n",
            "Epoch 467/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4308 - acc: 0.7882 - val_loss: 0.4950 - val_acc: 0.7656\n",
            "Epoch 468/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4308 - acc: 0.7882 - val_loss: 0.4950 - val_acc: 0.7656\n",
            "Epoch 469/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4307 - acc: 0.7882 - val_loss: 0.4950 - val_acc: 0.7656\n",
            "Epoch 470/1000\n",
            "576/576 [==============================] - 0s 39us/step - loss: 0.4306 - acc: 0.7882 - val_loss: 0.4950 - val_acc: 0.7656\n",
            "Epoch 471/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4306 - acc: 0.7882 - val_loss: 0.4950 - val_acc: 0.7656\n",
            "Epoch 472/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4306 - acc: 0.7882 - val_loss: 0.4951 - val_acc: 0.7656\n",
            "Epoch 473/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4306 - acc: 0.7882 - val_loss: 0.4951 - val_acc: 0.7656\n",
            "Epoch 474/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4305 - acc: 0.7882 - val_loss: 0.4951 - val_acc: 0.7656\n",
            "Epoch 475/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4305 - acc: 0.7882 - val_loss: 0.4951 - val_acc: 0.7656\n",
            "Epoch 476/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4304 - acc: 0.7882 - val_loss: 0.4951 - val_acc: 0.7656\n",
            "Epoch 477/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4304 - acc: 0.7882 - val_loss: 0.4951 - val_acc: 0.7656\n",
            "Epoch 478/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4303 - acc: 0.7865 - val_loss: 0.4951 - val_acc: 0.7656\n",
            "Epoch 479/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4303 - acc: 0.7865 - val_loss: 0.4951 - val_acc: 0.7656\n",
            "Epoch 480/1000\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4303 - acc: 0.7847 - val_loss: 0.4951 - val_acc: 0.7656\n",
            "Epoch 481/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4302 - acc: 0.7847 - val_loss: 0.4952 - val_acc: 0.7656\n",
            "Epoch 482/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4302 - acc: 0.7865 - val_loss: 0.4952 - val_acc: 0.7656\n",
            "Epoch 483/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4302 - acc: 0.7865 - val_loss: 0.4952 - val_acc: 0.7656\n",
            "Epoch 484/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4301 - acc: 0.7847 - val_loss: 0.4952 - val_acc: 0.7656\n",
            "Epoch 485/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4301 - acc: 0.7865 - val_loss: 0.4952 - val_acc: 0.7656\n",
            "Epoch 486/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4300 - acc: 0.7847 - val_loss: 0.4952 - val_acc: 0.7656\n",
            "Epoch 487/1000\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4300 - acc: 0.7847 - val_loss: 0.4952 - val_acc: 0.7656\n",
            "Epoch 488/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4300 - acc: 0.7847 - val_loss: 0.4952 - val_acc: 0.7656\n",
            "Epoch 489/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4299 - acc: 0.7847 - val_loss: 0.4952 - val_acc: 0.7656\n",
            "Epoch 490/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4299 - acc: 0.7847 - val_loss: 0.4952 - val_acc: 0.7656\n",
            "Epoch 491/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4298 - acc: 0.7830 - val_loss: 0.4953 - val_acc: 0.7656\n",
            "Epoch 492/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4298 - acc: 0.7847 - val_loss: 0.4953 - val_acc: 0.7656\n",
            "Epoch 493/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4297 - acc: 0.7847 - val_loss: 0.4953 - val_acc: 0.7656\n",
            "Epoch 494/1000\n",
            "576/576 [==============================] - 0s 40us/step - loss: 0.4297 - acc: 0.7847 - val_loss: 0.4953 - val_acc: 0.7656\n",
            "Epoch 495/1000\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4297 - acc: 0.7830 - val_loss: 0.4953 - val_acc: 0.7656\n",
            "Epoch 496/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4296 - acc: 0.7847 - val_loss: 0.4953 - val_acc: 0.7656\n",
            "Epoch 497/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4296 - acc: 0.7830 - val_loss: 0.4953 - val_acc: 0.7656\n",
            "Epoch 498/1000\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4295 - acc: 0.7830 - val_loss: 0.4953 - val_acc: 0.7656\n",
            "Epoch 499/1000\n",
            "576/576 [==============================] - 0s 59us/step - loss: 0.4295 - acc: 0.7847 - val_loss: 0.4953 - val_acc: 0.7656\n",
            "Epoch 500/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4295 - acc: 0.7847 - val_loss: 0.4953 - val_acc: 0.7656\n",
            "Epoch 501/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4294 - acc: 0.7847 - val_loss: 0.4953 - val_acc: 0.7656\n",
            "Epoch 502/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4293 - acc: 0.7847 - val_loss: 0.4953 - val_acc: 0.7656\n",
            "Epoch 503/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4293 - acc: 0.7847 - val_loss: 0.4954 - val_acc: 0.7656\n",
            "Epoch 504/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4293 - acc: 0.7847 - val_loss: 0.4954 - val_acc: 0.7656\n",
            "Epoch 505/1000\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4292 - acc: 0.7847 - val_loss: 0.4954 - val_acc: 0.7656\n",
            "Epoch 506/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4292 - acc: 0.7830 - val_loss: 0.4954 - val_acc: 0.7656\n",
            "Epoch 507/1000\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4291 - acc: 0.7847 - val_loss: 0.4954 - val_acc: 0.7656\n",
            "Epoch 508/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4291 - acc: 0.7830 - val_loss: 0.4954 - val_acc: 0.7656\n",
            "Epoch 509/1000\n",
            "576/576 [==============================] - 0s 39us/step - loss: 0.4291 - acc: 0.7830 - val_loss: 0.4954 - val_acc: 0.7656\n",
            "Epoch 510/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4290 - acc: 0.7865 - val_loss: 0.4954 - val_acc: 0.7656\n",
            "Epoch 511/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4290 - acc: 0.7847 - val_loss: 0.4954 - val_acc: 0.7604\n",
            "Epoch 512/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4289 - acc: 0.7847 - val_loss: 0.4954 - val_acc: 0.7604\n",
            "Epoch 513/1000\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4289 - acc: 0.7847 - val_loss: 0.4955 - val_acc: 0.7604\n",
            "Epoch 514/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4289 - acc: 0.7865 - val_loss: 0.4955 - val_acc: 0.7604\n",
            "Epoch 515/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4288 - acc: 0.7847 - val_loss: 0.4955 - val_acc: 0.7604\n",
            "Epoch 516/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4288 - acc: 0.7865 - val_loss: 0.4955 - val_acc: 0.7604\n",
            "Epoch 517/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4288 - acc: 0.7865 - val_loss: 0.4955 - val_acc: 0.7604\n",
            "Epoch 518/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4287 - acc: 0.7865 - val_loss: 0.4955 - val_acc: 0.7604\n",
            "Epoch 519/1000\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4287 - acc: 0.7865 - val_loss: 0.4955 - val_acc: 0.7604\n",
            "Epoch 520/1000\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.4287 - acc: 0.7865 - val_loss: 0.4955 - val_acc: 0.7604\n",
            "Epoch 521/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4286 - acc: 0.7865 - val_loss: 0.4955 - val_acc: 0.7604\n",
            "Epoch 522/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4285 - acc: 0.7847 - val_loss: 0.4955 - val_acc: 0.7604\n",
            "Epoch 523/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4285 - acc: 0.7865 - val_loss: 0.4956 - val_acc: 0.7604\n",
            "Epoch 524/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4284 - acc: 0.7865 - val_loss: 0.4956 - val_acc: 0.7604\n",
            "Epoch 525/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4284 - acc: 0.7865 - val_loss: 0.4956 - val_acc: 0.7604\n",
            "Epoch 526/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4284 - acc: 0.7865 - val_loss: 0.4956 - val_acc: 0.7604\n",
            "Epoch 527/1000\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4283 - acc: 0.7865 - val_loss: 0.4956 - val_acc: 0.7604\n",
            "Epoch 528/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4283 - acc: 0.7847 - val_loss: 0.4956 - val_acc: 0.7604\n",
            "Epoch 529/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4283 - acc: 0.7865 - val_loss: 0.4956 - val_acc: 0.7604\n",
            "Epoch 530/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4282 - acc: 0.7865 - val_loss: 0.4956 - val_acc: 0.7604\n",
            "Epoch 531/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4282 - acc: 0.7847 - val_loss: 0.4956 - val_acc: 0.7604\n",
            "Epoch 532/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4282 - acc: 0.7865 - val_loss: 0.4957 - val_acc: 0.7604\n",
            "Epoch 533/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4281 - acc: 0.7847 - val_loss: 0.4957 - val_acc: 0.7604\n",
            "Epoch 534/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4281 - acc: 0.7865 - val_loss: 0.4957 - val_acc: 0.7604\n",
            "Epoch 535/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4280 - acc: 0.7830 - val_loss: 0.4957 - val_acc: 0.7604\n",
            "Epoch 536/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4280 - acc: 0.7865 - val_loss: 0.4957 - val_acc: 0.7604\n",
            "Epoch 537/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4280 - acc: 0.7865 - val_loss: 0.4957 - val_acc: 0.7604\n",
            "Epoch 538/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4279 - acc: 0.7865 - val_loss: 0.4957 - val_acc: 0.7604\n",
            "Epoch 539/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4279 - acc: 0.7847 - val_loss: 0.4957 - val_acc: 0.7604\n",
            "Epoch 540/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4278 - acc: 0.7865 - val_loss: 0.4958 - val_acc: 0.7604\n",
            "Epoch 541/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4278 - acc: 0.7865 - val_loss: 0.4958 - val_acc: 0.7604\n",
            "Epoch 542/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4278 - acc: 0.7865 - val_loss: 0.4958 - val_acc: 0.7604\n",
            "Epoch 543/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4277 - acc: 0.7882 - val_loss: 0.4958 - val_acc: 0.7604\n",
            "Epoch 544/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4277 - acc: 0.7865 - val_loss: 0.4958 - val_acc: 0.7604\n",
            "Epoch 545/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4277 - acc: 0.7847 - val_loss: 0.4958 - val_acc: 0.7604\n",
            "Epoch 546/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4276 - acc: 0.7847 - val_loss: 0.4958 - val_acc: 0.7604\n",
            "Epoch 547/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4276 - acc: 0.7865 - val_loss: 0.4958 - val_acc: 0.7604\n",
            "Epoch 548/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4275 - acc: 0.7847 - val_loss: 0.4958 - val_acc: 0.7604\n",
            "Epoch 549/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4275 - acc: 0.7847 - val_loss: 0.4958 - val_acc: 0.7604\n",
            "Epoch 550/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4275 - acc: 0.7865 - val_loss: 0.4958 - val_acc: 0.7604\n",
            "Epoch 551/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4274 - acc: 0.7882 - val_loss: 0.4958 - val_acc: 0.7604\n",
            "Epoch 552/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4274 - acc: 0.7865 - val_loss: 0.4959 - val_acc: 0.7604\n",
            "Epoch 553/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4274 - acc: 0.7865 - val_loss: 0.4959 - val_acc: 0.7604\n",
            "Epoch 554/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4273 - acc: 0.7882 - val_loss: 0.4959 - val_acc: 0.7604\n",
            "Epoch 555/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4273 - acc: 0.7830 - val_loss: 0.4959 - val_acc: 0.7604\n",
            "Epoch 556/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4273 - acc: 0.7847 - val_loss: 0.4959 - val_acc: 0.7604\n",
            "Epoch 557/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4272 - acc: 0.7830 - val_loss: 0.4959 - val_acc: 0.7604\n",
            "Epoch 558/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4272 - acc: 0.7865 - val_loss: 0.4959 - val_acc: 0.7604\n",
            "Epoch 559/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4272 - acc: 0.7830 - val_loss: 0.4959 - val_acc: 0.7604\n",
            "Epoch 560/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4271 - acc: 0.7830 - val_loss: 0.4959 - val_acc: 0.7604\n",
            "Epoch 561/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4270 - acc: 0.7847 - val_loss: 0.4959 - val_acc: 0.7604\n",
            "Epoch 562/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4270 - acc: 0.7847 - val_loss: 0.4959 - val_acc: 0.7604\n",
            "Epoch 563/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4270 - acc: 0.7847 - val_loss: 0.4959 - val_acc: 0.7604\n",
            "Epoch 564/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4270 - acc: 0.7830 - val_loss: 0.4959 - val_acc: 0.7604\n",
            "Epoch 565/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4269 - acc: 0.7847 - val_loss: 0.4959 - val_acc: 0.7604\n",
            "Epoch 566/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4269 - acc: 0.7847 - val_loss: 0.4959 - val_acc: 0.7604\n",
            "Epoch 567/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4269 - acc: 0.7830 - val_loss: 0.4959 - val_acc: 0.7604\n",
            "Epoch 568/1000\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4268 - acc: 0.7830 - val_loss: 0.4959 - val_acc: 0.7604\n",
            "Epoch 569/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4268 - acc: 0.7847 - val_loss: 0.4959 - val_acc: 0.7656\n",
            "Epoch 570/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4268 - acc: 0.7847 - val_loss: 0.4960 - val_acc: 0.7656\n",
            "Epoch 571/1000\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4267 - acc: 0.7847 - val_loss: 0.4960 - val_acc: 0.7656\n",
            "Epoch 572/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4267 - acc: 0.7830 - val_loss: 0.4960 - val_acc: 0.7656\n",
            "Epoch 573/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4267 - acc: 0.7830 - val_loss: 0.4960 - val_acc: 0.7656\n",
            "Epoch 574/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4266 - acc: 0.7830 - val_loss: 0.4960 - val_acc: 0.7656\n",
            "Epoch 575/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4266 - acc: 0.7847 - val_loss: 0.4960 - val_acc: 0.7656\n",
            "Epoch 576/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4266 - acc: 0.7830 - val_loss: 0.4960 - val_acc: 0.7656\n",
            "Epoch 577/1000\n",
            "576/576 [==============================] - 0s 40us/step - loss: 0.4265 - acc: 0.7847 - val_loss: 0.4960 - val_acc: 0.7656\n",
            "Epoch 578/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4265 - acc: 0.7830 - val_loss: 0.4960 - val_acc: 0.7656\n",
            "Epoch 579/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4264 - acc: 0.7830 - val_loss: 0.4960 - val_acc: 0.7656\n",
            "Epoch 580/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4264 - acc: 0.7830 - val_loss: 0.4960 - val_acc: 0.7656\n",
            "Epoch 581/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4264 - acc: 0.7847 - val_loss: 0.4960 - val_acc: 0.7656\n",
            "Epoch 582/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4263 - acc: 0.7830 - val_loss: 0.4960 - val_acc: 0.7656\n",
            "Epoch 583/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4263 - acc: 0.7847 - val_loss: 0.4960 - val_acc: 0.7656\n",
            "Epoch 584/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4263 - acc: 0.7847 - val_loss: 0.4960 - val_acc: 0.7656\n",
            "Epoch 585/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4262 - acc: 0.7865 - val_loss: 0.4960 - val_acc: 0.7656\n",
            "Epoch 586/1000\n",
            "576/576 [==============================] - 0s 40us/step - loss: 0.4262 - acc: 0.7847 - val_loss: 0.4960 - val_acc: 0.7656\n",
            "Epoch 587/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4261 - acc: 0.7847 - val_loss: 0.4960 - val_acc: 0.7656\n",
            "Epoch 588/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4261 - acc: 0.7865 - val_loss: 0.4960 - val_acc: 0.7656\n",
            "Epoch 589/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4261 - acc: 0.7865 - val_loss: 0.4960 - val_acc: 0.7656\n",
            "Epoch 590/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4260 - acc: 0.7865 - val_loss: 0.4960 - val_acc: 0.7656\n",
            "Epoch 591/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4260 - acc: 0.7882 - val_loss: 0.4961 - val_acc: 0.7656\n",
            "Epoch 592/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4260 - acc: 0.7865 - val_loss: 0.4961 - val_acc: 0.7656\n",
            "Epoch 593/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4259 - acc: 0.7865 - val_loss: 0.4961 - val_acc: 0.7656\n",
            "Epoch 594/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4259 - acc: 0.7865 - val_loss: 0.4961 - val_acc: 0.7708\n",
            "Epoch 595/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4259 - acc: 0.7865 - val_loss: 0.4961 - val_acc: 0.7708\n",
            "Epoch 596/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4258 - acc: 0.7865 - val_loss: 0.4961 - val_acc: 0.7708\n",
            "Epoch 597/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4258 - acc: 0.7865 - val_loss: 0.4961 - val_acc: 0.7708\n",
            "Epoch 598/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4258 - acc: 0.7865 - val_loss: 0.4961 - val_acc: 0.7708\n",
            "Epoch 599/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4257 - acc: 0.7865 - val_loss: 0.4961 - val_acc: 0.7708\n",
            "Epoch 600/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4257 - acc: 0.7865 - val_loss: 0.4961 - val_acc: 0.7708\n",
            "Epoch 601/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4257 - acc: 0.7865 - val_loss: 0.4961 - val_acc: 0.7708\n",
            "Epoch 602/1000\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4256 - acc: 0.7865 - val_loss: 0.4961 - val_acc: 0.7708\n",
            "Epoch 603/1000\n",
            "576/576 [==============================] - 0s 40us/step - loss: 0.4256 - acc: 0.7865 - val_loss: 0.4962 - val_acc: 0.7708\n",
            "Epoch 604/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4255 - acc: 0.7865 - val_loss: 0.4962 - val_acc: 0.7656\n",
            "Epoch 605/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4255 - acc: 0.7865 - val_loss: 0.4962 - val_acc: 0.7656\n",
            "Epoch 606/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4255 - acc: 0.7882 - val_loss: 0.4962 - val_acc: 0.7656\n",
            "Epoch 607/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4254 - acc: 0.7847 - val_loss: 0.4962 - val_acc: 0.7656\n",
            "Epoch 608/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4254 - acc: 0.7847 - val_loss: 0.4962 - val_acc: 0.7656\n",
            "Epoch 609/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4253 - acc: 0.7865 - val_loss: 0.4962 - val_acc: 0.7656\n",
            "Epoch 610/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4253 - acc: 0.7882 - val_loss: 0.4962 - val_acc: 0.7656\n",
            "Epoch 611/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4253 - acc: 0.7847 - val_loss: 0.4962 - val_acc: 0.7656\n",
            "Epoch 612/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4252 - acc: 0.7882 - val_loss: 0.4962 - val_acc: 0.7656\n",
            "Epoch 613/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4252 - acc: 0.7882 - val_loss: 0.4962 - val_acc: 0.7656\n",
            "Epoch 614/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4252 - acc: 0.7882 - val_loss: 0.4962 - val_acc: 0.7656\n",
            "Epoch 615/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4252 - acc: 0.7882 - val_loss: 0.4962 - val_acc: 0.7656\n",
            "Epoch 616/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4251 - acc: 0.7882 - val_loss: 0.4962 - val_acc: 0.7656\n",
            "Epoch 617/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4251 - acc: 0.7882 - val_loss: 0.4962 - val_acc: 0.7656\n",
            "Epoch 618/1000\n",
            "576/576 [==============================] - 0s 39us/step - loss: 0.4251 - acc: 0.7882 - val_loss: 0.4962 - val_acc: 0.7656\n",
            "Epoch 619/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4250 - acc: 0.7882 - val_loss: 0.4962 - val_acc: 0.7656\n",
            "Epoch 620/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4250 - acc: 0.7882 - val_loss: 0.4962 - val_acc: 0.7656\n",
            "Epoch 621/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4249 - acc: 0.7882 - val_loss: 0.4963 - val_acc: 0.7656\n",
            "Epoch 622/1000\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4249 - acc: 0.7882 - val_loss: 0.4963 - val_acc: 0.7656\n",
            "Epoch 623/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4249 - acc: 0.7882 - val_loss: 0.4963 - val_acc: 0.7656\n",
            "Epoch 624/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4248 - acc: 0.7865 - val_loss: 0.4963 - val_acc: 0.7656\n",
            "Epoch 625/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4248 - acc: 0.7882 - val_loss: 0.4963 - val_acc: 0.7656\n",
            "Epoch 626/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4248 - acc: 0.7882 - val_loss: 0.4963 - val_acc: 0.7656\n",
            "Epoch 627/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4247 - acc: 0.7882 - val_loss: 0.4963 - val_acc: 0.7656\n",
            "Epoch 628/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4247 - acc: 0.7865 - val_loss: 0.4963 - val_acc: 0.7656\n",
            "Epoch 629/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4246 - acc: 0.7882 - val_loss: 0.4963 - val_acc: 0.7656\n",
            "Epoch 630/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4246 - acc: 0.7865 - val_loss: 0.4963 - val_acc: 0.7656\n",
            "Epoch 631/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4246 - acc: 0.7865 - val_loss: 0.4963 - val_acc: 0.7656\n",
            "Epoch 632/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4245 - acc: 0.7882 - val_loss: 0.4963 - val_acc: 0.7656\n",
            "Epoch 633/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4245 - acc: 0.7865 - val_loss: 0.4963 - val_acc: 0.7708\n",
            "Epoch 634/1000\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4245 - acc: 0.7882 - val_loss: 0.4963 - val_acc: 0.7708\n",
            "Epoch 635/1000\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.4244 - acc: 0.7882 - val_loss: 0.4963 - val_acc: 0.7708\n",
            "Epoch 636/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4244 - acc: 0.7882 - val_loss: 0.4963 - val_acc: 0.7708\n",
            "Epoch 637/1000\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4244 - acc: 0.7882 - val_loss: 0.4963 - val_acc: 0.7708\n",
            "Epoch 638/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4243 - acc: 0.7882 - val_loss: 0.4963 - val_acc: 0.7708\n",
            "Epoch 639/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4243 - acc: 0.7882 - val_loss: 0.4964 - val_acc: 0.7708\n",
            "Epoch 640/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4243 - acc: 0.7899 - val_loss: 0.4964 - val_acc: 0.7708\n",
            "Epoch 641/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4243 - acc: 0.7882 - val_loss: 0.4964 - val_acc: 0.7708\n",
            "Epoch 642/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4242 - acc: 0.7882 - val_loss: 0.4964 - val_acc: 0.7708\n",
            "Epoch 643/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4242 - acc: 0.7882 - val_loss: 0.4964 - val_acc: 0.7708\n",
            "Epoch 644/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4241 - acc: 0.7882 - val_loss: 0.4964 - val_acc: 0.7708\n",
            "Epoch 645/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4241 - acc: 0.7882 - val_loss: 0.4964 - val_acc: 0.7708\n",
            "Epoch 646/1000\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4241 - acc: 0.7882 - val_loss: 0.4964 - val_acc: 0.7708\n",
            "Epoch 647/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4241 - acc: 0.7882 - val_loss: 0.4964 - val_acc: 0.7708\n",
            "Epoch 648/1000\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4240 - acc: 0.7882 - val_loss: 0.4964 - val_acc: 0.7708\n",
            "Epoch 649/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4240 - acc: 0.7882 - val_loss: 0.4964 - val_acc: 0.7708\n",
            "Epoch 650/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4240 - acc: 0.7882 - val_loss: 0.4964 - val_acc: 0.7708\n",
            "Epoch 651/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4239 - acc: 0.7882 - val_loss: 0.4964 - val_acc: 0.7708\n",
            "Epoch 652/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4239 - acc: 0.7882 - val_loss: 0.4964 - val_acc: 0.7708\n",
            "Epoch 653/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4238 - acc: 0.7882 - val_loss: 0.4964 - val_acc: 0.7708\n",
            "Epoch 654/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4238 - acc: 0.7882 - val_loss: 0.4964 - val_acc: 0.7708\n",
            "Epoch 655/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4238 - acc: 0.7882 - val_loss: 0.4964 - val_acc: 0.7708\n",
            "Epoch 656/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4238 - acc: 0.7882 - val_loss: 0.4964 - val_acc: 0.7708\n",
            "Epoch 657/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4237 - acc: 0.7882 - val_loss: 0.4964 - val_acc: 0.7708\n",
            "Epoch 658/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4237 - acc: 0.7882 - val_loss: 0.4964 - val_acc: 0.7708\n",
            "Epoch 659/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4237 - acc: 0.7899 - val_loss: 0.4964 - val_acc: 0.7708\n",
            "Epoch 660/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4236 - acc: 0.7899 - val_loss: 0.4964 - val_acc: 0.7708\n",
            "Epoch 661/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4236 - acc: 0.7899 - val_loss: 0.4965 - val_acc: 0.7708\n",
            "Epoch 662/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4236 - acc: 0.7899 - val_loss: 0.4965 - val_acc: 0.7708\n",
            "Epoch 663/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4235 - acc: 0.7899 - val_loss: 0.4965 - val_acc: 0.7708\n",
            "Epoch 664/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4235 - acc: 0.7899 - val_loss: 0.4965 - val_acc: 0.7708\n",
            "Epoch 665/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4235 - acc: 0.7899 - val_loss: 0.4965 - val_acc: 0.7708\n",
            "Epoch 666/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4235 - acc: 0.7899 - val_loss: 0.4965 - val_acc: 0.7708\n",
            "Epoch 667/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4234 - acc: 0.7899 - val_loss: 0.4965 - val_acc: 0.7708\n",
            "Epoch 668/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4234 - acc: 0.7899 - val_loss: 0.4965 - val_acc: 0.7708\n",
            "Epoch 669/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4233 - acc: 0.7899 - val_loss: 0.4965 - val_acc: 0.7708\n",
            "Epoch 670/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4233 - acc: 0.7899 - val_loss: 0.4965 - val_acc: 0.7708\n",
            "Epoch 671/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4233 - acc: 0.7899 - val_loss: 0.4965 - val_acc: 0.7708\n",
            "Epoch 672/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4233 - acc: 0.7899 - val_loss: 0.4965 - val_acc: 0.7708\n",
            "Epoch 673/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4232 - acc: 0.7899 - val_loss: 0.4965 - val_acc: 0.7708\n",
            "Epoch 674/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4232 - acc: 0.7899 - val_loss: 0.4965 - val_acc: 0.7708\n",
            "Epoch 675/1000\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4232 - acc: 0.7899 - val_loss: 0.4965 - val_acc: 0.7708\n",
            "Epoch 676/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4231 - acc: 0.7899 - val_loss: 0.4965 - val_acc: 0.7708\n",
            "Epoch 677/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4231 - acc: 0.7899 - val_loss: 0.4965 - val_acc: 0.7708\n",
            "Epoch 678/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4231 - acc: 0.7899 - val_loss: 0.4965 - val_acc: 0.7708\n",
            "Epoch 679/1000\n",
            "576/576 [==============================] - 0s 40us/step - loss: 0.4230 - acc: 0.7899 - val_loss: 0.4966 - val_acc: 0.7708\n",
            "Epoch 680/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4230 - acc: 0.7899 - val_loss: 0.4966 - val_acc: 0.7708\n",
            "Epoch 681/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4230 - acc: 0.7899 - val_loss: 0.4966 - val_acc: 0.7708\n",
            "Epoch 682/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4229 - acc: 0.7899 - val_loss: 0.4966 - val_acc: 0.7708\n",
            "Epoch 683/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4229 - acc: 0.7899 - val_loss: 0.4966 - val_acc: 0.7708\n",
            "Epoch 684/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4228 - acc: 0.7899 - val_loss: 0.4966 - val_acc: 0.7708\n",
            "Epoch 685/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4228 - acc: 0.7899 - val_loss: 0.4966 - val_acc: 0.7708\n",
            "Epoch 686/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4228 - acc: 0.7899 - val_loss: 0.4966 - val_acc: 0.7708\n",
            "Epoch 687/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4228 - acc: 0.7899 - val_loss: 0.4966 - val_acc: 0.7708\n",
            "Epoch 688/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4228 - acc: 0.7899 - val_loss: 0.4966 - val_acc: 0.7708\n",
            "Epoch 689/1000\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4227 - acc: 0.7899 - val_loss: 0.4966 - val_acc: 0.7708\n",
            "Epoch 690/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4227 - acc: 0.7899 - val_loss: 0.4966 - val_acc: 0.7708\n",
            "Epoch 691/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4227 - acc: 0.7899 - val_loss: 0.4966 - val_acc: 0.7708\n",
            "Epoch 692/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4226 - acc: 0.7899 - val_loss: 0.4967 - val_acc: 0.7708\n",
            "Epoch 693/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4226 - acc: 0.7899 - val_loss: 0.4967 - val_acc: 0.7708\n",
            "Epoch 694/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4226 - acc: 0.7899 - val_loss: 0.4967 - val_acc: 0.7708\n",
            "Epoch 695/1000\n",
            "576/576 [==============================] - 0s 40us/step - loss: 0.4226 - acc: 0.7899 - val_loss: 0.4967 - val_acc: 0.7708\n",
            "Epoch 696/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4225 - acc: 0.7899 - val_loss: 0.4967 - val_acc: 0.7708\n",
            "Epoch 697/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4225 - acc: 0.7899 - val_loss: 0.4967 - val_acc: 0.7708\n",
            "Epoch 698/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4225 - acc: 0.7917 - val_loss: 0.4967 - val_acc: 0.7708\n",
            "Epoch 699/1000\n",
            "576/576 [==============================] - 0s 39us/step - loss: 0.4224 - acc: 0.7899 - val_loss: 0.4967 - val_acc: 0.7708\n",
            "Epoch 700/1000\n",
            "576/576 [==============================] - 0s 40us/step - loss: 0.4224 - acc: 0.7899 - val_loss: 0.4967 - val_acc: 0.7708\n",
            "Epoch 701/1000\n",
            "576/576 [==============================] - 0s 40us/step - loss: 0.4223 - acc: 0.7917 - val_loss: 0.4967 - val_acc: 0.7708\n",
            "Epoch 702/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4223 - acc: 0.7917 - val_loss: 0.4967 - val_acc: 0.7708\n",
            "Epoch 703/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4223 - acc: 0.7917 - val_loss: 0.4967 - val_acc: 0.7708\n",
            "Epoch 704/1000\n",
            "576/576 [==============================] - 0s 60us/step - loss: 0.4223 - acc: 0.7899 - val_loss: 0.4967 - val_acc: 0.7708\n",
            "Epoch 705/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4222 - acc: 0.7899 - val_loss: 0.4967 - val_acc: 0.7708\n",
            "Epoch 706/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4222 - acc: 0.7917 - val_loss: 0.4967 - val_acc: 0.7708\n",
            "Epoch 707/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4222 - acc: 0.7917 - val_loss: 0.4967 - val_acc: 0.7708\n",
            "Epoch 708/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4222 - acc: 0.7917 - val_loss: 0.4968 - val_acc: 0.7708\n",
            "Epoch 709/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4221 - acc: 0.7917 - val_loss: 0.4968 - val_acc: 0.7708\n",
            "Epoch 710/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4221 - acc: 0.7917 - val_loss: 0.4968 - val_acc: 0.7708\n",
            "Epoch 711/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4220 - acc: 0.7917 - val_loss: 0.4968 - val_acc: 0.7708\n",
            "Epoch 712/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4220 - acc: 0.7917 - val_loss: 0.4968 - val_acc: 0.7708\n",
            "Epoch 713/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4220 - acc: 0.7917 - val_loss: 0.4968 - val_acc: 0.7708\n",
            "Epoch 714/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4220 - acc: 0.7917 - val_loss: 0.4968 - val_acc: 0.7708\n",
            "Epoch 715/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4219 - acc: 0.7917 - val_loss: 0.4968 - val_acc: 0.7708\n",
            "Epoch 716/1000\n",
            "576/576 [==============================] - 0s 40us/step - loss: 0.4219 - acc: 0.7917 - val_loss: 0.4968 - val_acc: 0.7708\n",
            "Epoch 717/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4218 - acc: 0.7917 - val_loss: 0.4968 - val_acc: 0.7708\n",
            "Epoch 718/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4218 - acc: 0.7917 - val_loss: 0.4968 - val_acc: 0.7708\n",
            "Epoch 719/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4218 - acc: 0.7917 - val_loss: 0.4968 - val_acc: 0.7708\n",
            "Epoch 720/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4218 - acc: 0.7917 - val_loss: 0.4968 - val_acc: 0.7708\n",
            "Epoch 721/1000\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4218 - acc: 0.7934 - val_loss: 0.4968 - val_acc: 0.7708\n",
            "Epoch 722/1000\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4217 - acc: 0.7917 - val_loss: 0.4968 - val_acc: 0.7708\n",
            "Epoch 723/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4217 - acc: 0.7934 - val_loss: 0.4969 - val_acc: 0.7708\n",
            "Epoch 724/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4217 - acc: 0.7934 - val_loss: 0.4969 - val_acc: 0.7708\n",
            "Epoch 725/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4216 - acc: 0.7934 - val_loss: 0.4969 - val_acc: 0.7708\n",
            "Epoch 726/1000\n",
            "576/576 [==============================] - 0s 68us/step - loss: 0.4216 - acc: 0.7917 - val_loss: 0.4969 - val_acc: 0.7708\n",
            "Epoch 727/1000\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4216 - acc: 0.7934 - val_loss: 0.4969 - val_acc: 0.7708\n",
            "Epoch 728/1000\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4215 - acc: 0.7934 - val_loss: 0.4969 - val_acc: 0.7708\n",
            "Epoch 729/1000\n",
            "576/576 [==============================] - 0s 59us/step - loss: 0.4215 - acc: 0.7934 - val_loss: 0.4969 - val_acc: 0.7708\n",
            "Epoch 730/1000\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4215 - acc: 0.7934 - val_loss: 0.4969 - val_acc: 0.7708\n",
            "Epoch 731/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4215 - acc: 0.7934 - val_loss: 0.4969 - val_acc: 0.7708\n",
            "Epoch 732/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4214 - acc: 0.7934 - val_loss: 0.4969 - val_acc: 0.7708\n",
            "Epoch 733/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4214 - acc: 0.7934 - val_loss: 0.4969 - val_acc: 0.7708\n",
            "Epoch 734/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4214 - acc: 0.7934 - val_loss: 0.4970 - val_acc: 0.7708\n",
            "Epoch 735/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4213 - acc: 0.7934 - val_loss: 0.4970 - val_acc: 0.7708\n",
            "Epoch 736/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4213 - acc: 0.7934 - val_loss: 0.4970 - val_acc: 0.7708\n",
            "Epoch 737/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4213 - acc: 0.7934 - val_loss: 0.4970 - val_acc: 0.7708\n",
            "Epoch 738/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4213 - acc: 0.7934 - val_loss: 0.4970 - val_acc: 0.7708\n",
            "Epoch 739/1000\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4213 - acc: 0.7934 - val_loss: 0.4970 - val_acc: 0.7708\n",
            "Epoch 740/1000\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4212 - acc: 0.7934 - val_loss: 0.4970 - val_acc: 0.7708\n",
            "Epoch 741/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4211 - acc: 0.7934 - val_loss: 0.4970 - val_acc: 0.7708\n",
            "Epoch 742/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4211 - acc: 0.7934 - val_loss: 0.4970 - val_acc: 0.7708\n",
            "Epoch 743/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4211 - acc: 0.7934 - val_loss: 0.4970 - val_acc: 0.7708\n",
            "Epoch 744/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4211 - acc: 0.7934 - val_loss: 0.4970 - val_acc: 0.7708\n",
            "Epoch 745/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4211 - acc: 0.7934 - val_loss: 0.4970 - val_acc: 0.7708\n",
            "Epoch 746/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4210 - acc: 0.7934 - val_loss: 0.4970 - val_acc: 0.7708\n",
            "Epoch 747/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4210 - acc: 0.7934 - val_loss: 0.4971 - val_acc: 0.7708\n",
            "Epoch 748/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4210 - acc: 0.7934 - val_loss: 0.4971 - val_acc: 0.7708\n",
            "Epoch 749/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4209 - acc: 0.7934 - val_loss: 0.4971 - val_acc: 0.7708\n",
            "Epoch 750/1000\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4209 - acc: 0.7934 - val_loss: 0.4971 - val_acc: 0.7708\n",
            "Epoch 751/1000\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4209 - acc: 0.7934 - val_loss: 0.4971 - val_acc: 0.7708\n",
            "Epoch 752/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4209 - acc: 0.7934 - val_loss: 0.4971 - val_acc: 0.7708\n",
            "Epoch 753/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4208 - acc: 0.7934 - val_loss: 0.4971 - val_acc: 0.7708\n",
            "Epoch 754/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4208 - acc: 0.7934 - val_loss: 0.4971 - val_acc: 0.7708\n",
            "Epoch 755/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4208 - acc: 0.7934 - val_loss: 0.4971 - val_acc: 0.7708\n",
            "Epoch 756/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4207 - acc: 0.7934 - val_loss: 0.4971 - val_acc: 0.7708\n",
            "Epoch 757/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4207 - acc: 0.7934 - val_loss: 0.4972 - val_acc: 0.7708\n",
            "Epoch 758/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4207 - acc: 0.7934 - val_loss: 0.4972 - val_acc: 0.7708\n",
            "Epoch 759/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4207 - acc: 0.7934 - val_loss: 0.4972 - val_acc: 0.7708\n",
            "Epoch 760/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4206 - acc: 0.7934 - val_loss: 0.4972 - val_acc: 0.7708\n",
            "Epoch 761/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4206 - acc: 0.7934 - val_loss: 0.4972 - val_acc: 0.7708\n",
            "Epoch 762/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4206 - acc: 0.7934 - val_loss: 0.4972 - val_acc: 0.7708\n",
            "Epoch 763/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4206 - acc: 0.7934 - val_loss: 0.4972 - val_acc: 0.7708\n",
            "Epoch 764/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4205 - acc: 0.7934 - val_loss: 0.4972 - val_acc: 0.7708\n",
            "Epoch 765/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4205 - acc: 0.7934 - val_loss: 0.4972 - val_acc: 0.7708\n",
            "Epoch 766/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4205 - acc: 0.7934 - val_loss: 0.4972 - val_acc: 0.7708\n",
            "Epoch 767/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4204 - acc: 0.7934 - val_loss: 0.4972 - val_acc: 0.7708\n",
            "Epoch 768/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4204 - acc: 0.7934 - val_loss: 0.4973 - val_acc: 0.7708\n",
            "Epoch 769/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4204 - acc: 0.7934 - val_loss: 0.4973 - val_acc: 0.7708\n",
            "Epoch 770/1000\n",
            "576/576 [==============================] - 0s 60us/step - loss: 0.4204 - acc: 0.7934 - val_loss: 0.4973 - val_acc: 0.7708\n",
            "Epoch 771/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4203 - acc: 0.7934 - val_loss: 0.4973 - val_acc: 0.7708\n",
            "Epoch 772/1000\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4203 - acc: 0.7934 - val_loss: 0.4973 - val_acc: 0.7708\n",
            "Epoch 773/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4203 - acc: 0.7934 - val_loss: 0.4973 - val_acc: 0.7708\n",
            "Epoch 774/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4202 - acc: 0.7934 - val_loss: 0.4973 - val_acc: 0.7708\n",
            "Epoch 775/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4202 - acc: 0.7934 - val_loss: 0.4973 - val_acc: 0.7708\n",
            "Epoch 776/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4202 - acc: 0.7934 - val_loss: 0.4973 - val_acc: 0.7708\n",
            "Epoch 777/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4201 - acc: 0.7934 - val_loss: 0.4973 - val_acc: 0.7708\n",
            "Epoch 778/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4201 - acc: 0.7934 - val_loss: 0.4974 - val_acc: 0.7708\n",
            "Epoch 779/1000\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4201 - acc: 0.7934 - val_loss: 0.4974 - val_acc: 0.7708\n",
            "Epoch 780/1000\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.4200 - acc: 0.7934 - val_loss: 0.4974 - val_acc: 0.7708\n",
            "Epoch 781/1000\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4200 - acc: 0.7934 - val_loss: 0.4974 - val_acc: 0.7656\n",
            "Epoch 782/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4200 - acc: 0.7934 - val_loss: 0.4974 - val_acc: 0.7656\n",
            "Epoch 783/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4200 - acc: 0.7934 - val_loss: 0.4974 - val_acc: 0.7656\n",
            "Epoch 784/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4199 - acc: 0.7934 - val_loss: 0.4974 - val_acc: 0.7656\n",
            "Epoch 785/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4199 - acc: 0.7934 - val_loss: 0.4974 - val_acc: 0.7656\n",
            "Epoch 786/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4199 - acc: 0.7934 - val_loss: 0.4974 - val_acc: 0.7656\n",
            "Epoch 787/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4198 - acc: 0.7934 - val_loss: 0.4975 - val_acc: 0.7656\n",
            "Epoch 788/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4198 - acc: 0.7934 - val_loss: 0.4975 - val_acc: 0.7656\n",
            "Epoch 789/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4198 - acc: 0.7934 - val_loss: 0.4975 - val_acc: 0.7656\n",
            "Epoch 790/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4197 - acc: 0.7934 - val_loss: 0.4975 - val_acc: 0.7656\n",
            "Epoch 791/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4197 - acc: 0.7934 - val_loss: 0.4975 - val_acc: 0.7656\n",
            "Epoch 792/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4197 - acc: 0.7934 - val_loss: 0.4975 - val_acc: 0.7656\n",
            "Epoch 793/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4197 - acc: 0.7934 - val_loss: 0.4975 - val_acc: 0.7656\n",
            "Epoch 794/1000\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4196 - acc: 0.7934 - val_loss: 0.4976 - val_acc: 0.7656\n",
            "Epoch 795/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4196 - acc: 0.7934 - val_loss: 0.4976 - val_acc: 0.7656\n",
            "Epoch 796/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4196 - acc: 0.7934 - val_loss: 0.4976 - val_acc: 0.7656\n",
            "Epoch 797/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4195 - acc: 0.7934 - val_loss: 0.4976 - val_acc: 0.7656\n",
            "Epoch 798/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4195 - acc: 0.7934 - val_loss: 0.4976 - val_acc: 0.7656\n",
            "Epoch 799/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4195 - acc: 0.7934 - val_loss: 0.4976 - val_acc: 0.7656\n",
            "Epoch 800/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4195 - acc: 0.7934 - val_loss: 0.4976 - val_acc: 0.7656\n",
            "Epoch 801/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4194 - acc: 0.7934 - val_loss: 0.4976 - val_acc: 0.7656\n",
            "Epoch 802/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4194 - acc: 0.7934 - val_loss: 0.4977 - val_acc: 0.7656\n",
            "Epoch 803/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4194 - acc: 0.7934 - val_loss: 0.4977 - val_acc: 0.7656\n",
            "Epoch 804/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4193 - acc: 0.7934 - val_loss: 0.4977 - val_acc: 0.7656\n",
            "Epoch 805/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4193 - acc: 0.7934 - val_loss: 0.4977 - val_acc: 0.7656\n",
            "Epoch 806/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4192 - acc: 0.7934 - val_loss: 0.4977 - val_acc: 0.7656\n",
            "Epoch 807/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4193 - acc: 0.7934 - val_loss: 0.4977 - val_acc: 0.7656\n",
            "Epoch 808/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4192 - acc: 0.7934 - val_loss: 0.4977 - val_acc: 0.7656\n",
            "Epoch 809/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4192 - acc: 0.7934 - val_loss: 0.4977 - val_acc: 0.7656\n",
            "Epoch 810/1000\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4192 - acc: 0.7934 - val_loss: 0.4978 - val_acc: 0.7656\n",
            "Epoch 811/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4191 - acc: 0.7934 - val_loss: 0.4978 - val_acc: 0.7656\n",
            "Epoch 812/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4191 - acc: 0.7934 - val_loss: 0.4978 - val_acc: 0.7656\n",
            "Epoch 813/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4191 - acc: 0.7934 - val_loss: 0.4978 - val_acc: 0.7656\n",
            "Epoch 814/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4191 - acc: 0.7934 - val_loss: 0.4978 - val_acc: 0.7656\n",
            "Epoch 815/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4190 - acc: 0.7934 - val_loss: 0.4978 - val_acc: 0.7656\n",
            "Epoch 816/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4190 - acc: 0.7934 - val_loss: 0.4978 - val_acc: 0.7656\n",
            "Epoch 817/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4190 - acc: 0.7934 - val_loss: 0.4979 - val_acc: 0.7656\n",
            "Epoch 818/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4189 - acc: 0.7934 - val_loss: 0.4979 - val_acc: 0.7656\n",
            "Epoch 819/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4189 - acc: 0.7934 - val_loss: 0.4979 - val_acc: 0.7656\n",
            "Epoch 820/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4189 - acc: 0.7934 - val_loss: 0.4979 - val_acc: 0.7656\n",
            "Epoch 821/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4188 - acc: 0.7934 - val_loss: 0.4979 - val_acc: 0.7656\n",
            "Epoch 822/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4188 - acc: 0.7934 - val_loss: 0.4979 - val_acc: 0.7656\n",
            "Epoch 823/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4188 - acc: 0.7934 - val_loss: 0.4979 - val_acc: 0.7656\n",
            "Epoch 824/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4188 - acc: 0.7934 - val_loss: 0.4979 - val_acc: 0.7656\n",
            "Epoch 825/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4187 - acc: 0.7934 - val_loss: 0.4980 - val_acc: 0.7656\n",
            "Epoch 826/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4187 - acc: 0.7934 - val_loss: 0.4980 - val_acc: 0.7656\n",
            "Epoch 827/1000\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4187 - acc: 0.7934 - val_loss: 0.4980 - val_acc: 0.7656\n",
            "Epoch 828/1000\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4186 - acc: 0.7934 - val_loss: 0.4980 - val_acc: 0.7656\n",
            "Epoch 829/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4186 - acc: 0.7934 - val_loss: 0.4980 - val_acc: 0.7656\n",
            "Epoch 830/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4186 - acc: 0.7934 - val_loss: 0.4980 - val_acc: 0.7656\n",
            "Epoch 831/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4186 - acc: 0.7934 - val_loss: 0.4980 - val_acc: 0.7656\n",
            "Epoch 832/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4185 - acc: 0.7934 - val_loss: 0.4980 - val_acc: 0.7656\n",
            "Epoch 833/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4185 - acc: 0.7934 - val_loss: 0.4980 - val_acc: 0.7656\n",
            "Epoch 834/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4185 - acc: 0.7934 - val_loss: 0.4981 - val_acc: 0.7656\n",
            "Epoch 835/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4184 - acc: 0.7934 - val_loss: 0.4981 - val_acc: 0.7656\n",
            "Epoch 836/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4184 - acc: 0.7934 - val_loss: 0.4981 - val_acc: 0.7656\n",
            "Epoch 837/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4184 - acc: 0.7934 - val_loss: 0.4981 - val_acc: 0.7656\n",
            "Epoch 838/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4183 - acc: 0.7934 - val_loss: 0.4981 - val_acc: 0.7656\n",
            "Epoch 839/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4183 - acc: 0.7934 - val_loss: 0.4981 - val_acc: 0.7656\n",
            "Epoch 840/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4183 - acc: 0.7934 - val_loss: 0.4982 - val_acc: 0.7656\n",
            "Epoch 841/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4183 - acc: 0.7917 - val_loss: 0.4982 - val_acc: 0.7656\n",
            "Epoch 842/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4182 - acc: 0.7934 - val_loss: 0.4982 - val_acc: 0.7656\n",
            "Epoch 843/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4182 - acc: 0.7917 - val_loss: 0.4982 - val_acc: 0.7656\n",
            "Epoch 844/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4181 - acc: 0.7917 - val_loss: 0.4982 - val_acc: 0.7656\n",
            "Epoch 845/1000\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4181 - acc: 0.7934 - val_loss: 0.4982 - val_acc: 0.7656\n",
            "Epoch 846/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4181 - acc: 0.7934 - val_loss: 0.4983 - val_acc: 0.7656\n",
            "Epoch 847/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4180 - acc: 0.7934 - val_loss: 0.4983 - val_acc: 0.7656\n",
            "Epoch 848/1000\n",
            "576/576 [==============================] - 0s 59us/step - loss: 0.4180 - acc: 0.7934 - val_loss: 0.4983 - val_acc: 0.7656\n",
            "Epoch 849/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4180 - acc: 0.7934 - val_loss: 0.4983 - val_acc: 0.7656\n",
            "Epoch 850/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4179 - acc: 0.7934 - val_loss: 0.4983 - val_acc: 0.7656\n",
            "Epoch 851/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4179 - acc: 0.7917 - val_loss: 0.4983 - val_acc: 0.7656\n",
            "Epoch 852/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4179 - acc: 0.7917 - val_loss: 0.4984 - val_acc: 0.7656\n",
            "Epoch 853/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4178 - acc: 0.7917 - val_loss: 0.4984 - val_acc: 0.7656\n",
            "Epoch 854/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4178 - acc: 0.7934 - val_loss: 0.4984 - val_acc: 0.7656\n",
            "Epoch 855/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4178 - acc: 0.7917 - val_loss: 0.4984 - val_acc: 0.7656\n",
            "Epoch 856/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4178 - acc: 0.7917 - val_loss: 0.4984 - val_acc: 0.7656\n",
            "Epoch 857/1000\n",
            "576/576 [==============================] - 0s 40us/step - loss: 0.4177 - acc: 0.7917 - val_loss: 0.4984 - val_acc: 0.7656\n",
            "Epoch 858/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4177 - acc: 0.7917 - val_loss: 0.4985 - val_acc: 0.7656\n",
            "Epoch 859/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4177 - acc: 0.7917 - val_loss: 0.4985 - val_acc: 0.7656\n",
            "Epoch 860/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4177 - acc: 0.7917 - val_loss: 0.4985 - val_acc: 0.7656\n",
            "Epoch 861/1000\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4176 - acc: 0.7917 - val_loss: 0.4985 - val_acc: 0.7656\n",
            "Epoch 862/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4176 - acc: 0.7917 - val_loss: 0.4985 - val_acc: 0.7656\n",
            "Epoch 863/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4175 - acc: 0.7917 - val_loss: 0.4985 - val_acc: 0.7656\n",
            "Epoch 864/1000\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4175 - acc: 0.7917 - val_loss: 0.4985 - val_acc: 0.7656\n",
            "Epoch 865/1000\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4175 - acc: 0.7917 - val_loss: 0.4986 - val_acc: 0.7656\n",
            "Epoch 866/1000\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4175 - acc: 0.7917 - val_loss: 0.4986 - val_acc: 0.7656\n",
            "Epoch 867/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4174 - acc: 0.7917 - val_loss: 0.4986 - val_acc: 0.7656\n",
            "Epoch 868/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4174 - acc: 0.7917 - val_loss: 0.4986 - val_acc: 0.7656\n",
            "Epoch 869/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4174 - acc: 0.7917 - val_loss: 0.4986 - val_acc: 0.7656\n",
            "Epoch 870/1000\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4173 - acc: 0.7917 - val_loss: 0.4986 - val_acc: 0.7656\n",
            "Epoch 871/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4173 - acc: 0.7917 - val_loss: 0.4986 - val_acc: 0.7656\n",
            "Epoch 872/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4173 - acc: 0.7917 - val_loss: 0.4987 - val_acc: 0.7656\n",
            "Epoch 873/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4172 - acc: 0.7917 - val_loss: 0.4987 - val_acc: 0.7656\n",
            "Epoch 874/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4172 - acc: 0.7917 - val_loss: 0.4987 - val_acc: 0.7656\n",
            "Epoch 875/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4172 - acc: 0.7917 - val_loss: 0.4987 - val_acc: 0.7656\n",
            "Epoch 876/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4172 - acc: 0.7917 - val_loss: 0.4987 - val_acc: 0.7656\n",
            "Epoch 877/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4172 - acc: 0.7917 - val_loss: 0.4987 - val_acc: 0.7656\n",
            "Epoch 878/1000\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4171 - acc: 0.7917 - val_loss: 0.4988 - val_acc: 0.7656\n",
            "Epoch 879/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4171 - acc: 0.7917 - val_loss: 0.4988 - val_acc: 0.7656\n",
            "Epoch 880/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4170 - acc: 0.7917 - val_loss: 0.4988 - val_acc: 0.7656\n",
            "Epoch 881/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4170 - acc: 0.7917 - val_loss: 0.4988 - val_acc: 0.7656\n",
            "Epoch 882/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4170 - acc: 0.7917 - val_loss: 0.4988 - val_acc: 0.7656\n",
            "Epoch 883/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4169 - acc: 0.7917 - val_loss: 0.4988 - val_acc: 0.7656\n",
            "Epoch 884/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4169 - acc: 0.7917 - val_loss: 0.4989 - val_acc: 0.7656\n",
            "Epoch 885/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4169 - acc: 0.7917 - val_loss: 0.4989 - val_acc: 0.7656\n",
            "Epoch 886/1000\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4169 - acc: 0.7917 - val_loss: 0.4989 - val_acc: 0.7656\n",
            "Epoch 887/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4168 - acc: 0.7917 - val_loss: 0.4989 - val_acc: 0.7656\n",
            "Epoch 888/1000\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4168 - acc: 0.7917 - val_loss: 0.4989 - val_acc: 0.7656\n",
            "Epoch 889/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4168 - acc: 0.7917 - val_loss: 0.4989 - val_acc: 0.7656\n",
            "Epoch 890/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4167 - acc: 0.7917 - val_loss: 0.4990 - val_acc: 0.7656\n",
            "Epoch 891/1000\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4167 - acc: 0.7917 - val_loss: 0.4990 - val_acc: 0.7656\n",
            "Epoch 892/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4167 - acc: 0.7917 - val_loss: 0.4990 - val_acc: 0.7656\n",
            "Epoch 893/1000\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4167 - acc: 0.7917 - val_loss: 0.4990 - val_acc: 0.7656\n",
            "Epoch 894/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4167 - acc: 0.7917 - val_loss: 0.4990 - val_acc: 0.7656\n",
            "Epoch 895/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4166 - acc: 0.7917 - val_loss: 0.4991 - val_acc: 0.7656\n",
            "Epoch 896/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4166 - acc: 0.7917 - val_loss: 0.4991 - val_acc: 0.7656\n",
            "Epoch 897/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4166 - acc: 0.7917 - val_loss: 0.4991 - val_acc: 0.7656\n",
            "Epoch 898/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4165 - acc: 0.7917 - val_loss: 0.4991 - val_acc: 0.7656\n",
            "Epoch 899/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4165 - acc: 0.7917 - val_loss: 0.4991 - val_acc: 0.7656\n",
            "Epoch 900/1000\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.4165 - acc: 0.7917 - val_loss: 0.4992 - val_acc: 0.7656\n",
            "Epoch 901/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4165 - acc: 0.7917 - val_loss: 0.4992 - val_acc: 0.7656\n",
            "Epoch 902/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4165 - acc: 0.7917 - val_loss: 0.4992 - val_acc: 0.7656\n",
            "Epoch 903/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4164 - acc: 0.7917 - val_loss: 0.4992 - val_acc: 0.7656\n",
            "Epoch 904/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4164 - acc: 0.7917 - val_loss: 0.4992 - val_acc: 0.7656\n",
            "Epoch 905/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4164 - acc: 0.7917 - val_loss: 0.4992 - val_acc: 0.7656\n",
            "Epoch 906/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4163 - acc: 0.7917 - val_loss: 0.4993 - val_acc: 0.7656\n",
            "Epoch 907/1000\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.4164 - acc: 0.7917 - val_loss: 0.4993 - val_acc: 0.7656\n",
            "Epoch 908/1000\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4163 - acc: 0.7917 - val_loss: 0.4993 - val_acc: 0.7656\n",
            "Epoch 909/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4163 - acc: 0.7917 - val_loss: 0.4993 - val_acc: 0.7656\n",
            "Epoch 910/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4162 - acc: 0.7917 - val_loss: 0.4993 - val_acc: 0.7656\n",
            "Epoch 911/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4162 - acc: 0.7917 - val_loss: 0.4993 - val_acc: 0.7656\n",
            "Epoch 912/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4162 - acc: 0.7917 - val_loss: 0.4993 - val_acc: 0.7656\n",
            "Epoch 913/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4161 - acc: 0.7917 - val_loss: 0.4994 - val_acc: 0.7656\n",
            "Epoch 914/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4161 - acc: 0.7917 - val_loss: 0.4994 - val_acc: 0.7656\n",
            "Epoch 915/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4161 - acc: 0.7917 - val_loss: 0.4994 - val_acc: 0.7656\n",
            "Epoch 916/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4161 - acc: 0.7934 - val_loss: 0.4994 - val_acc: 0.7656\n",
            "Epoch 917/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4161 - acc: 0.7917 - val_loss: 0.4994 - val_acc: 0.7708\n",
            "Epoch 918/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4160 - acc: 0.7917 - val_loss: 0.4994 - val_acc: 0.7708\n",
            "Epoch 919/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4160 - acc: 0.7934 - val_loss: 0.4995 - val_acc: 0.7708\n",
            "Epoch 920/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4159 - acc: 0.7934 - val_loss: 0.4995 - val_acc: 0.7708\n",
            "Epoch 921/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4159 - acc: 0.7917 - val_loss: 0.4995 - val_acc: 0.7708\n",
            "Epoch 922/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4159 - acc: 0.7934 - val_loss: 0.4995 - val_acc: 0.7708\n",
            "Epoch 923/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4159 - acc: 0.7934 - val_loss: 0.4995 - val_acc: 0.7708\n",
            "Epoch 924/1000\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4158 - acc: 0.7934 - val_loss: 0.4995 - val_acc: 0.7708\n",
            "Epoch 925/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4158 - acc: 0.7934 - val_loss: 0.4995 - val_acc: 0.7708\n",
            "Epoch 926/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4158 - acc: 0.7934 - val_loss: 0.4996 - val_acc: 0.7708\n",
            "Epoch 927/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4158 - acc: 0.7917 - val_loss: 0.4996 - val_acc: 0.7708\n",
            "Epoch 928/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4157 - acc: 0.7934 - val_loss: 0.4996 - val_acc: 0.7708\n",
            "Epoch 929/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4157 - acc: 0.7934 - val_loss: 0.4996 - val_acc: 0.7708\n",
            "Epoch 930/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4157 - acc: 0.7934 - val_loss: 0.4996 - val_acc: 0.7708\n",
            "Epoch 931/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4156 - acc: 0.7934 - val_loss: 0.4996 - val_acc: 0.7708\n",
            "Epoch 932/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4156 - acc: 0.7934 - val_loss: 0.4997 - val_acc: 0.7708\n",
            "Epoch 933/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4156 - acc: 0.7934 - val_loss: 0.4997 - val_acc: 0.7708\n",
            "Epoch 934/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4156 - acc: 0.7934 - val_loss: 0.4997 - val_acc: 0.7708\n",
            "Epoch 935/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4155 - acc: 0.7934 - val_loss: 0.4997 - val_acc: 0.7708\n",
            "Epoch 936/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4155 - acc: 0.7934 - val_loss: 0.4997 - val_acc: 0.7708\n",
            "Epoch 937/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4155 - acc: 0.7934 - val_loss: 0.4998 - val_acc: 0.7708\n",
            "Epoch 938/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4155 - acc: 0.7934 - val_loss: 0.4998 - val_acc: 0.7708\n",
            "Epoch 939/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4154 - acc: 0.7951 - val_loss: 0.4998 - val_acc: 0.7708\n",
            "Epoch 940/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4154 - acc: 0.7934 - val_loss: 0.4998 - val_acc: 0.7708\n",
            "Epoch 941/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4154 - acc: 0.7934 - val_loss: 0.4998 - val_acc: 0.7708\n",
            "Epoch 942/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4154 - acc: 0.7934 - val_loss: 0.4999 - val_acc: 0.7708\n",
            "Epoch 943/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4154 - acc: 0.7934 - val_loss: 0.4999 - val_acc: 0.7708\n",
            "Epoch 944/1000\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4153 - acc: 0.7934 - val_loss: 0.4999 - val_acc: 0.7708\n",
            "Epoch 945/1000\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4153 - acc: 0.7951 - val_loss: 0.4999 - val_acc: 0.7708\n",
            "Epoch 946/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4153 - acc: 0.7951 - val_loss: 0.4999 - val_acc: 0.7708\n",
            "Epoch 947/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4152 - acc: 0.7951 - val_loss: 0.5000 - val_acc: 0.7708\n",
            "Epoch 948/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4152 - acc: 0.7951 - val_loss: 0.5000 - val_acc: 0.7708\n",
            "Epoch 949/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4152 - acc: 0.7951 - val_loss: 0.5000 - val_acc: 0.7708\n",
            "Epoch 950/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4152 - acc: 0.7951 - val_loss: 0.5000 - val_acc: 0.7708\n",
            "Epoch 951/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4151 - acc: 0.7951 - val_loss: 0.5001 - val_acc: 0.7708\n",
            "Epoch 952/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4151 - acc: 0.7951 - val_loss: 0.5001 - val_acc: 0.7708\n",
            "Epoch 953/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4151 - acc: 0.7951 - val_loss: 0.5001 - val_acc: 0.7708\n",
            "Epoch 954/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4151 - acc: 0.7951 - val_loss: 0.5001 - val_acc: 0.7708\n",
            "Epoch 955/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4150 - acc: 0.7951 - val_loss: 0.5002 - val_acc: 0.7708\n",
            "Epoch 956/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4150 - acc: 0.7951 - val_loss: 0.5002 - val_acc: 0.7708\n",
            "Epoch 957/1000\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4150 - acc: 0.7951 - val_loss: 0.5002 - val_acc: 0.7656\n",
            "Epoch 958/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4150 - acc: 0.7951 - val_loss: 0.5002 - val_acc: 0.7656\n",
            "Epoch 959/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4149 - acc: 0.7951 - val_loss: 0.5002 - val_acc: 0.7656\n",
            "Epoch 960/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4149 - acc: 0.7951 - val_loss: 0.5003 - val_acc: 0.7656\n",
            "Epoch 961/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4149 - acc: 0.7951 - val_loss: 0.5003 - val_acc: 0.7656\n",
            "Epoch 962/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4149 - acc: 0.7951 - val_loss: 0.5003 - val_acc: 0.7656\n",
            "Epoch 963/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4148 - acc: 0.7951 - val_loss: 0.5003 - val_acc: 0.7656\n",
            "Epoch 964/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4148 - acc: 0.7951 - val_loss: 0.5003 - val_acc: 0.7656\n",
            "Epoch 965/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4148 - acc: 0.7951 - val_loss: 0.5004 - val_acc: 0.7656\n",
            "Epoch 966/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4148 - acc: 0.7951 - val_loss: 0.5004 - val_acc: 0.7656\n",
            "Epoch 967/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4147 - acc: 0.7951 - val_loss: 0.5004 - val_acc: 0.7656\n",
            "Epoch 968/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4147 - acc: 0.7951 - val_loss: 0.5004 - val_acc: 0.7604\n",
            "Epoch 969/1000\n",
            "576/576 [==============================] - 0s 40us/step - loss: 0.4147 - acc: 0.7951 - val_loss: 0.5004 - val_acc: 0.7604\n",
            "Epoch 970/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4147 - acc: 0.7951 - val_loss: 0.5005 - val_acc: 0.7604\n",
            "Epoch 971/1000\n",
            "576/576 [==============================] - 0s 64us/step - loss: 0.4146 - acc: 0.7951 - val_loss: 0.5005 - val_acc: 0.7604\n",
            "Epoch 972/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4146 - acc: 0.7951 - val_loss: 0.5005 - val_acc: 0.7604\n",
            "Epoch 973/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4146 - acc: 0.7951 - val_loss: 0.5005 - val_acc: 0.7604\n",
            "Epoch 974/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4146 - acc: 0.7951 - val_loss: 0.5006 - val_acc: 0.7604\n",
            "Epoch 975/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4145 - acc: 0.7951 - val_loss: 0.5006 - val_acc: 0.7604\n",
            "Epoch 976/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4145 - acc: 0.7951 - val_loss: 0.5006 - val_acc: 0.7604\n",
            "Epoch 977/1000\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4145 - acc: 0.7951 - val_loss: 0.5006 - val_acc: 0.7604\n",
            "Epoch 978/1000\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.4145 - acc: 0.7951 - val_loss: 0.5006 - val_acc: 0.7604\n",
            "Epoch 979/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4145 - acc: 0.7951 - val_loss: 0.5007 - val_acc: 0.7604\n",
            "Epoch 980/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4145 - acc: 0.7951 - val_loss: 0.5007 - val_acc: 0.7604\n",
            "Epoch 981/1000\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4144 - acc: 0.7969 - val_loss: 0.5007 - val_acc: 0.7604\n",
            "Epoch 982/1000\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4144 - acc: 0.7951 - val_loss: 0.5007 - val_acc: 0.7604\n",
            "Epoch 983/1000\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4144 - acc: 0.7951 - val_loss: 0.5007 - val_acc: 0.7604\n",
            "Epoch 984/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4144 - acc: 0.7969 - val_loss: 0.5008 - val_acc: 0.7604\n",
            "Epoch 985/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4143 - acc: 0.7951 - val_loss: 0.5008 - val_acc: 0.7604\n",
            "Epoch 986/1000\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4143 - acc: 0.7951 - val_loss: 0.5008 - val_acc: 0.7604\n",
            "Epoch 987/1000\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4143 - acc: 0.7969 - val_loss: 0.5008 - val_acc: 0.7604\n",
            "Epoch 988/1000\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4142 - acc: 0.7969 - val_loss: 0.5009 - val_acc: 0.7604\n",
            "Epoch 989/1000\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4143 - acc: 0.7969 - val_loss: 0.5009 - val_acc: 0.7604\n",
            "Epoch 990/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4142 - acc: 0.7969 - val_loss: 0.5009 - val_acc: 0.7604\n",
            "Epoch 991/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4142 - acc: 0.7969 - val_loss: 0.5009 - val_acc: 0.7604\n",
            "Epoch 992/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4142 - acc: 0.7969 - val_loss: 0.5010 - val_acc: 0.7604\n",
            "Epoch 993/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4142 - acc: 0.7969 - val_loss: 0.5010 - val_acc: 0.7604\n",
            "Epoch 994/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4141 - acc: 0.7969 - val_loss: 0.5010 - val_acc: 0.7604\n",
            "Epoch 995/1000\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4141 - acc: 0.7969 - val_loss: 0.5010 - val_acc: 0.7604\n",
            "Epoch 996/1000\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4141 - acc: 0.7969 - val_loss: 0.5010 - val_acc: 0.7604\n",
            "Epoch 997/1000\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4140 - acc: 0.7969 - val_loss: 0.5011 - val_acc: 0.7604\n",
            "Epoch 998/1000\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4140 - acc: 0.7969 - val_loss: 0.5011 - val_acc: 0.7604\n",
            "Epoch 999/1000\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4140 - acc: 0.7986 - val_loss: 0.5011 - val_acc: 0.7604\n",
            "Epoch 1000/1000\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4140 - acc: 0.7986 - val_loss: 0.5011 - val_acc: 0.7604\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6sAAAHVCAYAAAAXVW0dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xt4VNW9//HPygUQRJGLVYkt6FG5\nhhBSYP+4DUItAuXioRaoJwXECOehSC0U66FeaytgMXjK0YMgp/ywokeLd+TxUCJooyXBEGtA4VCs\nAeUHUSJykUyyfn/sydyYJJNkQibJ+/U86d57zZ49a2Lapx/XWt9lrLUCAAAAACCeJDR2BwAAAAAA\nCEdYBQAAAADEHcIqAAAAACDuEFYBAAAAAHGHsAoAAAAAiDuEVQAAAABA3CGsAgAAAADiDmEVAAAA\nABB3CKsAAAAAgLiT1NgdCNe5c2fbrVu3xu4GAAAAAKAB5OfnH7PWdqnpvrgLq926dVNeXl5jdwMA\nAAAA0ACMMZ9Ecx/TgAEAAAAAcYewCgAAAACIO4RVAAAAAEDcibs1qwAAAADOv7KyMhUXF+vMmTON\n3RU0E23atFFKSoqSk5Pr9H7CKgAAAAAVFxerffv26tatm4wxjd0dNHHWWpWUlKi4uFjdu3ev0zOY\nBgwAAABAZ86cUadOnQiqiAljjDp16lSvkXrCKgAAAABJIqgipur790RYBQAAAADEHcIqAAAAgEZX\nUlKitLQ0paWl6bLLLlPXrl3912fPno3qGTNnztRHH30U9WeuWbNGCxYsqGuX623JkiX+79mrVy89\n99xzMXv2ypUrdfXVV8sYo+PHj8fsuecTBZYAAAAA1E1urpSTI3k8kuPU61GdOnVSQUGBJOm+++7T\nhRdeqIULF4bcY62VtVYJCZHH3NatW1evPjSGRYsWacGCBdq7d68GDRqkf/7nf1ZiYmK9nzt8+HBN\nmjRJQ4YMiUEvGwdhFQAAAECoBQskX3CsUmmpVFgoVVRICQlSaqp08cVV35+WJmVn17or+/fv14QJ\nE9S/f3+9//77evPNN3X//fdr165dOn36tH70ox/pnnvukSQNHTpUv//979WnTx917txZc+bM0ebN\nm9W2bVu99NJLuvTSS6P6zA0bNmjp0qWy1mrChAn6zW9+I6/Xq5kzZ6qgoEDWWmVlZWn+/Pl69NFH\n9eSTTyopKUmpqanasGFDrb+jJPXo0UPJyckqLS1Vx44d/d8lLS1Nn3/+uYYOHar9+/drzZo1euON\nN3TixAkdOHBAU6ZM0W9/+9tznte/f/869SOeEFYBAAAA1F5pqRtUJfdYWlp9WK2HvXv3av369crI\nyJAkPfzww+rYsaO8Xq9GjhypKVOmqFevXmHdK9WIESP08MMP684779RTTz2lu+66q8bPKi4u1pIl\nS5SXl6eLL75Yo0eP1quvvqouXbro2LFj+uCDDyTJP7V22bJl+uSTT9SqVat6TbfduXOn+vTpo44d\nO9Z47+7du5Wfn6/k5GRde+21+ulPf6orrriizp8drwirAAAAAEJFMwKamyuNGiWdPSu1aiU9/XS9\npwJX5eqrr/YHVUl65plntHbtWnm9Xh0+fFhFRUXnhNULLrhAN954oyRpwIAB2rFjR1Sf9d577+n6\n669X586dJUnTp0/X9u3btXjxYn300UeaP3++xo0bpxtuuEGS1Lt3b91yyy2aOHGiJk2aVOvvtnz5\ncq1evVr79u3T66+/HtV7Ro8erYsuukiSOyL7j3/8o1mGVQosAQAAAKg9x5G2bpUefNA9NlBQlaR2\n7dr5z/ft26eVK1fqz3/+swoLCzVmzJiIe3m2atXKf56YmCiv11uvPnTq1EmFhYUaNmyYVq1apdtv\nv12StGXLFs2ZM0c7d+7UwIEDVV5eHvK+zMxMpaWlacKECRGfu2jRIhUVFenZZ5/Vrbfeqm+++UaS\nlJSUpArfyHX492vdunVMv1u8IqwCAAAAqBvHkX75ywYNquG++uortW/fXhdddJE+++wzbdmyJabP\nHzRokLZt26aSkhJ5vV5t3LhRI0aM0NGjR2Wt1Q9/+EM98MAD2rVrl8rLy1VcXKzrr79ey5Yt07Fj\nx3Tq1KmQ561fv14FBQV6+eWXq/3cm266SX379vWvee3WrZvy8/MlSc8//3xMv2NTQVgFAAAA0GSk\np6erV69e6tGjhzIzM+td7Xbt2rVKSUnx/yQlJenBBx+Ux+NRWlqaBg8erHHjxunTTz/V8OHDlZaW\nppkzZ/qLLk2fPl2pqalKT0/XwoUL1b59+zr35Z577tHvfvc7WWu1aNEirVy5Uunp6fryyy9r/awV\nK1YoJSVFn3/+uXr37u0fCW5KjLW2sfsQIiMjw+bl5TV2N6q0Y4e0ZYs0btx5/RdIAAAAQIPas2eP\nevbs2djdQDMT6e/KGJNvrc2o4i1+FFiqhdxcdwupigppxYoGn5oPAAAAAC0W04BrIScnUJ377Fn3\nGgAAAAAQe4TVWvB4pCTfWHSrVu41AAAAACD2CKu14DjS4sXu+R/+wBRgAAAAAGgohNVaGj3aPW7Z\n4q5hBQAAAADEHmG1lr74wj0+9ZQ0ahSBFQAAAAAaAmG1loqK3KO1FFkCAAAAYqWkpERpaWlKS0vT\nZZddpq5du/qvz549G9UzZs6cqY8++ijqz1yzZo0WLFhQ1y7X25IlS/zfs1evXnruuedi9uypU6fq\nuuuuU58+fTR79mx5vd6YPft8IazW0qhRkjHuD0WWAAAA0KId+FJ6Y797rKdOnTqpoKBABQUFmjNn\njn72s5/5r1u1aiVJstaqonJ7jgjWrVun6667rt59OZ8WLVqkgoIC/elPf9Jtt92m8vLymDw3MzNT\ne/fuVWFhoUpLS7Vu3bqYPPd8Yp/VWnIcqV8/6fhx6Y9/pMgSAAAAmqH//lAq/qr6e06XSYdOSFaS\nkdS1vXRBctX3p1wk/bB3rbuyf/9+TZgwQf3799f777+vN998U/fff7927dql06dP60c/+pHuuece\nSdLQoUP1+9//Xn369FHnzp01Z84cbd68WW3bttVLL72kSy+9NKrP3LBhg5YuXSprrSZMmKDf/OY3\n8nq9mjlzpgoKCmStVVZWlubPn69HH31UTz75pJKSkpSamqoNGzbU+jtKUo8ePZScnKzS0lJ17NjR\n/13S0tL0+eefa+jQodq/f7/WrFmjN954QydOnNCBAwc0ZcoU/fa3vz3neWPHjpUkGWM0cOBAFRcX\n16lfjYmwWgepqdK2bQRVAAAAtGCnvW5QldzjaW/1YbUe9u7dq/Xr1ysjI0OS9PDDD6tjx47yer0a\nOXKkpkyZol69eoW8p7S0VCNGjNDDDz+sO++8U0899ZTuuuuuGj+ruLhYS5YsUV5eni6++GKNHj1a\nr776qrp06aJjx47pgw8+kCQdP35ckrRs2TJ98sknatWqlb+tLnbu3Kk+ffqoY8eONd67e/du5efn\nKzk5Wddee61++tOf6oorroh479mzZ/X000/r8ccfr3PfGktUYdUYM0bSSkmJktZYax8Oe/1RSSN9\nl20lXWqt7eB7rVzSB77X/mGtnRCLjjemxESpuFjasUMaNqyxewMAAADEWDQjoAe+lFa+K5VXSIkJ\n0sz+0lWXNEh3rr76an9QlaRnnnlGa9euldfr1eHDh1VUVHROWL3gggt04403SpIGDBigHTt2RPVZ\n7733nq6//np17txZkjR9+nRt375dixcv1kcffaT58+dr3LhxuuGGGyRJvXv31i233KKJEydq0qRJ\ntf5uy5cv1+rVq7Vv3z69/vrrUb1n9OjRuuiiiyS5I7L/+Mc/qgyrt99+u0aPHi2nCY601bhm1RiT\nKGmVpBsl9ZI0zRgT8pdgrf2ZtTbNWpsm6d8l/Sno5dOVrzWHoJqbK23Y4BZYuuEGqgEDAACghbrq\nEumOwdL469xjAwVVSWrXrp3/fN++fVq5cqX+/Oc/q7CwUGPGjNGZM2fOeU/lOldJSkxMrHeBoU6d\nOqmwsFDDhg3TqlWrdPvtt0uStmzZojlz5mjnzp0aOHDgOWtOMzMzlZaWpgkTIkehRYsWqaioSM8+\n+6xuvfVWffPNN5KkpKQk//rc8O/XunXrqL7br371K504cULLli2r25duZNEUWBooab+19oC19qyk\njZImVnP/NEnPxKJz8SgnR6r8+6MaMAAAAFq0qy6RxvxTgwbVcF999ZXat2+viy66SJ999pm2bNkS\n0+cPGjRI27ZtU0lJibxerzZu3KgRI0bo6NGjstbqhz/8oR544AHt2rVL5eXlKi4u1vXXX69ly5bp\n2LFjOnXqVMjz1q9fr4KCAr388svVfu5NN92kvn37+te8duvWTfn5+ZKk559/vtbf44knnlBOTo42\nbNighISmWVc3ml53lfRp0HWxr+0cxpjvSOou6c9BzW2MMXnGmHeNMbUfF48zHo9bBVhypwNTDRgA\nAAA4f9LT09WrVy/16NFDmZmZGjJkSL2et3btWqWkpPh/kpKS9OCDD8rj8SgtLU2DBw/WuHHj9Omn\nn2r48OFKS0vTzJkz/UWXpk+frtTUVKWnp2vhwoVq3759nftyzz336He/+52stVq0aJFWrlyp9PR0\nffll7aotl5eXa968efrss880ePBgpaWl6aGHHqpzvxqLsdZWf4MxUySNsdbO9l3/i6RB1tp5Ee5d\nLCnFWvvToLau1tpDxpir5IbYUdba/w17X5akLEn69re/PeCTTz6p59dqWNu3uyF1xgzpqacauzcA\nAABA/e3Zs0c9e/Zs7G6gmYn0d2WMybfWZlTxFr9oRlYPSboy6DrF1xbJVIVNAbbWHvIdD0jKkdQ/\n/E3W2tXW2gxrbUaXLl2i6FLjGj5c6tJF2r2bNasAAAAA0BCiCas7JV1jjOlujGklN5CeM+HaGNND\n0iWScoPaLjHGtPadd5Y0RFJRLDremHJzpWPHpF27pFGjCKwAAAAAEGs1hlVrrVfSPElbJO2R9Jy1\n9kNjzAPGmOCSVlMlbbSh84p7SsozxuyWtE3Sw9baJh9Wc3IkX2EuiiwBAAAAQAOIap9Va+3rkl4P\na7sn7Pq+CO/7i6S+9ehfXPJ4pKQkyet1iy1RZAkAAAAAYqtp1jBuZI4j/du/uedr17rXAAAAAIDY\nIazW0Zgx7vGNN1izCgAAAACxRlitoy++cI//9/9SZAkAAACor5EjR2rLli0hbdnZ2Zo7d26177vw\nwgslSYcPH9aUKVMi3uPxeJSXl1ftc7Kzs3Xq1Cn/9dixY3X8+PFoul6t++67T4888ki9n1NXM2bM\nUPfu3ZWWlqZ+/fpp69atMXv2v/3bv+nKK6/0/zOINcJqHRUUuEdrKbIEAACAlik3V/rtb2MzcDNt\n2jRt3LgxpG3jxo2aNm1aVO+/4oor9Pzzz9f588PD6uuvv64OHTrU+XnxZPny5SooKFB2drbmzJkT\ns+f+4Ac/0F//+teYPS8cYbWORo6UjHHPKbIEAACA5mTBAvf/31b307+/NHSodPfd7rF//+rvX7Cg\n+s+cMmWKXnvtNZ09e1aSdPDgQR0+fFjDhg3T119/rVGjRik9PV19+/bVSy+9dM77Dx48qD59+kiS\nTp8+ralTp6pnz56aPHmyTp8+7b9v7ty5ysjIUO/evXXvvfdKkh577DEdPnxYI0eO1MiRIyVJ3bp1\n07FjxyRJK1asUJ8+fdSnTx9lZ2f7P69nz5667bbb1Lt3b91www0hn1OTSM88efKkxo0bp379+qlP\nnz569tlnJUl33XWXevXqpdTUVC1cuDDqzwjnOI4OHTrkvw7+jnl5efL4Qs19992nWbNmyePx6Kqr\nrtJjjz0W8XmDBw/W5ZdfXuf+1CSqasA4l+O4/6X74APp5ZcpsgQAAICWpbQ0sJ1jRYV7ffHFdX9e\nx44dNXDgQG3evFkTJ07Uxo0bdfPNN8sYozZt2mjTpk266KKLdOzYMQ0ePFgTJkyQqRw9CvP444+r\nbdu22rNnjwoLC5Wenu5/7aGHHlLHjh1VXl6uUaNGqbCwUPPnz9eKFSu0bds2de7cOeRZ+fn5Wrdu\nnd577z1ZazVo0CCNGDFCl1xyifbt26dnnnlGTz75pG6++Wa98MILuuWWW2r8rlU988CBA7riiiv0\n2muvSZJKS0tVUlKiTZs2ae/evTLG1Gtq8htvvKFJkyZFde/evXu1bds2nThxQtddd53mzp2r5OTk\nOn92XRBW66FrV+mtt6Ty8sbuCQAAABA7voG+auXmurVbzp51Zxo+/XT9B3AqpwJXhtW1a9dKkqy1\nuvvuu7V9+3YlJCTo0KFDOnLkiC677LKIz9m+fbvmz58vSUpNTVVqaqr/teeee06rV6+W1+vVZ599\npqKiopDXw7399tuaPHmy2rVrJ0m66aabtGPHDk2YMMG/FlSSBgwYoIMHD0b1Pat65pgxY/Tzn/9c\nixcv1vjx4zVs2DB5vV61adNGt956q8aPH6/x48dH9RnBFi1apLvvvlvFxcXKjXLO9rhx49S6dWu1\nbt1al156qY4cOaKUlJRaf3Z9MA24jnJzpWefdf8t0ve+R4ElAAAAtCyOI23dKj34oHuMxUzDiRMn\nauvWrdq1a5dOnTqlAQMGSJKefvppHT16VPn5+SooKNC3vvUtnTlzptbP//vf/65HHnlEW7duVWFh\nocaNG1en51Rq3bq1/zwxMVFer7fOz5Kka6+9Vrt27VLfvn21ZMkSPfDAA0pKStJf//pXTZkyRa++\n+qrGVG5LEuT73/++0tLSNHv27IjPXb58uT7++GMtXbpUs2bN8rcnJSWpwjc8Hv57iPV3qwvCah3l\n5ARGVCmwBAAAgJbIcaRf/jJ2S+IuvPBCjRw5UrNmzQoprFRaWqpLL71UycnJ2rZtmz755JNqnzN8\n+HD98Y9/lCT97W9/U2FhoSTpq6++Urt27XTxxRfryJEj2rx5s/897du314kTJ8551rBhw/Tiiy/q\n1KlTOnnypDZt2qRhw4bV63tW9czDhw+rbdu2uuWWW7Ro0SLt2rVLX3/9tUpLSzV27Fg9+uij2r17\n9znP27JliwoKCrRmzZpqP3fevHmqqKjwV13u1q2b8vPzJUkvvPBCvb5TQyCs1pHH4053kKTERAos\nAQAAALEwbdo07d69OySs/vjHP1ZeXp769u2r9evXq0ePHtU+Y+7cufr666/Vs2dP3XPPPf4R2n79\n+ql///7q0aOHpk+friFDhvjfk5WVpTFjxvgLLFVKT0/XjBkzNHDgQA0aNEizZ89W//79a/Wdfv3r\nXyslJcX/U9UzP/jgAw0cOFBpaWm6//77tWTJEp04cULjx49Xamqqhg4dqhUrVtTqs4MZY7RkyRIt\nW7ZMknTvvffqjjvuUEZGhhITE2v9vF/84hdKSUnRqVOnlJKSovvuu6/OfYvYX2ttTB9YXxkZGbam\nPZDixdtvuyF16lRpw4bG7g0AAABQd3v27FHPnj0buxtoZiL9XRlj8q21GTW9l5HVehg61C2yVFDA\nmlUAAAAAiCXCaj3k5krFxdKHH7qV0AisAAAAABAbhNV6yMmRKmdRU2QJAAAAAGKHsFoPHo+U5Nup\nNjmZIksAAAAAECuE1XpwHOk3v3HPV66MXcluAAAAAGjpCKv1NH68e/yf/2HNKgAAAADECmG1no4c\ncY/PP0+RJQAAAKCuRo4cqS1btoS0ZWdna+7cudW+78ILL5QkHT58WFOmTIl4j8fjUU3bY2ZnZ+vU\nqVP+67Fjx+r48ePRdL1a9913nx555JF6P6euZsyYoe7duystLU39+vXT1q1bY/LcU6dOady4cerR\no4d69+6tu+66KybPDUZYrae//MU9WkuRJQAAALQsh05WKPfzch06WVHvZ02bNk0bN24Madu4caOm\nTZsW1fuvuOIKPf/883X+/PCw+vrrr6tDhw51fl48Wb58uQoKCpSdna05c+bE7LkLFy7U3r179f77\n7+udd97R5s2bY/ZsibBabx6PlOD7LbZqRZElAAAANH3/U1yup/d5q/15am+ZNnxcrrc+q9CGj8v1\n1N6yau//n+Lyaj9zypQpeu2113T27FlJ0sGDB3X48GENGzZMX3/9tUaNGqX09HT17dtXL7300jnv\nP3jwoPr06SNJOn36tKZOnaqePXtq8uTJOn36tP++uXPnKiMjQ71799a9994rSXrsscd0+PBhjRw5\nUiNHjpQkdevWTceOHZMkrVixQn369FGfPn2UnZ3t/7yePXvqtttuU+/evXXDDTeEfE5NIj3z5MmT\nGjdunPr166c+ffro2WeflSTddddd6tWrl1JTU7Vw4cKoPyOc4zg6dOiQ/zr4O+bl5cnjCzP33Xef\nZs2aJY/Ho6uuukqPPfbYOc9q27at/3fVqlUrpaenq7i4uM59iyQppk9rgRzHDahvvSVlZ1NkCQAA\nAC3DN+WSbxdHWd9168S6P69jx44aOHCgNm/erIkTJ2rjxo26+eabZYxRmzZttGnTJl100UU6duyY\nBg8erAkTJsgYE/FZjz/+uNq2bas9e/aosLBQ6enp/tceeughdezYUeXl5Ro1apQKCws1f/58rVix\nQtu2bVPnzp1DnpWfn69169bpvffek7VWgwYN0ogRI3TJJZdo3759euaZZ/Tkk0/q5ptv1gsvvKBb\nbrmlxu9a1TMPHDigK664Qq+99pokqbS0VCUlJdq0aZP27t0rY0y9pia/8cYbmjRpUlT37t27V9u2\nbdOJEyd03XXXae7cuUpOTo547/Hjx/XKK6/ojjvuqHPfIiGs1lZurjvX1+ORHEe5udKOHVJ5uXTH\nHVLfvgRWAAAANG2jU2pOnYdOVuiZfeUqt1KikSZ0S1TXdvWbuFk5FbgyrK5du1aSZK3V3Xffre3b\ntyshIUGHDh3SkSNHdNlll0V8zvbt2zV//nxJUmpqqlJTU/2vPffcc1q9erW8Xq8+++wzFRUVhbwe\n7u2339bkyZPVrl07SdJNN92kHTt2aMKECf61oJI0YMAAHTx4MKrvWdUzx4wZo5///OdavHixxo8f\nr2HDhsnr9apNmza69dZbNX78eI2vrPBaC4sWLdLdd9+t4uJi5UZZZGfcuHFq3bq1WrdurUsvvVRH\njhxRSkrKOfd5vV5NmzZN8+fP11VXXVXrvlWHacC1kZsrDRsm3X23v5pSTo4bVCXWrAIAAKDl6Nou\nQdOuSdTwy91jfYOqJE2cOFFbt27Vrl27dOrUKQ0YMECS9PTTT+vo0aPKz89XQUGBvvWtb+nMmTO1\nfv7f//53PfLII9q6dasKCws1bty4Oj2nUuvWrf3niYmJ8nq9dX6WJF177bXatWuX+vbtqyVLluiB\nBx5QUlKS/vrXv2rKlCl69dVXNWbMmHPe9/3vf19paWmaPXt2xOcuX75cH3/8sZYuXapZs2b525OS\nklRR4a43Dv89RPvdsrKydM0112jBggW1/r41IazWRoRk6vG4a1UlKTGRNasAAABoObq2S5BzWWyC\nquRW9h05cqRmzZoVUliptLRUl156qZKTk7Vt2zZ98skn1T5n+PDh+uMf/yhJ+tvf/qbCwkJJ0ldf\nfaV27drp4osv1pEjR0IKArVv314nTpw451nDhg3Tiy++qFOnTunkyZPatGmThg0bVq/vWdUzDx8+\nrLZt2+qWW27RokWLtGvXLn399dcqLS3V2LFj9eijj2r37t3nPG/Lli0qKCjQmjVrqv3cefPmqaKi\nwl91uVu3bsrPz5ckvfDCC7X+HkuWLFFpaal/zW2sEVZrw+ORKudp+6opOY67x2pysjR5MlOAAQAA\ngPqYNm2adu/eHRJWf/zjHysvL099+/bV+vXr1aNHj2qfMXfuXH399dfq2bOn7rnnHv8Ibb9+/dS/\nf3/16NFD06dP15AhQ/zvycrK0pgxY/xFgyqlp6drxowZGjhwoAYNGqTZs2erf//+tfpOv/71r5WS\nkuL/qeqZH3zwgQYOHKi0tDTdf//9WrJkiU6cOKHx48crNTVVQ4cO1YoVK2r12cGMMVqyZImWLVsm\nSbr33nt1xx13KCMjQ4mJtVtwXFxcrIceekhFRUVKT09XWlpajWG51v211tZ813mUkZFha9oDqVE9\n9JC0ZIn0X/8l/eQn/uZrr5WMcZsJrAAAAGhq9uzZo549ezZ2N9DMRPq7MsbkW2szanovI6u1Vbmg\n+Y033DWscg8HDkgff+xfygoAAAAAqAfCam198YV7fPbZkCJLvnXJFFkCAAAAgBggrNbWu++6R2tD\niixVLmVNTqbIEgAAAJqmeFsiiKatvn9PhNXa8nikBN+vLajI0hNPuE3DhzdazwAAAIA6a9OmjUpK\nSgisiAlrrUpKStSmTZs6PyMphv1pGRzH3Wv1nXek7Gx/NaUrr3RffvNNaccOaetWCi0BAACg6UhJ\nSVFxcbGOHj3a2F1BM9GmTRulpKTU+f2E1drKzZX+8hfJ65XuuEPq21dyHO3c6b4cNDuYsAoAAIAm\nIzk5Wd27d2/sbgB+TAOurZwcqbzcPQ+qphRhdjAAAAAAoI4Iq7Xl8bhpVJISE/2p1HGkH/xAuuAC\npgADAAAAQH0RVmvLcdyFqQkJ0s03h6TS73xHOn1aOn68EfsHAAAAAM0AYbUuhg51Kyq9/767hlXu\nobIi8OTJ/mYAAAAAQB0QVusiN1f69FOpqEgaNUrKzVVOjltzSZLKyvxLWQEAAAAAdUBYrYucHLfs\nr+QvsuTxSK1bu00JCRRYAgAAAID6IKzWhccjJfl2/UlOljweOY5bWKlzZ+lb32rU3gEAAABAk0dY\nrQvHkR57zD2//vqQl778Ujp0yD87GAAAAABQB4TVurrySve4eXPIutWKCrc5aAtWAAAAAEAtEVbr\navdu92htyLrV5GS3OSmJdasAAAAAUFeE1boaOdKtpCRJrVr5160+/bTbNHhw43UNAAAAAJo6wmpd\nOY40ZIg7hJqd7V5Luvxy9+Xt21m3CgAAAAB1RVitq9xc6d133c1V77jDn0q3b3dfDpodDAAAAACo\nJcJqXeXkSOXl7nlQKvV4pMREt9k3OxgAAAAAUEuE1bryeNw0Krnp1JdKHUe67Ta3eerURukZAAAA\nADR5hNW6chxp61Y3sP7TP4XW7Qx0AAAgAElEQVS8dNVV7vEPf2DdKgAAAADURVRh1RgzxhjzkTFm\nvzHmrgivP2qMKfD9fGyMOR702k+MMft8Pz+JZecbnTHumtU9e0JS6ZEj7ssVFaxbBQAAAIC6qDGs\nGmMSJa2SdKOkXpKmGWN6Bd9jrf2ZtTbNWpsm6d8l/cn33o6S7pU0SNJASfcaYy6J7VdoRDk5biUl\nKSSVTprkNhnDulUAAAAAqItoRlYHStpvrT1grT0raaOkidXcP03SM77z70t601r7hbX2S0lvShpT\nnw7HFY/H3bpGkpKT/al06FDpyiuljh1DdrUBAAAAAEQpmrDaVdKnQdfFvrZzGGO+I6m7pD/X9r1N\nkuNIv/udez56tL85N1c6fFgqKZEWLGDNKgAAAADUVqwLLE2V9Ly1trw2bzLGZBlj8owxeUePHo1x\nlxpY9+7u8bXX/OtWc3Lc9aoSa1YBAAAAoC6iCauHJF0ZdJ3ia4tkqgJTgKN+r7V2tbU2w1qb0aVL\nlyi6FEcKC92jtf5k6vG4s4Ild5Ywa1YBAAAAoHaiCas7JV1jjOlujGklN5C+HH6TMaaHpEskBU96\n3SLpBmPMJb7CSjf42pqPkSOlBN+v0VdNyXGk//5vt+m73228rgEAAABAU1VjWLXWeiXNkxsy90h6\nzlr7oTHmAWPMhKBbp0raaG1leVzJWvuFpAflBt6dkh7wtTUfjuNWVEpKCqmm1Lmz+/I777DXKgAA\nAADUVlI0N1lrX5f0eljbPWHX91Xx3qckPVXH/sW/3Fz3x+uV7rhD6ttXchy99Zb7ctDsYKoCAwAA\nAECUYl1gqeXJyZHKffWkgqopBe9qw16rAAAAAFA7hNX68njcNCpJiYn+VOo40sKFbvOkSY3SMwAA\nAABosgir9eU40p//LLVtK3XrFvLSP/2Te3z2WdatAgAAAEBtEFZj5cwZad++kFRaXOy+VFHBfqsA\nAAAAUBuE1VjIyXErKUkhqfSGGwK3BM0QBgAAAADUgLAaCx6PlJzsniclhaTSyi1YjTnvvQIAAACA\nJouwGguOI61b554PGeJvDh5w9XqZBgwAAAAA0SKsxsqVV7rHbdv861arGXAFAAAAAFSDsBorb7/t\nHq31r1t1HOnpp93mwYMbr2sAAAAA0NQQVmPF4wksUA2qpnT55W7T9u1sXwMAAAAA0SKsxlLCub/O\n7dvdY9CAKwAAAACgBoTVWMnJcTdUlUKqKXk87npVSWrVinWrAAAAABANwmqseDxS69buedA0YMeR\nFi92mydMaJSeAQAAAECTQ1iNFceRtm6V2rcPVAb26dHDPf73f7NuFQAAAACiQViNtVOnpAMHQlLp\nJ5+4L1VUsG4VAAAAAKJBWI2l4HWrQan0+uslY9zmoBnCAAAAAIAqEFZjyeNxqyhV6tTJf1pZKLgy\ntAIAAAAAqkZYjSXHkX77W/e8okJasEDKza2qUDAAAAAAoAqE1Vg7c8Y9Bm2sWs2AKwAAAAAgAsJq\nrHk87sJUyb+xquNIK1a4TeXl/gFXAAAAAEAVCKux5jjSbbe55z/6kb+5tDRwCxWBAQAAAKB6hNWG\nULmx6vr1/i1sIgy4AgAAAACqQFhtCMeOucegjVUdR7rzTrf5ppsar2sAAAAA0BQQVhvC2LGB86CN\nVa+7zm165hn/gCsAAAAAIALCakOJsLHq4cPuMWjAFQAAAAAQAWG1IeTkuFvXSCEbq44eHbglaMAV\nAAAAABCGsNoQqtlYNcKAKwAAAAAgDGG1ITiO9JvfuOcVFf6NVasYcAUAAAAAhCGsNpRvvnGP1voX\nqFYz4AoAAAAACEJYbSgRNlZ1HGnFCrepvNw/4AoAAAAACENYbSiOI/3rv7rnP/yhv7m0NHALFYEB\nAAAAIDLCakPq3ds9btjg31jV45GSktxm34ArAAAAACAMYbUhHTniHoM2VnUc6Ve/cpvHjm28rgEA\nAABAPCOsNqTvfS9wHrSxauWA65/+5B9wBQAAAAAEIaw2tMoiS0Ebq370kXsMKhQMAAAAAAhCWG1I\nVWysOnJkILsGDbgCAAAAAHwIqw0peGNVa0M2Vk3w/eaDBlwBAAAAAD6E1YbkOFJ2tnteUeHfWDV4\nwLWsjGnAAAAAABCOsNrQvvgicO5boOrxSK1bB5qDBlwBAAAAACKsNrzgjVWNkTp1qmrAFQAAAADg\nQ1htaI4jzZnjnpeX+5NpSUlgvSoVgQEAAAAgFGH1fOjQwT0G7VUTYcAVAAAAAOBDWD0fxo4NnPv2\nqnEc6Ve/cpuCBlwBAAAAACKsnj8R9qpJTHSPQQOuAAAAAAARVs+P4BTq9fqvR44MZFffgCsAAAAA\nQITV88PjkVq1cs+tDVmgWjm6CgAAAAAIIKyeD44jrVzpngftVZOT415KIQOuAAAAANDiEVbPl5KS\nwHlQReDWrQPNVAQGAAAAAFdUYdUYM8YY85ExZr8x5q4q7rnZGFNkjPnQGPPHoPZyY0yB7+flWHW8\nyfF4pORk99y3V43jSNnZ7mXQgCsAAAAAtHg1hlVjTKKkVZJulNRL0jRjTK+we66R9EtJQ6y1vSUt\nCHr5tLU2zfczIXZdb2IcR1q40D0P2qsmeMD1zBlp/frG6R4AAAAAxJNoRlYHStpvrT1grT0raaOk\niWH33CZplbX2S0my1v6/2HazmWjXzj0G7VXj8YRuYbNuHaOrAAAAABBNWO0q6dOg62JfW7BrJV1r\njHnHGPOuMWZM0GttjDF5vvZJkT7AGJPluyfv6NGjtfoCTcr115+zV43jSP/yL4FbKLQEAAAAALEr\nsJQk6RpJHknTJD1pjOnge+071toMSdMlZRtjrg5/s7V2tbU2w1qb0aVLlxh1KU4l+H7llaFV0m23\nBV5mv1UAAAAAiC6sHpJ0ZdB1iq8tWLGkl621Zdbav0v6WG54lbX2kO94QFKOpP717HPTlZPjzvWV\n3GnAQQtUK6cCB2VYAAAAAGixogmrOyVdY4zpboxpJWmqpPCqvi/KHVWVMaaz3GnBB4wxlxhjWge1\nD5FUFKO+Nz0ej5SU5J4HLVANzrBlZUwDBgAAAIAaw6q11itpnqQtkvZIes5a+6Ex5gFjTGV13y2S\nSowxRZK2SVpkrS2R1FNSnjFmt6/9YWttyw2rjiPNmhW49i1QZb9VAAAAAAhlbOWQXpzIyMiweXl5\njd2NhpObKw0f7gbVpCRp1SopK0urV0u33+7ecsEF0tatbrYFAAAAgObEGJPvq2tUrVgVWEK0HEfK\nynLPw/ZbrVyv6tvVBgAAAABaLMJqY6ic5xu232py8rm3AAAAAEBLRFhtDDfeGDgP2m91+XK3KWjA\nFQAAAABaJMJqY4mwV83Jk4GXv/mGqcAAAAAAWi7CamOoYq+a4Km/FRVMBQYAAADQchFWG0MVe9UE\nF1kyxr0GAAAAgJaIsNoYHEfKznYTaUWFf4GqxyO1aePeYgwjqwAAAABaLsJqYwkeNj1zRlq/vqoM\nCwAAAAAtDmG1sXg8gSJL1krr1vn3W63ky7AAAAAA0OIQVhuL40iZmYFrr9e/32pSktsUlGEBAAAA\noEUhrDam2bMD50H7rc6cGWj2ZVgAAAAAaFEIq40twn6rM2ZICQmBlz2e894rAAAAAGhUhNXGFLzf\n6tmzIQtUE/gnAwAAAKAFIxI1pioWqObkuNWAJamsjCJLAAAAAFoewmpjchxp1qzANUWWAAAAAEAS\nYbXxZWZKycnuuTFSp05VZVgAAAAAaDEIq43NcaRf/tI9Ly+XFiyQcnOVmSm1ahW4rVOnxukeAAAA\nADQGwmo8aN3aPVorffONlJMjx5GWLnWbgzIsAAAAALQIhNV40Llz4Lyiwj+Mevp0oNmXYQEAAACg\nRSCsxoOSksA+qwkJ7rVCp/4GZVgAAAAAaPYIq/HA4wlMBZb8qTQ4wxrjz7AAAAAA0OwRVuOB40gr\nV7rnFRX+Baoej9SmTeA2RlYBAAAAtBSE1XgRPIx65oy0fr0cR8rOdputpcgSAAAAgJaDsBovPB4p\nKck9t1Zat07KzY2UYQEAAACg2SOsxgvHkWbMCFx7vVJOTlUZFgAAAACaNcJqPJk5MzCMmpgoeTxy\nHGnWrMAtZWVsYQMAAACg+SOsxpvExHOa+vcPnLOFDQAAAICWgLAaT3Jy3DQquUOovgWqbGEDAAAA\noKUhrMaTKhaosoUNAAAAgJaGsBpPwheonj3LFjYAAAAAWiTCarzJzJSSk91ztrABAAAA0EIRVuON\n47hVgSuxhQ0AAACAFoiwGo9mzJASfP9oqtjCxjdDGAAAAACaJcJqvKoMq9b6mzIzAzvbMLoKAAAA\noDkjrMajnJxASA3awsZxpB//OHBbWZl7KwAAAAA0N4TVeOTxBIZQpZAh1CFDAs0VFWxjAwAAAKB5\nIqzGo2oWqAZXBTZGev/9RugfAAAAADQwwmq8ysyUWrVyz4MWqHo8EXe2AQAAAIBmhbAar8K3sPEt\nUK1iZxsAAAAAaFYIq/EsPT1wHrRA9Sc/Cey5agzrVgEAAAA0P4TVeBa8QFXyL1B1HGnBArepvNw9\nZyowAAAAgOaEsBrPgheoSiELVDt0cJuslb75hqnAAAAAAJoXwmo8C68KHLRAtUuXQDNb2AAAAABo\nbgir8S64KrDkT6VVzBAGAAAAgGaBsBrvHEdascI9D1qgWs0MYQAAAABo8girTcFXXwXOfQtUw2cI\nnz0rrV9//rsGAAAAAA2BsNoUBC9IDVqgmpkZGF21ltFVAAAAAM0HYbUpKCmREoL+UQVtYTNzZqC5\nrIyqwAAAAACah6jCqjFmjDHmI2PMfmPMXVXcc7MxpsgY86Ex5o9B7T8xxuzz/fwkVh1vUTweKSkp\ncB00hDpgQKCZqsAAAAAAmosaw6oxJlHSKkk3SuolaZoxplfYPddI+qWkIdba3pIW+No7SrpX0iBJ\nAyXda4y5JKbfoCWoZoEqVYEBAAAANEfRjKwOlLTfWnvAWntW0kZJE8PuuU3SKmvtl5Jkrf1/vvbv\nS3rTWvuF77U3JY2JTddbmOAtbIIWqFIVGAAAAEBzFE1Y7Srp06DrYl9bsGslXWuMeccY864xZkwt\n3itjTJYxJs8Yk3f06NHoe9+SVDG6SlVgAAAAAM1RrAosJUm6RpJH0jRJTxpjOkT7ZmvtamtthrU2\no0uXLjHqUjOUmRlYuxo0ukpVYAAAAADNTTRh9ZCkK4OuU3xtwYolvWytLbPW/l3Sx3LDazTvRbQc\nR/qXfwlc+8r/ho+uUhUYAAAAQFMXTVjdKekaY0x3Y0wrSVMlvRx2z4tyR1VljOksd1rwAUlbJN1g\njLnEV1jpBl8b6mrw4MB5UPnf9PSIzQAAAADQJCXVdIO11muMmSc3ZCZKespa+6Ex5gFJedbalxUI\npUWSyiUtstaWSJIx5kG5gVeSHrDWftEQX6TFqCz/a6179JX/rdyKtaLCvY2qwAAAAACaMmOtbew+\nhMjIyLB5eXmN3Y34lZvr7rt69qx73bq1tG2bcuWENCcmSv/xH1JWVmN1FAAAAADOZYzJt9Zm1HRf\nrAos4XyJsipwebk0bx6FlgAAAAA0TYTVpqiaqsBJQRO7vV4KLQEAAABomgirTZHjSDNmBK6DqgLf\neWeg2VoKLQEAAABomgirTdV3vxs4Dyr/26GDW3dJCqm/BAAAAABNCmG1qaqsClzJl0o9Hik52W0K\nmiEMAAAAAE0KYbWpCk6lkj+VVlF/CQAAAACaFMJqU1VNKs3MZHQVAAAAQNNGWG3KwlPp2rURR1d9\n9ZcAAAAAoMkgrDZljiONHRu4Livzj66mpweaKyqk48fPc98AAAAAoB4Iq03d5ZeHXn/+uaRz6y89\n+ihTgQEAAAA0HYTVpi4zU0pKClxv3izl5srjkRITA81eL4WWAAAAADQdhNWmznGk2bMD175CS44j\nrVolJfj+CVNoCQAAAEBTQlhtDjIzA8OoQak0K4tCSwAAAACaJsJqc+A4bmCtFJRKv/vdQHNFhdSp\n0/ntGgAAAADUBWG1uRg8OHAelEpLSgJTgSXp/ffPc78AAAAAoA4Iq81FePlfXyr1eELrLz35pLR6\n9fntGgAAAADUFmG1ufB4pOTkwLVv3arjhK5bLS+X5s2j0BIAAACA+EZYbS7CU6mvKrB07u42Xi+F\nlgAAAADEN8Jqc5KZKbVq5Z5bK61d6x9dvfPOwG3WSsePN04XAQAAACAahNXmxHGksWMD12Vl/tHV\nDh1Cl7Q++ihTgQEAAADEL8Jqc3PZZaHXn38uyV3SWrkVq+ROBfblWAAAAACIO4TV5iYzM7TQ0ubN\n/qnAq1YFtrGx1l+DCQAAAADiDmG1uXEc6dZbA9dBhZaysqTbbgu8VFZGoSUAAAAA8Ymw2hwFj66G\nDaGmpwduq6ig0BIAAACA+ERYbY7Ct7EJGkItKaHQEgAAAID4R1htrqoYQqXQEgAAAICmgLDaXFUx\nhFpZaKkysAZtxwoAAAAAcYOw2lxVM4SalSX94AeBl4K2YwUAAACAuEBYba4i7VUTNIRaxXasAAAA\nABAXCKvNWTVDqOHbsb7yirR69XnuHwAAAABUgbDa3F1+eei1bwg1fDvW8nJp3jzWrgIAAACID4TV\n5i58CHXzZn8izcyUkpICL1EZGAAAAEC8IKw2d+FDqGfP+hNpDctaAQAAAKDREFZbgsxMqVUr9zws\nkUZa1rpsWSP0EQAAAACCEFZbAseRxo4NXIftVRO+rPWVVxhdBQAAANC4CKstRTV71WRmhm7JWlHB\n2lUAAAAAjYuw2lJUs1eN40j/8R+sXQUAAAAQPwirLUUNe9WwdhUAAABAPCGstiSR9qrJyfFfsnYV\nAAAAQLwgrLYkjiPdeWfg2lrp+HH/JWtXAQAAAMQLwmpL06GDZEzg+tFH/cOnrF0FAAAAEC8Iqy2N\nxxM6fOr1hgyfsnYVAAAAQDwgrLY0jiOtWlXt8Gn42tWXXvIXDgYAAACA84Kw2hJFGj4NGl0NX7tq\nbUjhYAAAAABocITVlip8+PTzz/2n4WtXpXMKBwMAAABAgyKstlSZmVJycuD6lVdC5vpmZUkLFwZe\nDiscDAAAAAANirDaUjmOdOutgevy8nPm+oYXDn7kEdauAgAAADg/CKstWWamlJQUuA6rDBxeOLii\nQvrXf2XtKgAAAICGF1VYNcaMMcZ8ZIzZb4y5K8LrM4wxR40xBb6f2UGvlQe1vxzLzqOeaqgMXPly\n8OhqeTlb2QAAAABoeDWGVWNMoqRVkm6U1EvSNGNMrwi3PmutTfP9rAlqPx3UPiE23UbMZGVJ48cH\nrsM2Vs3KkiZODH0LW9kAAAAAaGjRjKwOlLTfWnvAWntW0kZJE2t4D5qSK64IvX7llZC5vr/4xblb\n2TAdGAAAAEBDiiasdpX0adB1sa8t3D8bYwqNMc8bY64Mam9jjMkzxrxrjJkU6QOMMVm+e/KOHj0a\nfe8RG+Ebq1ZUhKxdrdzKJnw6cNAtAAAAABBTsSqw9IqkbtbaVElvSvpD0GvfsdZmSJouKdsYc3X4\nm621q621GdbajC5dusSoS4haeBoNW7sqRZ4OHLQ1KwAAAADEVDRh9ZCk4JHSFF+bn7W2xFr7je9y\njaQBQa8d8h0PSMqR1L8e/UVDycqSxo0LXJeVnTN0+otfVLs1KwAAAADETDRhdaeka4wx3Y0xrSRN\nlRRS1dcYc3nQ5QRJe3ztlxhjWvvOO0saIqkoFh1HA0hJCb0OGzqNtDUra1cBAAAANIQaw6q11itp\nnqQtckPoc9baD40xDxhjKqv7zjfGfGiM2S1pvqQZvvaekvJ87dskPWytJazGq8zMGodOw5e3spUN\nAAAAgIZgrLWN3YcQGRkZNi8vr7G70XLNnSs98UTgOjFR2rHDHVb1mTxZevHFwC3GuG/JyjqP/QQA\nAADQJBlj8n11jaoVqwJLaC6iGDplKxsAAAAADY2wilCOI/3gB6FtYfuuVrWVDdOBAQAAAMQKYRXn\nCh86Ddt3VYq8lc1LL1EdGAAAAEBsEFZxrsqh0wTfn0eEfVelyNOB58whsAIAAACoP8IqIsvKCp0O\nXFZ2zjzfSNOBCawAAAAAYoGwiqpdfnnoddjaVSnydGAKLgEAAACoL8IqqhZeGTjC2lXJnQ4cvD2r\nRMElAAAAAPVDWEXVKuf5VgbWKtauOo701ltSr16hb6fgEgAAAIC6IqyielGsXZXcwLpmDfuvAgAA\nAIgNwipqdtlloddVDJlWtf/q7NkEVgAAAAC1Q1hFzcLXrlYzZBqp4FJRkTRsGFOCAQAAAESPsIqa\nVTVkWkUFpfD9VytvZ0owAAAAgGgRVhGdSEOmNUwHjhRYqRAMAAAAIBqEVUQvfMjUWmnevCqnA+/Y\ncW6F4BdflBYvbuB+AgAAAGjyCKuIXuWQaULQn43XK+XkVHl7eIVgyR1dJbACAAAAqA5hFbWTlSUt\nXBi4tlY6frzK2yMtd5UIrAAAAACqR1hF7XXoEJo+H3mk2lK/WVnSokXnthNYAQAAAFSFsIra83hC\n5/ZWVNRY6nfpUnfJazgCKwAAAIBICKuoPceRVq2KeiubSgRWAAAAANEirKJuarGVTTACKwAAAIBo\nEFZRd5G2sqlhOrBEYAUAAABQM8Iq6i5Sqd8opgNLBFYAAAAA1SOson7qOB1Yqj6wjhhR4wAtAAAA\ngGaMsIr6q+N0YKnqwLp9O4EVAAAAaMkIq6i/ekwHlqoOrGVl0uzZBFYAAACgJSKsIjbqMR1Yqjqw\nFhVJQ4dG/RgAAAAAzQRhFbFTj+nAkhtY//M/QwdoJamiQrr9dgovAQAAAC0JYRWxU8/pwJI7QPvE\nE+cGVonCSwAAAEBLQlhFbNVzOnDlI554QkqI8Ne5fTvTggEAAICWgLCK2Is0HXjOnFrN483Kkt5+\nWxo+/NzXKqcFT57MKCsAAADQXBFWEXuV04GDh0atdefx1iKwOo701luRCy9J0osvSkOGEFoBAACA\n5oiwioaRlSU9/vi5i0+XL6/1HN7KwkuRpgVb64ZWpgYDAAAAzQthFQ0nK0tatCi0rZYVgoMf9fbb\n0qRJkYsvUTEYAAAAaF4Iq2hYkTZQrWWF4EqOI23aVHXxJcl9bPfujLICAAAATR1hFQ1v6VJ3SDRY\nLSsEB6tplPXgQXeUldAKAAAANF2EVZwfkSoE12E6cKXKUdZ33olcMVgitAIAAABNGWEV50dlheDg\nodDycmn27HqV8q2pYrAUCK0jRlA1GAAAAGgqCKs4f7KypIkTQ9uKimKSIpculf7yl6pHWSVp+3bp\n//wfqXdvRloBAACAeEdYxfkVPh1YksrK6lRwKVzlKGtNobWoiOnBAAAAQLwjrOL8ijQdWKpXwaVI\nHxFNaGVNKwAAABC/CKs4/7Ky3P1nggOrtdKcOTFNjZWh9T//s+qtbqRAaL38cmnyZNa1AgAAAPGA\nsIrGcZ4Ca+VHvf22++i0tKrv+/xz6cUX3XWt3bsTXAEAAIDGZKy1jd2HEBkZGTYvL6+xu4HzZfJk\nNyEGS0yUduxwh0YbQG6udNddbsGlaKSlSYMHS5mZDdYlAAAAoMUwxuRbazNquo+RVTSuX/xCSk4O\nbYvBljbViXZNa6WCAncQmBFXAAAA4PwhrKJxVSbHXr1C22O0pU00H/2Xv0iTJknf+U7N7zl4MDBV\nuH9/ae5cgisAAADQEJgGjPiQmysNG+aOqgbr1Utas+a8zb/NzXV30Xn3XXcNa7S6dZO+/W23u0wX\nBgAAAKrGNGA0LVVtaXMeRljDu7Fpk/TZZ24V4Z49z+1SJAcPSsWnKlTS1av/KirTY3ll2vBxmd74\n1KtDJysavN8AAABAc8PIKuLL6tVu2d7wv8tJk9wU2Qhyc6X1693c/PHHkUdcv51aodueLFdScqDr\nwSG3Sxupwkod2xgN/laCurbj3xMBAACgZYp2ZDUpyoeNkbRSUqKkNdbah8NenyFpuaRDvqbfW2vX\n+F77iaQlvvZfW2v/ENU3QMuUleUewwPriy9KixdLS5ee9y45Tui03tWrpexsae/eQBe7D7BKSHTP\nI43EHj3jHku+sdpXWq4OrcqVaKQEIyUlSP06JSitc2LDfhEAAADEtYJj5dpdUiFvhTvQkWDc4wW+\n1HbaG2ir6nhBktT5AqO+HZv+AEmNI6vGmERJH0v6nqRiSTslTbPWFgXdM0NShrV2Xth7O0rKk5Qh\nyUrKlzTAWvtlVZ/HyCokVT3COny49PDDcbEoNHjE1V5coe8vKVdCkmQk33/UTttEqV1y8/sfGQAA\ngKasqgBZXViUqg+WRlK577rcumszT3mlMzFcPZZopOnXJMbl/5eM5cjqQEn7rbUHfA/eKGmipKJq\n3+X6vqQ3rbVf+N77pqQxkp6J4r1oyaoaYd2+3V3D+tZbjR5YQ0dcE3TopPRBSYWOnbH66qz0VVnt\nnneq3P2RJH0jFZ+0KjhWrouSy9U6kRALAABahvBwWJtRxZqOlSHxgiR3JO201w2K5dadHVcZHMt9\n935TIZ2tbYD8Jpa/jbort9I/Tlh1bdfYPam7aMJqV0mfBl0XSxoU4b5/NsYMlzsK+zNr7adVvLdr\n+BuNMVmSsiTp29/+dnQ9R/OXlSX97/+65XmDlZW5+7CexyrB0ejaLjRAHjpZoQ9KKnTSa3XaK31x\n5v+3d/cxltX3fcff35nZ2V3YBRaDwObBLC5uhO3Wcbe2sVFkObVNWgscOWppUtVOUq1JYsWpnGJo\nWkXFSlvSyEmq2hbIIXGTyCSisbtpFD80jo2xgbDUSghg4mVN8G543gWWBfZpvv3jnLNz9u59nvt8\n3y/pauaee+6dM+zhzP3c7/f3+9XCaA+ePwJUwbchxJ62XGyuX7wdFytJkjrptVo46Gpi4/2jZUh8\n+VgREE8whPC3//DgX3PSLAZcuLmPdr8J0tWY1S78MfC5zDwUER8CPgu8s9snZ+bNwM1QtAEP6Jg0\nC6oxqo2B9YEHiqVuPvFBUy8AABgnSURBVPWp1SrshGkMr3DyH4ZjCc/2ebF8/kjz6m01Lvb0dcdY\nWpiP8QySJA3b3oMr3PX4MfYd6i+oDSP8VRXAqpV0MVYrgiuc/Hi1fWUFDvbxAXpbE1JNnBVnLHN8\nfpN5HrPaTVjdC1xQu38+qxMpAZCZz9TufgaoksVe4B0Nz/1arwepOXfjjfCa15zcEnzsWLENJjaw\nNnrjWYsnTaTU+Mfv0LHeW4ibea7Ta7RpNQartZI0qdqFpjUFn2GEqQl47WD1Nau2zyrUnbAPRbhb\nYfU5VchbSXjhaJt/lGEENcPfxGkMkMM4f33PdaJuwuo9wCURsZUifF4N/Hh9h4h4ZWY+Vt69Eniw\n/P5LwH+OiC3l/XcD16/5qDV/qjD6sz9bhNRKJnzoQ0W78BhmCh6E805d4P2vOfGCVLUQP/1yHr/Q\nDSrENtPYatyo22rtLH6iJ2n4ml3zJi3wDOK1GwNQY2BaX36W+fKx1dBUr4ytlKHpQLvQ1Mq0halh\nvPYctH3OmiocjvvDFgPk+HQMq5l5NCI+TBE8F4FbMvP+iLgB2JmZO4Cfj4grgaPAPuCD5XP3RcTH\nKQIvwA3VZEtSz7Zvhze8oRiv+kDD/F5Vm/CUBtZGzVqI4eQ3dI0X2oNH+hsX262O1dpKrWq7aekY\nG8qq7Snriofb/WHwD4J0on6D3Ljf3HUb0I6u9Bm+ujFtYUoag26rhaO85vheQJWOS9eMmkvXqKM7\n7yxmBD7SJDlde+3MBNZ+dTNhwjCrtIOyaREWF4o/oAksRVll6PGPo+vYaq2q/6cWo7g/7PAXrIa6\nQ8eGGOQk9aW+1NykfTjUy2vbAaVx6nbpGsOqptOdd8J11xVL2TSaoLVYJ1mras2oq7WjsnGxHK+U\nqwF40G8UDMb96Wf83SD+zeqhsD7xSFUJJODlo01mpZQmRLPQNOmV9mnrDPAaLw2HYVXz4WMfO3mm\nYICFBfj0p6dm4qVJ18v09tNQtR225QXYsFgE4xP+GwHrF1bXbVssH1usqsasfq2H6SpMLdZeq1nY\nHtebuyrcLUTzNs+o/V71WSmXF+DISg/t5Zp5g2pHnPUwZWiSNO26DauDWrpGGo9WS9usrEzdTMGT\nrNksxu10W7VtfAM2K1Xcw20WED8wzB88bROoqGenreP4zN2THKasWEmSBsGwqunXKrBmGljHpNUE\nUd3odZHydm+e17KOrdTo7A1FUBx1+HNsmSRpXhlWNRuqtVh/5meKqmplBpa2mTe9VnE7Wct4yF6q\nSAbjtetl/N0oK3/OSilJ0ng4ZlWz5c47my9tA068pKHrJhjP6hi6tbz2sbQFVJKkeeIES5pf7Za2\nceIlSZIkaay6Dav2NGn2XHYZfP3rRSW10cpK0Rb8sY+N/rgkSZIkdc2wqtlUBdZrr23++K/+alF9\nvfPO0R6XJEmSpK4YVjXbbrwRbrqpaP9tdPvt8Pa3w4/+qKFVkiRJmjCGVc2+7dvhjjuatwVnwhe+\nAJdfDjffPPpjkyRJktSUYVXzoVNbsGNZJUmSpIliWNV8adcWDI5llSRJkiaEYVXzp2oLft/7IOLk\nx2+/Hd72NkOrJEmSNEaGVc2nyy6Dz38evvnN5mNZYXUCJluDJUmSpJEzrGq+1ceyNquyZhatwVu3\nOgGTJEmSNEKGVQmKsaztqqyPPFJMwGRrsCRJkjQShlWpUlVZb7oJXv3q5vu4NqskSZI0EoZVqdH2\n7UUltdUyN67NKkmSJA2dYVVq5cYb4Vvfat0aXK3NamuwJEmSNHCGVamdemtwq7VZXepGkiRJGjjD\nqtSNTmuzAjz0JFz3e/Af/wB27x/t8UmSJEkzxrAqdavd2qzn/ABc+V/g0h+Bp0+FX/sWfOJbhlZJ\nkiSpT4ZVqVfNZg1+1RtgYbGoukYACbv2G1olSZKkPhlWpX5VswbfdBMsH4CVo8VMwZlArVW4Cq03\nfB3ueHRcRytJkiRNlcjMcR/DCbZt25Y7d+4c92FIvdtxO3z17+Dw5vb7nbkBrrgELr9wNMclSZIk\nTZCIuDczt3Xcz7AqDdgdj8IXvwv7Xm6/n6FVkiRJc8iwKo3bHY/CV3fD4wfb77d5GS7eAu96TfFV\nkiRJmmHdhtWlURyMNJcuv7C47d4Pn38QHm4xydKBw/CXTxS38zYXgfUt5xtcJUmSNNcMq9KwXbwF\nPvq2zqEVYO+B4vaNR+HcTfDOrbYJS5IkaS7ZBiyNWjehtc42YUmSJM0Q24ClSVWvtH75Yfje/qIV\nuBXbhCVJkjSHDKvSuFy8Ba4pP1DqdjIm24QlSZI0J2wDlibJ7v1w156i2rr3QHfPOesUWAo4Z5Ot\nwpIkSZp4tgFL0+jiLaths9s24adfLL4+frBoFT7rFNi0Dt52oVVXSZIkTS0rq9I06LZNuNHmZTjn\nVHjlZse5SpIkaSJYWZVmSX3N1l7ahA8cLm679hfjXM/cCBecZruwJEmSJp5hVZomjW3Cd+2Bxw/A\nEwfbtwpX9r1U3GwXliRJ0oQzrErTqh5coWgV/uajcPDI6jjWdp5+EZ4GHrkP/vgh24UlSZI0UQyr\n0qy4vFYhrSZn2vMc7Hu583Mb24XP2wzHVpxhWJIkSWNjWJVmUX0N137ahavxsM4wLEmSpDExrEqz\nbq3twnByy/Bp62HdguFVkiRJQ2NYlebNWtqFYbVlGAyvkiRJGhrDqjTPmrULHzgEBw933zLcKrw6\n5lWSJElrYFiVVGhsF4b+Wobr4bU+5nUpYNOyMw5LkiSpK4ZVSa01axl+8gU4mt2HV6jte3B1xmED\nrCRJktowrErqTr1lGNYWXqF5gD1zI2xcsoVYkiRJhlVJfWoXXhcX4PlD3Y15rdv30ur3jS3EiwuG\nWEmSpDnSVViNiCuA3wQWgc9k5n9tsd/7gduAf5yZOyPiIuBB4KFyl7sy85q1HrSkCdQYXmF1zOvR\nlSJorqkCWzoeYjfC0kLRRgzFz3A2YkmSpJnRMaxGxCLwSeBdwB7gnojYkZkPNOy3GfgIcHfDSzyc\nmW8c0PFKmiaXNwmP9QD70pHul8xp9HRVhT24uq1xNuLFBZfUkSRJmlLdVFbfDOzKzN0AEXErcBXw\nQMN+HwduBP7dQI9Q0mxpDLDVkjmPH4AXDvffQlypz0ZcMcRKkiRNnW7C6nnA92v39wBvqe8QEW8C\nLsjMP4mIxrC6NSK+DTwP/IfM/EbjD4iI7cB2gAsv9I2jNFeaLZkDJ7cQDyvE7ngITq+FWMfFSpIk\nTYQ1T7AUEQvAJ4APNnn4MeDCzHwmIv4R8IWIeF1mPl/fKTNvBm4G2LZtW671mCTNgGYtxHByiN20\nDC8dhb0H+vs5LxwubnXVuNjzNhezE1cV3+rnudSOJEnS0HUTVvcCF9Tun19uq2wGXg98LSIAzgV2\nRMSVmbkTOASQmfdGxMPAa4GdAzh2SfOoVYhtnI243wmd6poG4BZL7dhaLEmSNFDdhNV7gEsiYitF\nSL0a+PHqwcx8Djiruh8RXwN+sZwN+GxgX2Yei4iLgUuA3QM8fkkqNJuNGIYTYiv1pXYqzcbHGmYl\nSZJ61jGsZubRiPgw8CWKpWtuycz7I+IGYGdm7mjz9B8CboiII8AKcE1m7hvEgUtSV3oJsWsdF1tp\nNj628sh9xc+trx3rWFlJkqSTROZkDRHdtm1b7txpl7CkMarGxa5bKO5XY1bXstROL6o1ZBvDrONl\nJUnSDIiIezOzSTXhRGueYEmSZk6rcbHQfKmdQbYWQ20N2UZtxstWYfbU5aIF2UArSZKmnGFVknrR\naqkdaN1aPOgwC83Hy3Jw9dtWgdaxs5IkaUoYViVpUFqNj620qsoOaqxso6aBttRuIihbjiVJ0gQw\nrErSqLSrysLJa8jWA+Qwxsu2mwiq3nL8qs2w0iTQOjGUJEkaIsOqJE2KdmNloX1ltvradG3YNfq7\nDq/5+EH4yydaTwxl67EkSeqDYVWSpkWnyix0DrSDHjtb13JiqNIj98GO78BpG4pK7ablYnuz47Ra\nK0nS3DOsStIs6TbQtpoIathL9LxwpLgBJ0wI1eh4tfaU5mvSGmolSZp5hlVJmjedJoKC1QrtgUNw\n8HDrtuNhTAxV16kKXIXaV2wsWo3bVWudNEqSpKliWJUknaybCm2l3cRQw249rjxTtSC3qdbWJ43a\nsgFOWdf8eK3aSpI0EQyrkqS16TQxFDRvPW5VBR12tRZg/8vFrZ2qanvmRljuULV1EilJkgbOsCpJ\nGr5uWo/rOlVrRxVqobZebbuqbemR+2DHQ3B6i/VrDbeSJHXNsCpJmjzdVGvh5FDbqvo5zEmjGr1Q\njvHtxiP3wRe/WwTXpcXOAdf2ZEnSHDGsSpKmV7ehFrpbp3bUVVvoPUTX25PXReuQWw/uTi4lSZpC\nhlVJ0nzoZdIo6L5qO6pJpBrt67Cu7Qltyw2TS21cV6x1axVXkjTBDKuSJDXTS9UWOq9fO+5wW+lm\ncqnK8SruBlhagM3rIehcmTboSpIGwLAqSdIg9DqJVC9tyeNoT66rWpWf7CFg14NuN2Ny65Vrg64k\nCcOqJEnj0WtbcqWbmZLr4W/fS6ObXKqZrn92rW25Crqv2FgE3W6CvNVcSZo5hlVJkqZJr+3JMF1V\n3LpnOo3LbbCWtmUnoZKkiWNYlSRp1g2qittukqlJCrr9tC3XJ6E6fbmchCphXRezLVvZlaShMKxK\nkqTm+qniVrptV24Mf08cHH9F97nDxa2tgydvqiq7p68vQu7SAuRK92voblqGU5fhtPVWeCUJw6ok\nSRqGUQbdSajm1j13qM8n1gLwNx6FM9bDhnW9BV6ru5JmiGFVkiRNln6Dbr9tyy8dGe8kVK08ewjo\nMfger+4uFwF33ULRzuz4XUlTyLAqSZJmw1qqub1OQtUYhCepsgsntzEPY/xuNx8GGHwlrYFhVZIk\nqd9JqOr6aV9u/Lr3wGB+n0FpO363ybjdZvtUwfeM9UXwzez9wwCDrzSXDKuSJEmDsJbKbqWfZYYm\ncdxuM88eKlubu9EsCDcE317H8zqDszR1DKuSJEmTYq0V3lbV3Wkfv9uon/G87WZwrtbmXVqElT6q\n4s7kLA2FYVWSJGlWjLK62ykAT0vwhQEcZ8NMzqevh41L7dfqNQBLHRlWJUmStGoQ43cr/bY1Nwbh\naQq+UCxf1PcSRpUmSxl1O+bXCa80IwyrkiRJGo5JCL7TMoNzJz2N+W2lNu733FPhWFn57bf12eCr\nITOsSpIkafINMvhWBjGD86TO5NzJ493M5txOfYmj9bBlIywABw93P+lV41cnvFIDw6okSZLm0yDG\n+FbWWvmd5gA8kLZnVie82rK+CLzrFopxv91OENbq67oFeNsA/601MoZVSZIkaa2GUfkdZACepnG/\n+xuD71qrwMAj98GO78Cm9UXb8+b1EKztv6uV4KEzrEqSJEmTaNABuAq/Bw4V7br9BrVpCr51Lxwp\nbgBPvrj216tXgherSvBKEYjXGoQNxIBhVZIkSZoPw5zwaq2tutM24VVdYyX4iQEE4UoViM89FdYv\nFR8UdFoPeIYmvzKsSpIkSerNKCe8WksQPprw9ADD47j0NCFWOfnVnXvgF9461YHVsCpJkiRp/AY5\n4VXd7v3w5YfhyRcGE4CnpRJ8dAX+5hnDqiRJkiRNpIu3wDXbBv+6w6gEDzIQLy3Aa18xmN91TAyr\nkiRJktSrYVWC66pAvG6huN9NAHbMqiRJkiRpqEYRiCfYwrgPQJIkSZKkRoZVSZIkSdLEMaxKkiRJ\nkiaOYVWSJEmSNHEMq5IkSZKkiWNYlSRJkiRNHMOqJEmSJGniGFYlSZIkSRPHsCpJkiRJmjhdhdWI\nuCIiHoqIXRFxXZv93h8RGRHbatuuL5/3UES8ZxAHLUmSJEmabUuddoiIReCTwLuAPcA9EbEjMx9o\n2G8z8BHg7tq2S4GrgdcBrwL+b0S8NjOPDe5XkCRJkiTNmm4qq28GdmXm7sw8DNwKXNVkv48DNwIv\n17ZdBdyamYcy83vArvL1JEmSJElqqZuweh7w/dr9PeW24yLiTcAFmfknvT63fP72iNgZETufeuqp\nrg5ckiRJkjS71jzBUkQsAJ8APtrva2TmzZm5LTO3nX322Ws9JEmSJEnSlOs4ZhXYC1xQu39+ua2y\nGXg98LWIADgX2BERV3bxXEmSJEmSTtJNZfUe4JKI2BoRyxQTJu2oHszM5zLzrMy8KDMvAu4CrszM\nneV+V0fE+ojYClwC/MXAfwtJkiRJ0kzpWFnNzKMR8WHgS8AicEtm3h8RNwA7M3NHm+feHxF/CDwA\nHAV+rtNMwPfee+/TEfG3Pf0Wo3cW8PS4D0ITyXND7Xh+qBXPDbXj+aFWPDfUyqSfG6/uZqfIzGEf\nyMyJiJ2Zua3znpo3nhtqx/NDrXhuqB3PD7XiuaFWZuXcWPMES5IkSZIkDZphVZIkSZI0cQyr/bl5\n3AegieW5oXY8P9SK54ba8fxQK54bamUmzg3HrEqSJEmSJo6VVUmSJEnSxDGsSpIkSZImjmG1RxFx\nRUQ8FBG7IuK6cR+PRisiLoiIP4+IByLi/oj4SLn9zIj4SkR8t/y6pdweEfHfy/PlryLiTeP9DTRs\nEbEYEd+OiP9T3t8aEXeX58AfRMRyuX19eX9X+fhF4zxuDV9EnBERt0XEdyLiwYi4zGuHACLi35Z/\nU/46Ij4XERu8dsyviLglIp6MiL+ubev5WhERHyj3/25EfGAcv4sGq8W58d/Kvyt/FRGfj4gzao9d\nX54bD0XEe2rbpybPGFZ7EBGLwCeBHwEuBf5lRFw63qPSiB0FPpqZlwJvBX6uPAeuA/4sMy8B/qy8\nD8W5ckl52w58evSHrBH7CPBg7f6NwK9n5t8D9gM/XW7/aWB/uf3Xy/00234T+GJm/gDwDynOE68d\ncy4izgN+HtiWma8HFoGr8doxz34HuKJhW0/Xiog4E/hl4C3Am4FfrgKuptrvcPK58RXg9Zn5D4C/\nAa4HKN+fXg28rnzOp8oP1KcqzxhWe/NmYFdm7s7Mw8CtwFVjPiaNUGY+lpn/r/z+AMWbzfMozoPP\nlrt9Fnhf+f1VwP/Mwl3AGRHxyhEftkYkIs4H/hnwmfJ+AO8Ebit3aTw3qnPmNuCHy/01gyLidOCH\ngN8CyMzDmfksXjtUWAI2RsQScArwGF475lZm3g7sa9jc67XiPcBXMnNfZu6nCDSNIUdTptm5kZlf\nzsyj5d27gPPL768Cbs3MQ5n5PWAXRZaZqjxjWO3NecD3a/f3lNs0h8rWqx8E7gbOyczHyoceB84p\nv/ecmS+/AVwLrJT3XwE8W/sjUv/3P35ulI8/V+6v2bQVeAr47bJN/DMRcSpeO+ZeZu4Ffg14lCKk\nPgfci9cOnajXa4XXkPn0U8Cflt/PxLlhWJX6EBGbgP8F/EJmPl9/LIv1oFwTas5ExHuBJzPz3nEf\niybSEvAm4NOZ+YPAQVbb+ACvHfOqbM28iuIDjVcBp2IFTG14rVAzEfFLFMPVfn/cxzJIhtXe7AUu\nqN0/v9ymORIR6yiC6u9n5h+Vm5+oWvTKr0+W2z1n5sfbgSsj4hGKlpp3UoxRPKNs7YMT//2Pnxvl\n46cDz4zygDVSe4A9mXl3ef82ivDqtUP/BPheZj6VmUeAP6K4nnjtUF2v1wqvIXMkIj4IvBf4ifLD\nDJiRc8Ow2pt7gEvKGfqWKQYt7xjzMWmEynFBvwU8mJmfqD20A6hm2vsA8L9r2/91OVvfW4Hnam08\nmiGZeX1mnp+ZF1FcG76amT8B/DnwY+VujedGdc78WLm/n5TPqMx8HPh+RPz9ctMPAw/gtUNF++9b\nI+KU8m9MdW547VBdr9eKLwHvjogtZfX+3eU2zZiIuIJiCNKVmfli7aEdwNXlDOJbKSbh+gumLM+E\n17feRMQ/pRiXtgjckpm/MuZD0ghFxOXAN4D7WB2X+O8pxq3+IXAh8LfAP8/MfeUbj/9B0dL1IvCT\nmblz5AeukYqIdwC/mJnvjYiLKSqtZwLfBv5VZh6KiA3A71KMe94HXJ2Zu8d1zBq+iHgjxeRby8Bu\n4CcpPjT22jHnIuI/Af+CooXv28C/oRhD5rVjDkXE54B3AGcBT1DM6vsFerxWRMRPUbxHAfiVzPzt\nUf4eGrwW58b1wHpWOyzuysxryv1/iWIc61GKoWt/Wm6fmjxjWJUkSZIkTRzbgCVJkiRJE8ewKkmS\nJEmaOIZVSZIkSdLEMaxKkiRJkiaOYVWSJEmSNHEMq5IkSZKkiWNYlSRJkiRNnP8P4b15jufASaoA\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1152x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 576 samples, validate on 192 samples\n",
            "Epoch 1/1500\n",
            "576/576 [==============================] - 0s 330us/step - loss: 0.7386 - acc: 0.5399 - val_loss: 0.7221 - val_acc: 0.6094\n",
            "Epoch 2/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.7309 - acc: 0.5503 - val_loss: 0.7135 - val_acc: 0.6146\n",
            "Epoch 3/1500\n",
            "576/576 [==============================] - 0s 40us/step - loss: 0.7237 - acc: 0.5590 - val_loss: 0.7053 - val_acc: 0.6198\n",
            "Epoch 4/1500\n",
            "576/576 [==============================] - 0s 39us/step - loss: 0.7169 - acc: 0.5712 - val_loss: 0.6979 - val_acc: 0.6250\n",
            "Epoch 5/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.7105 - acc: 0.5781 - val_loss: 0.6909 - val_acc: 0.6458\n",
            "Epoch 6/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.7046 - acc: 0.5833 - val_loss: 0.6843 - val_acc: 0.6510\n",
            "Epoch 7/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.6990 - acc: 0.5885 - val_loss: 0.6783 - val_acc: 0.6615\n",
            "Epoch 8/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.6938 - acc: 0.6059 - val_loss: 0.6727 - val_acc: 0.6719\n",
            "Epoch 9/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.6889 - acc: 0.6094 - val_loss: 0.6675 - val_acc: 0.6771\n",
            "Epoch 10/1500\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.6843 - acc: 0.6146 - val_loss: 0.6626 - val_acc: 0.6875\n",
            "Epoch 11/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.6799 - acc: 0.6250 - val_loss: 0.6580 - val_acc: 0.6979\n",
            "Epoch 12/1500\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.6758 - acc: 0.6337 - val_loss: 0.6537 - val_acc: 0.6927\n",
            "Epoch 13/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.6719 - acc: 0.6424 - val_loss: 0.6496 - val_acc: 0.6875\n",
            "Epoch 14/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.6682 - acc: 0.6441 - val_loss: 0.6459 - val_acc: 0.6927\n",
            "Epoch 15/1500\n",
            "576/576 [==============================] - 0s 59us/step - loss: 0.6647 - acc: 0.6510 - val_loss: 0.6423 - val_acc: 0.6979\n",
            "Epoch 16/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.6613 - acc: 0.6615 - val_loss: 0.6389 - val_acc: 0.6979\n",
            "Epoch 17/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.6581 - acc: 0.6649 - val_loss: 0.6356 - val_acc: 0.6875\n",
            "Epoch 18/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.6550 - acc: 0.6667 - val_loss: 0.6323 - val_acc: 0.6979\n",
            "Epoch 19/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.6520 - acc: 0.6771 - val_loss: 0.6292 - val_acc: 0.7083\n",
            "Epoch 20/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.6491 - acc: 0.6823 - val_loss: 0.6262 - val_acc: 0.7083\n",
            "Epoch 21/1500\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.6463 - acc: 0.6875 - val_loss: 0.6234 - val_acc: 0.7083\n",
            "Epoch 22/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.6436 - acc: 0.6858 - val_loss: 0.6206 - val_acc: 0.7083\n",
            "Epoch 23/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.6410 - acc: 0.6910 - val_loss: 0.6180 - val_acc: 0.7083\n",
            "Epoch 24/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.6384 - acc: 0.6927 - val_loss: 0.6154 - val_acc: 0.7135\n",
            "Epoch 25/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.6359 - acc: 0.6944 - val_loss: 0.6129 - val_acc: 0.7135\n",
            "Epoch 26/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.6335 - acc: 0.6944 - val_loss: 0.6105 - val_acc: 0.7135\n",
            "Epoch 27/1500\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.6312 - acc: 0.6997 - val_loss: 0.6082 - val_acc: 0.7135\n",
            "Epoch 28/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.6289 - acc: 0.6997 - val_loss: 0.6060 - val_acc: 0.7135\n",
            "Epoch 29/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.6267 - acc: 0.6997 - val_loss: 0.6038 - val_acc: 0.7240\n",
            "Epoch 30/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.6246 - acc: 0.7014 - val_loss: 0.6017 - val_acc: 0.7240\n",
            "Epoch 31/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.6225 - acc: 0.7014 - val_loss: 0.5996 - val_acc: 0.7188\n",
            "Epoch 32/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.6205 - acc: 0.7066 - val_loss: 0.5976 - val_acc: 0.7240\n",
            "Epoch 33/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.6185 - acc: 0.7083 - val_loss: 0.5957 - val_acc: 0.7188\n",
            "Epoch 34/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.6166 - acc: 0.7101 - val_loss: 0.5938 - val_acc: 0.7188\n",
            "Epoch 35/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.6147 - acc: 0.7118 - val_loss: 0.5920 - val_acc: 0.7188\n",
            "Epoch 36/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.6129 - acc: 0.7118 - val_loss: 0.5902 - val_acc: 0.7240\n",
            "Epoch 37/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.6111 - acc: 0.7153 - val_loss: 0.5885 - val_acc: 0.7240\n",
            "Epoch 38/1500\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.6093 - acc: 0.7135 - val_loss: 0.5868 - val_acc: 0.7188\n",
            "Epoch 39/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.6076 - acc: 0.7170 - val_loss: 0.5852 - val_acc: 0.7188\n",
            "Epoch 40/1500\n",
            "576/576 [==============================] - 0s 65us/step - loss: 0.6058 - acc: 0.7170 - val_loss: 0.5836 - val_acc: 0.7240\n",
            "Epoch 41/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.6042 - acc: 0.7135 - val_loss: 0.5821 - val_acc: 0.7240\n",
            "Epoch 42/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.6025 - acc: 0.7153 - val_loss: 0.5806 - val_acc: 0.7240\n",
            "Epoch 43/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.6009 - acc: 0.7170 - val_loss: 0.5791 - val_acc: 0.7135\n",
            "Epoch 44/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.5994 - acc: 0.7170 - val_loss: 0.5777 - val_acc: 0.7135\n",
            "Epoch 45/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.5977 - acc: 0.7170 - val_loss: 0.5762 - val_acc: 0.7135\n",
            "Epoch 46/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.5963 - acc: 0.7205 - val_loss: 0.5748 - val_acc: 0.7135\n",
            "Epoch 47/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.5947 - acc: 0.7205 - val_loss: 0.5734 - val_acc: 0.7135\n",
            "Epoch 48/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.5933 - acc: 0.7205 - val_loss: 0.5721 - val_acc: 0.7188\n",
            "Epoch 49/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.5918 - acc: 0.7205 - val_loss: 0.5708 - val_acc: 0.7135\n",
            "Epoch 50/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.5904 - acc: 0.7240 - val_loss: 0.5695 - val_acc: 0.7135\n",
            "Epoch 51/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.5890 - acc: 0.7240 - val_loss: 0.5683 - val_acc: 0.7135\n",
            "Epoch 52/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.5876 - acc: 0.7274 - val_loss: 0.5671 - val_acc: 0.7135\n",
            "Epoch 53/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.5862 - acc: 0.7292 - val_loss: 0.5659 - val_acc: 0.7135\n",
            "Epoch 54/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.5849 - acc: 0.7274 - val_loss: 0.5647 - val_acc: 0.7188\n",
            "Epoch 55/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.5835 - acc: 0.7274 - val_loss: 0.5636 - val_acc: 0.7188\n",
            "Epoch 56/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.5822 - acc: 0.7257 - val_loss: 0.5625 - val_acc: 0.7188\n",
            "Epoch 57/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.5810 - acc: 0.7257 - val_loss: 0.5614 - val_acc: 0.7292\n",
            "Epoch 58/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.5797 - acc: 0.7257 - val_loss: 0.5603 - val_acc: 0.7292\n",
            "Epoch 59/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.5785 - acc: 0.7274 - val_loss: 0.5592 - val_acc: 0.7292\n",
            "Epoch 60/1500\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.5772 - acc: 0.7292 - val_loss: 0.5581 - val_acc: 0.7344\n",
            "Epoch 61/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.5759 - acc: 0.7292 - val_loss: 0.5571 - val_acc: 0.7344\n",
            "Epoch 62/1500\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.5747 - acc: 0.7292 - val_loss: 0.5561 - val_acc: 0.7396\n",
            "Epoch 63/1500\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.5735 - acc: 0.7344 - val_loss: 0.5550 - val_acc: 0.7396\n",
            "Epoch 64/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.5723 - acc: 0.7344 - val_loss: 0.5540 - val_acc: 0.7396\n",
            "Epoch 65/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.5711 - acc: 0.7344 - val_loss: 0.5530 - val_acc: 0.7396\n",
            "Epoch 66/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.5700 - acc: 0.7344 - val_loss: 0.5520 - val_acc: 0.7396\n",
            "Epoch 67/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.5688 - acc: 0.7344 - val_loss: 0.5510 - val_acc: 0.7396\n",
            "Epoch 68/1500\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.5676 - acc: 0.7326 - val_loss: 0.5500 - val_acc: 0.7396\n",
            "Epoch 69/1500\n",
            "576/576 [==============================] - 0s 64us/step - loss: 0.5664 - acc: 0.7309 - val_loss: 0.5490 - val_acc: 0.7396\n",
            "Epoch 70/1500\n",
            "576/576 [==============================] - 0s 62us/step - loss: 0.5653 - acc: 0.7326 - val_loss: 0.5481 - val_acc: 0.7396\n",
            "Epoch 71/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.5642 - acc: 0.7326 - val_loss: 0.5471 - val_acc: 0.7344\n",
            "Epoch 72/1500\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.5631 - acc: 0.7326 - val_loss: 0.5462 - val_acc: 0.7344\n",
            "Epoch 73/1500\n",
            "576/576 [==============================] - 0s 62us/step - loss: 0.5620 - acc: 0.7326 - val_loss: 0.5453 - val_acc: 0.7344\n",
            "Epoch 74/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.5609 - acc: 0.7326 - val_loss: 0.5445 - val_acc: 0.7396\n",
            "Epoch 75/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.5598 - acc: 0.7344 - val_loss: 0.5436 - val_acc: 0.7396\n",
            "Epoch 76/1500\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.5588 - acc: 0.7344 - val_loss: 0.5427 - val_acc: 0.7344\n",
            "Epoch 77/1500\n",
            "576/576 [==============================] - 0s 65us/step - loss: 0.5578 - acc: 0.7326 - val_loss: 0.5419 - val_acc: 0.7396\n",
            "Epoch 78/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.5568 - acc: 0.7361 - val_loss: 0.5411 - val_acc: 0.7396\n",
            "Epoch 79/1500\n",
            "576/576 [==============================] - 0s 66us/step - loss: 0.5558 - acc: 0.7344 - val_loss: 0.5403 - val_acc: 0.7396\n",
            "Epoch 80/1500\n",
            "576/576 [==============================] - 0s 60us/step - loss: 0.5548 - acc: 0.7344 - val_loss: 0.5395 - val_acc: 0.7396\n",
            "Epoch 81/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.5538 - acc: 0.7344 - val_loss: 0.5388 - val_acc: 0.7396\n",
            "Epoch 82/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.5528 - acc: 0.7361 - val_loss: 0.5380 - val_acc: 0.7396\n",
            "Epoch 83/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.5520 - acc: 0.7361 - val_loss: 0.5373 - val_acc: 0.7396\n",
            "Epoch 84/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.5510 - acc: 0.7413 - val_loss: 0.5365 - val_acc: 0.7396\n",
            "Epoch 85/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.5500 - acc: 0.7396 - val_loss: 0.5358 - val_acc: 0.7396\n",
            "Epoch 86/1500\n",
            "576/576 [==============================] - 0s 59us/step - loss: 0.5491 - acc: 0.7413 - val_loss: 0.5351 - val_acc: 0.7396\n",
            "Epoch 87/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.5481 - acc: 0.7448 - val_loss: 0.5344 - val_acc: 0.7396\n",
            "Epoch 88/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.5472 - acc: 0.7448 - val_loss: 0.5337 - val_acc: 0.7396\n",
            "Epoch 89/1500\n",
            "576/576 [==============================] - 0s 61us/step - loss: 0.5463 - acc: 0.7448 - val_loss: 0.5330 - val_acc: 0.7396\n",
            "Epoch 90/1500\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.5454 - acc: 0.7448 - val_loss: 0.5323 - val_acc: 0.7396\n",
            "Epoch 91/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.5445 - acc: 0.7448 - val_loss: 0.5316 - val_acc: 0.7396\n",
            "Epoch 92/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.5436 - acc: 0.7483 - val_loss: 0.5310 - val_acc: 0.7396\n",
            "Epoch 93/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.5428 - acc: 0.7465 - val_loss: 0.5303 - val_acc: 0.7396\n",
            "Epoch 94/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.5419 - acc: 0.7448 - val_loss: 0.5297 - val_acc: 0.7396\n",
            "Epoch 95/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.5410 - acc: 0.7431 - val_loss: 0.5290 - val_acc: 0.7396\n",
            "Epoch 96/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.5401 - acc: 0.7448 - val_loss: 0.5284 - val_acc: 0.7396\n",
            "Epoch 97/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.5394 - acc: 0.7448 - val_loss: 0.5278 - val_acc: 0.7396\n",
            "Epoch 98/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.5385 - acc: 0.7448 - val_loss: 0.5272 - val_acc: 0.7396\n",
            "Epoch 99/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.5377 - acc: 0.7448 - val_loss: 0.5266 - val_acc: 0.7396\n",
            "Epoch 100/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.5369 - acc: 0.7448 - val_loss: 0.5260 - val_acc: 0.7396\n",
            "Epoch 101/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.5361 - acc: 0.7448 - val_loss: 0.5254 - val_acc: 0.7396\n",
            "Epoch 102/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.5353 - acc: 0.7431 - val_loss: 0.5249 - val_acc: 0.7396\n",
            "Epoch 103/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.5345 - acc: 0.7448 - val_loss: 0.5243 - val_acc: 0.7396\n",
            "Epoch 104/1500\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.5337 - acc: 0.7448 - val_loss: 0.5238 - val_acc: 0.7396\n",
            "Epoch 105/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.5329 - acc: 0.7448 - val_loss: 0.5232 - val_acc: 0.7396\n",
            "Epoch 106/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.5322 - acc: 0.7465 - val_loss: 0.5227 - val_acc: 0.7396\n",
            "Epoch 107/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.5314 - acc: 0.7448 - val_loss: 0.5222 - val_acc: 0.7396\n",
            "Epoch 108/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.5306 - acc: 0.7448 - val_loss: 0.5216 - val_acc: 0.7396\n",
            "Epoch 109/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.5299 - acc: 0.7465 - val_loss: 0.5211 - val_acc: 0.7396\n",
            "Epoch 110/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.5292 - acc: 0.7448 - val_loss: 0.5206 - val_acc: 0.7396\n",
            "Epoch 111/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.5285 - acc: 0.7448 - val_loss: 0.5201 - val_acc: 0.7396\n",
            "Epoch 112/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.5278 - acc: 0.7448 - val_loss: 0.5196 - val_acc: 0.7396\n",
            "Epoch 113/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.5270 - acc: 0.7448 - val_loss: 0.5191 - val_acc: 0.7396\n",
            "Epoch 114/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.5263 - acc: 0.7448 - val_loss: 0.5186 - val_acc: 0.7396\n",
            "Epoch 115/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.5256 - acc: 0.7465 - val_loss: 0.5181 - val_acc: 0.7396\n",
            "Epoch 116/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.5249 - acc: 0.7465 - val_loss: 0.5177 - val_acc: 0.7396\n",
            "Epoch 117/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.5242 - acc: 0.7465 - val_loss: 0.5172 - val_acc: 0.7396\n",
            "Epoch 118/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.5235 - acc: 0.7465 - val_loss: 0.5167 - val_acc: 0.7396\n",
            "Epoch 119/1500\n",
            "576/576 [==============================] - 0s 69us/step - loss: 0.5229 - acc: 0.7483 - val_loss: 0.5163 - val_acc: 0.7396\n",
            "Epoch 120/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.5222 - acc: 0.7465 - val_loss: 0.5158 - val_acc: 0.7396\n",
            "Epoch 121/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.5216 - acc: 0.7465 - val_loss: 0.5154 - val_acc: 0.7396\n",
            "Epoch 122/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.5209 - acc: 0.7483 - val_loss: 0.5150 - val_acc: 0.7448\n",
            "Epoch 123/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.5203 - acc: 0.7465 - val_loss: 0.5145 - val_acc: 0.7448\n",
            "Epoch 124/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.5196 - acc: 0.7465 - val_loss: 0.5141 - val_acc: 0.7448\n",
            "Epoch 125/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.5190 - acc: 0.7465 - val_loss: 0.5137 - val_acc: 0.7448\n",
            "Epoch 126/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.5184 - acc: 0.7465 - val_loss: 0.5133 - val_acc: 0.7448\n",
            "Epoch 127/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.5178 - acc: 0.7483 - val_loss: 0.5129 - val_acc: 0.7448\n",
            "Epoch 128/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.5172 - acc: 0.7483 - val_loss: 0.5125 - val_acc: 0.7448\n",
            "Epoch 129/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.5166 - acc: 0.7500 - val_loss: 0.5121 - val_acc: 0.7448\n",
            "Epoch 130/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.5161 - acc: 0.7483 - val_loss: 0.5118 - val_acc: 0.7448\n",
            "Epoch 131/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.5155 - acc: 0.7483 - val_loss: 0.5114 - val_acc: 0.7500\n",
            "Epoch 132/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.5149 - acc: 0.7465 - val_loss: 0.5110 - val_acc: 0.7500\n",
            "Epoch 133/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.5144 - acc: 0.7483 - val_loss: 0.5107 - val_acc: 0.7500\n",
            "Epoch 134/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.5138 - acc: 0.7483 - val_loss: 0.5103 - val_acc: 0.7500\n",
            "Epoch 135/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.5132 - acc: 0.7483 - val_loss: 0.5099 - val_acc: 0.7500\n",
            "Epoch 136/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.5127 - acc: 0.7500 - val_loss: 0.5096 - val_acc: 0.7500\n",
            "Epoch 137/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.5121 - acc: 0.7517 - val_loss: 0.5092 - val_acc: 0.7500\n",
            "Epoch 138/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.5116 - acc: 0.7535 - val_loss: 0.5088 - val_acc: 0.7500\n",
            "Epoch 139/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.5110 - acc: 0.7535 - val_loss: 0.5085 - val_acc: 0.7500\n",
            "Epoch 140/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.5105 - acc: 0.7535 - val_loss: 0.5081 - val_acc: 0.7500\n",
            "Epoch 141/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.5100 - acc: 0.7535 - val_loss: 0.5078 - val_acc: 0.7500\n",
            "Epoch 142/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.5095 - acc: 0.7535 - val_loss: 0.5074 - val_acc: 0.7500\n",
            "Epoch 143/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.5089 - acc: 0.7552 - val_loss: 0.5071 - val_acc: 0.7552\n",
            "Epoch 144/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.5084 - acc: 0.7552 - val_loss: 0.5068 - val_acc: 0.7552\n",
            "Epoch 145/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.5079 - acc: 0.7552 - val_loss: 0.5064 - val_acc: 0.7552\n",
            "Epoch 146/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.5074 - acc: 0.7552 - val_loss: 0.5061 - val_acc: 0.7552\n",
            "Epoch 147/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.5070 - acc: 0.7552 - val_loss: 0.5058 - val_acc: 0.7552\n",
            "Epoch 148/1500\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.5064 - acc: 0.7569 - val_loss: 0.5055 - val_acc: 0.7552\n",
            "Epoch 149/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.5059 - acc: 0.7587 - val_loss: 0.5052 - val_acc: 0.7552\n",
            "Epoch 150/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.5055 - acc: 0.7569 - val_loss: 0.5049 - val_acc: 0.7552\n",
            "Epoch 151/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.5050 - acc: 0.7569 - val_loss: 0.5047 - val_acc: 0.7604\n",
            "Epoch 152/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.5045 - acc: 0.7587 - val_loss: 0.5044 - val_acc: 0.7604\n",
            "Epoch 153/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.5040 - acc: 0.7587 - val_loss: 0.5041 - val_acc: 0.7604\n",
            "Epoch 154/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.5036 - acc: 0.7569 - val_loss: 0.5038 - val_acc: 0.7604\n",
            "Epoch 155/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.5031 - acc: 0.7569 - val_loss: 0.5035 - val_acc: 0.7656\n",
            "Epoch 156/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.5027 - acc: 0.7569 - val_loss: 0.5033 - val_acc: 0.7656\n",
            "Epoch 157/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.5022 - acc: 0.7569 - val_loss: 0.5030 - val_acc: 0.7656\n",
            "Epoch 158/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.5018 - acc: 0.7569 - val_loss: 0.5027 - val_acc: 0.7656\n",
            "Epoch 159/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.5013 - acc: 0.7587 - val_loss: 0.5025 - val_acc: 0.7656\n",
            "Epoch 160/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.5009 - acc: 0.7569 - val_loss: 0.5022 - val_acc: 0.7708\n",
            "Epoch 161/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.5005 - acc: 0.7587 - val_loss: 0.5020 - val_acc: 0.7708\n",
            "Epoch 162/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.5000 - acc: 0.7587 - val_loss: 0.5017 - val_acc: 0.7708\n",
            "Epoch 163/1500\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4996 - acc: 0.7587 - val_loss: 0.5014 - val_acc: 0.7708\n",
            "Epoch 164/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4992 - acc: 0.7604 - val_loss: 0.5012 - val_acc: 0.7708\n",
            "Epoch 165/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4988 - acc: 0.7622 - val_loss: 0.5009 - val_acc: 0.7708\n",
            "Epoch 166/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4983 - acc: 0.7622 - val_loss: 0.5007 - val_acc: 0.7708\n",
            "Epoch 167/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4979 - acc: 0.7622 - val_loss: 0.5005 - val_acc: 0.7656\n",
            "Epoch 168/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4975 - acc: 0.7639 - val_loss: 0.5002 - val_acc: 0.7656\n",
            "Epoch 169/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4971 - acc: 0.7639 - val_loss: 0.5000 - val_acc: 0.7708\n",
            "Epoch 170/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4967 - acc: 0.7622 - val_loss: 0.4997 - val_acc: 0.7708\n",
            "Epoch 171/1500\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4964 - acc: 0.7639 - val_loss: 0.4995 - val_acc: 0.7760\n",
            "Epoch 172/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4960 - acc: 0.7639 - val_loss: 0.4993 - val_acc: 0.7760\n",
            "Epoch 173/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4956 - acc: 0.7622 - val_loss: 0.4991 - val_acc: 0.7812\n",
            "Epoch 174/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4953 - acc: 0.7604 - val_loss: 0.4988 - val_acc: 0.7760\n",
            "Epoch 175/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4949 - acc: 0.7622 - val_loss: 0.4986 - val_acc: 0.7760\n",
            "Epoch 176/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4945 - acc: 0.7604 - val_loss: 0.4984 - val_acc: 0.7760\n",
            "Epoch 177/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4942 - acc: 0.7622 - val_loss: 0.4982 - val_acc: 0.7760\n",
            "Epoch 178/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4938 - acc: 0.7622 - val_loss: 0.4980 - val_acc: 0.7760\n",
            "Epoch 179/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4935 - acc: 0.7622 - val_loss: 0.4978 - val_acc: 0.7760\n",
            "Epoch 180/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4931 - acc: 0.7604 - val_loss: 0.4976 - val_acc: 0.7760\n",
            "Epoch 181/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4928 - acc: 0.7622 - val_loss: 0.4975 - val_acc: 0.7760\n",
            "Epoch 182/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4924 - acc: 0.7622 - val_loss: 0.4973 - val_acc: 0.7760\n",
            "Epoch 183/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4921 - acc: 0.7622 - val_loss: 0.4971 - val_acc: 0.7760\n",
            "Epoch 184/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4918 - acc: 0.7622 - val_loss: 0.4969 - val_acc: 0.7760\n",
            "Epoch 185/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4915 - acc: 0.7587 - val_loss: 0.4968 - val_acc: 0.7760\n",
            "Epoch 186/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4911 - acc: 0.7604 - val_loss: 0.4966 - val_acc: 0.7760\n",
            "Epoch 187/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4908 - acc: 0.7604 - val_loss: 0.4964 - val_acc: 0.7708\n",
            "Epoch 188/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4905 - acc: 0.7587 - val_loss: 0.4962 - val_acc: 0.7708\n",
            "Epoch 189/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4902 - acc: 0.7604 - val_loss: 0.4961 - val_acc: 0.7656\n",
            "Epoch 190/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4898 - acc: 0.7604 - val_loss: 0.4959 - val_acc: 0.7656\n",
            "Epoch 191/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4895 - acc: 0.7604 - val_loss: 0.4958 - val_acc: 0.7656\n",
            "Epoch 192/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4892 - acc: 0.7604 - val_loss: 0.4957 - val_acc: 0.7656\n",
            "Epoch 193/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4889 - acc: 0.7587 - val_loss: 0.4955 - val_acc: 0.7656\n",
            "Epoch 194/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4886 - acc: 0.7604 - val_loss: 0.4954 - val_acc: 0.7656\n",
            "Epoch 195/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4883 - acc: 0.7604 - val_loss: 0.4952 - val_acc: 0.7656\n",
            "Epoch 196/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4881 - acc: 0.7604 - val_loss: 0.4951 - val_acc: 0.7656\n",
            "Epoch 197/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4878 - acc: 0.7604 - val_loss: 0.4950 - val_acc: 0.7656\n",
            "Epoch 198/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4875 - acc: 0.7604 - val_loss: 0.4949 - val_acc: 0.7656\n",
            "Epoch 199/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4872 - acc: 0.7604 - val_loss: 0.4948 - val_acc: 0.7656\n",
            "Epoch 200/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4870 - acc: 0.7604 - val_loss: 0.4946 - val_acc: 0.7656\n",
            "Epoch 201/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4867 - acc: 0.7604 - val_loss: 0.4945 - val_acc: 0.7656\n",
            "Epoch 202/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4864 - acc: 0.7587 - val_loss: 0.4944 - val_acc: 0.7656\n",
            "Epoch 203/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4861 - acc: 0.7622 - val_loss: 0.4943 - val_acc: 0.7708\n",
            "Epoch 204/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4858 - acc: 0.7622 - val_loss: 0.4942 - val_acc: 0.7708\n",
            "Epoch 205/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4855 - acc: 0.7622 - val_loss: 0.4941 - val_acc: 0.7708\n",
            "Epoch 206/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4853 - acc: 0.7622 - val_loss: 0.4940 - val_acc: 0.7708\n",
            "Epoch 207/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4851 - acc: 0.7622 - val_loss: 0.4939 - val_acc: 0.7708\n",
            "Epoch 208/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4847 - acc: 0.7622 - val_loss: 0.4938 - val_acc: 0.7708\n",
            "Epoch 209/1500\n",
            "576/576 [==============================] - 0s 59us/step - loss: 0.4845 - acc: 0.7622 - val_loss: 0.4937 - val_acc: 0.7708\n",
            "Epoch 210/1500\n",
            "576/576 [==============================] - 0s 65us/step - loss: 0.4842 - acc: 0.7622 - val_loss: 0.4936 - val_acc: 0.7708\n",
            "Epoch 211/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4840 - acc: 0.7622 - val_loss: 0.4935 - val_acc: 0.7708\n",
            "Epoch 212/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4837 - acc: 0.7604 - val_loss: 0.4934 - val_acc: 0.7708\n",
            "Epoch 213/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4835 - acc: 0.7604 - val_loss: 0.4933 - val_acc: 0.7708\n",
            "Epoch 214/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4832 - acc: 0.7604 - val_loss: 0.4932 - val_acc: 0.7708\n",
            "Epoch 215/1500\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4829 - acc: 0.7604 - val_loss: 0.4931 - val_acc: 0.7708\n",
            "Epoch 216/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4827 - acc: 0.7604 - val_loss: 0.4930 - val_acc: 0.7708\n",
            "Epoch 217/1500\n",
            "576/576 [==============================] - 0s 66us/step - loss: 0.4824 - acc: 0.7604 - val_loss: 0.4929 - val_acc: 0.7708\n",
            "Epoch 218/1500\n",
            "576/576 [==============================] - 0s 61us/step - loss: 0.4822 - acc: 0.7604 - val_loss: 0.4928 - val_acc: 0.7708\n",
            "Epoch 219/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4819 - acc: 0.7604 - val_loss: 0.4928 - val_acc: 0.7708\n",
            "Epoch 220/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4817 - acc: 0.7604 - val_loss: 0.4927 - val_acc: 0.7708\n",
            "Epoch 221/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4814 - acc: 0.7587 - val_loss: 0.4926 - val_acc: 0.7708\n",
            "Epoch 222/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4812 - acc: 0.7587 - val_loss: 0.4925 - val_acc: 0.7708\n",
            "Epoch 223/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4810 - acc: 0.7587 - val_loss: 0.4925 - val_acc: 0.7708\n",
            "Epoch 224/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4807 - acc: 0.7587 - val_loss: 0.4924 - val_acc: 0.7708\n",
            "Epoch 225/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4805 - acc: 0.7569 - val_loss: 0.4923 - val_acc: 0.7708\n",
            "Epoch 226/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4803 - acc: 0.7569 - val_loss: 0.4923 - val_acc: 0.7708\n",
            "Epoch 227/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4800 - acc: 0.7569 - val_loss: 0.4922 - val_acc: 0.7708\n",
            "Epoch 228/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4798 - acc: 0.7552 - val_loss: 0.4922 - val_acc: 0.7708\n",
            "Epoch 229/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4797 - acc: 0.7552 - val_loss: 0.4921 - val_acc: 0.7708\n",
            "Epoch 230/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4795 - acc: 0.7552 - val_loss: 0.4921 - val_acc: 0.7708\n",
            "Epoch 231/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4793 - acc: 0.7552 - val_loss: 0.4920 - val_acc: 0.7656\n",
            "Epoch 232/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4791 - acc: 0.7552 - val_loss: 0.4920 - val_acc: 0.7708\n",
            "Epoch 233/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4789 - acc: 0.7552 - val_loss: 0.4919 - val_acc: 0.7708\n",
            "Epoch 234/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4787 - acc: 0.7552 - val_loss: 0.4919 - val_acc: 0.7708\n",
            "Epoch 235/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4785 - acc: 0.7552 - val_loss: 0.4918 - val_acc: 0.7760\n",
            "Epoch 236/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4783 - acc: 0.7569 - val_loss: 0.4918 - val_acc: 0.7760\n",
            "Epoch 237/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4782 - acc: 0.7569 - val_loss: 0.4917 - val_acc: 0.7760\n",
            "Epoch 238/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4779 - acc: 0.7552 - val_loss: 0.4917 - val_acc: 0.7760\n",
            "Epoch 239/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4778 - acc: 0.7552 - val_loss: 0.4916 - val_acc: 0.7760\n",
            "Epoch 240/1500\n",
            "576/576 [==============================] - 0s 60us/step - loss: 0.4775 - acc: 0.7569 - val_loss: 0.4916 - val_acc: 0.7760\n",
            "Epoch 241/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4774 - acc: 0.7552 - val_loss: 0.4915 - val_acc: 0.7760\n",
            "Epoch 242/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4772 - acc: 0.7552 - val_loss: 0.4915 - val_acc: 0.7760\n",
            "Epoch 243/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4770 - acc: 0.7552 - val_loss: 0.4914 - val_acc: 0.7760\n",
            "Epoch 244/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4768 - acc: 0.7569 - val_loss: 0.4914 - val_acc: 0.7760\n",
            "Epoch 245/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4767 - acc: 0.7587 - val_loss: 0.4914 - val_acc: 0.7760\n",
            "Epoch 246/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4765 - acc: 0.7587 - val_loss: 0.4913 - val_acc: 0.7760\n",
            "Epoch 247/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4763 - acc: 0.7587 - val_loss: 0.4913 - val_acc: 0.7708\n",
            "Epoch 248/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4761 - acc: 0.7604 - val_loss: 0.4912 - val_acc: 0.7708\n",
            "Epoch 249/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4760 - acc: 0.7587 - val_loss: 0.4912 - val_acc: 0.7708\n",
            "Epoch 250/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4758 - acc: 0.7604 - val_loss: 0.4912 - val_acc: 0.7708\n",
            "Epoch 251/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4756 - acc: 0.7569 - val_loss: 0.4911 - val_acc: 0.7708\n",
            "Epoch 252/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4755 - acc: 0.7569 - val_loss: 0.4911 - val_acc: 0.7708\n",
            "Epoch 253/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4752 - acc: 0.7604 - val_loss: 0.4911 - val_acc: 0.7708\n",
            "Epoch 254/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4751 - acc: 0.7604 - val_loss: 0.4910 - val_acc: 0.7708\n",
            "Epoch 255/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4749 - acc: 0.7587 - val_loss: 0.4910 - val_acc: 0.7708\n",
            "Epoch 256/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4748 - acc: 0.7604 - val_loss: 0.4910 - val_acc: 0.7708\n",
            "Epoch 257/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4746 - acc: 0.7604 - val_loss: 0.4910 - val_acc: 0.7708\n",
            "Epoch 258/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4745 - acc: 0.7604 - val_loss: 0.4909 - val_acc: 0.7708\n",
            "Epoch 259/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4743 - acc: 0.7622 - val_loss: 0.4909 - val_acc: 0.7708\n",
            "Epoch 260/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4741 - acc: 0.7622 - val_loss: 0.4909 - val_acc: 0.7708\n",
            "Epoch 261/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4740 - acc: 0.7622 - val_loss: 0.4909 - val_acc: 0.7708\n",
            "Epoch 262/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4738 - acc: 0.7622 - val_loss: 0.4909 - val_acc: 0.7708\n",
            "Epoch 263/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4737 - acc: 0.7622 - val_loss: 0.4909 - val_acc: 0.7708\n",
            "Epoch 264/1500\n",
            "576/576 [==============================] - 0s 61us/step - loss: 0.4735 - acc: 0.7622 - val_loss: 0.4909 - val_acc: 0.7708\n",
            "Epoch 265/1500\n",
            "576/576 [==============================] - 0s 62us/step - loss: 0.4734 - acc: 0.7622 - val_loss: 0.4909 - val_acc: 0.7708\n",
            "Epoch 266/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4733 - acc: 0.7622 - val_loss: 0.4909 - val_acc: 0.7760\n",
            "Epoch 267/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4731 - acc: 0.7622 - val_loss: 0.4909 - val_acc: 0.7760\n",
            "Epoch 268/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4729 - acc: 0.7622 - val_loss: 0.4908 - val_acc: 0.7760\n",
            "Epoch 269/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4728 - acc: 0.7622 - val_loss: 0.4908 - val_acc: 0.7760\n",
            "Epoch 270/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4726 - acc: 0.7639 - val_loss: 0.4908 - val_acc: 0.7760\n",
            "Epoch 271/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4724 - acc: 0.7639 - val_loss: 0.4908 - val_acc: 0.7812\n",
            "Epoch 272/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4723 - acc: 0.7656 - val_loss: 0.4908 - val_acc: 0.7812\n",
            "Epoch 273/1500\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4722 - acc: 0.7639 - val_loss: 0.4908 - val_acc: 0.7812\n",
            "Epoch 274/1500\n",
            "576/576 [==============================] - 0s 59us/step - loss: 0.4720 - acc: 0.7639 - val_loss: 0.4908 - val_acc: 0.7812\n",
            "Epoch 275/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4719 - acc: 0.7656 - val_loss: 0.4908 - val_acc: 0.7865\n",
            "Epoch 276/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4718 - acc: 0.7656 - val_loss: 0.4908 - val_acc: 0.7865\n",
            "Epoch 277/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4716 - acc: 0.7656 - val_loss: 0.4908 - val_acc: 0.7865\n",
            "Epoch 278/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4715 - acc: 0.7656 - val_loss: 0.4908 - val_acc: 0.7865\n",
            "Epoch 279/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4714 - acc: 0.7656 - val_loss: 0.4908 - val_acc: 0.7865\n",
            "Epoch 280/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4713 - acc: 0.7656 - val_loss: 0.4908 - val_acc: 0.7865\n",
            "Epoch 281/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4711 - acc: 0.7656 - val_loss: 0.4908 - val_acc: 0.7865\n",
            "Epoch 282/1500\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.4709 - acc: 0.7656 - val_loss: 0.4908 - val_acc: 0.7865\n",
            "Epoch 283/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4708 - acc: 0.7656 - val_loss: 0.4908 - val_acc: 0.7865\n",
            "Epoch 284/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4707 - acc: 0.7656 - val_loss: 0.4908 - val_acc: 0.7865\n",
            "Epoch 285/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4705 - acc: 0.7656 - val_loss: 0.4908 - val_acc: 0.7865\n",
            "Epoch 286/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4704 - acc: 0.7674 - val_loss: 0.4908 - val_acc: 0.7865\n",
            "Epoch 287/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4703 - acc: 0.7674 - val_loss: 0.4908 - val_acc: 0.7865\n",
            "Epoch 288/1500\n",
            "576/576 [==============================] - 0s 61us/step - loss: 0.4702 - acc: 0.7674 - val_loss: 0.4907 - val_acc: 0.7865\n",
            "Epoch 289/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4700 - acc: 0.7674 - val_loss: 0.4907 - val_acc: 0.7865\n",
            "Epoch 290/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4699 - acc: 0.7674 - val_loss: 0.4907 - val_acc: 0.7865\n",
            "Epoch 291/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4698 - acc: 0.7674 - val_loss: 0.4907 - val_acc: 0.7865\n",
            "Epoch 292/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4696 - acc: 0.7656 - val_loss: 0.4907 - val_acc: 0.7865\n",
            "Epoch 293/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4695 - acc: 0.7656 - val_loss: 0.4907 - val_acc: 0.7865\n",
            "Epoch 294/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4694 - acc: 0.7656 - val_loss: 0.4907 - val_acc: 0.7865\n",
            "Epoch 295/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4693 - acc: 0.7674 - val_loss: 0.4907 - val_acc: 0.7865\n",
            "Epoch 296/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4692 - acc: 0.7691 - val_loss: 0.4907 - val_acc: 0.7865\n",
            "Epoch 297/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4690 - acc: 0.7691 - val_loss: 0.4907 - val_acc: 0.7865\n",
            "Epoch 298/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4689 - acc: 0.7691 - val_loss: 0.4907 - val_acc: 0.7865\n",
            "Epoch 299/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4688 - acc: 0.7674 - val_loss: 0.4907 - val_acc: 0.7865\n",
            "Epoch 300/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4687 - acc: 0.7674 - val_loss: 0.4907 - val_acc: 0.7865\n",
            "Epoch 301/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4685 - acc: 0.7691 - val_loss: 0.4907 - val_acc: 0.7917\n",
            "Epoch 302/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4684 - acc: 0.7691 - val_loss: 0.4907 - val_acc: 0.7917\n",
            "Epoch 303/1500\n",
            "576/576 [==============================] - 0s 67us/step - loss: 0.4683 - acc: 0.7691 - val_loss: 0.4907 - val_acc: 0.7917\n",
            "Epoch 304/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4682 - acc: 0.7674 - val_loss: 0.4907 - val_acc: 0.7917\n",
            "Epoch 305/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4681 - acc: 0.7691 - val_loss: 0.4907 - val_acc: 0.7917\n",
            "Epoch 306/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4680 - acc: 0.7708 - val_loss: 0.4907 - val_acc: 0.7917\n",
            "Epoch 307/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4679 - acc: 0.7674 - val_loss: 0.4907 - val_acc: 0.7865\n",
            "Epoch 308/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4677 - acc: 0.7691 - val_loss: 0.4907 - val_acc: 0.7865\n",
            "Epoch 309/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4676 - acc: 0.7691 - val_loss: 0.4906 - val_acc: 0.7865\n",
            "Epoch 310/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4675 - acc: 0.7708 - val_loss: 0.4906 - val_acc: 0.7865\n",
            "Epoch 311/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4674 - acc: 0.7691 - val_loss: 0.4906 - val_acc: 0.7865\n",
            "Epoch 312/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4673 - acc: 0.7691 - val_loss: 0.4906 - val_acc: 0.7917\n",
            "Epoch 313/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4671 - acc: 0.7691 - val_loss: 0.4906 - val_acc: 0.7917\n",
            "Epoch 314/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4671 - acc: 0.7708 - val_loss: 0.4906 - val_acc: 0.7917\n",
            "Epoch 315/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4670 - acc: 0.7691 - val_loss: 0.4906 - val_acc: 0.7865\n",
            "Epoch 316/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4668 - acc: 0.7691 - val_loss: 0.4906 - val_acc: 0.7917\n",
            "Epoch 317/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4667 - acc: 0.7726 - val_loss: 0.4906 - val_acc: 0.7917\n",
            "Epoch 318/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4666 - acc: 0.7726 - val_loss: 0.4906 - val_acc: 0.7917\n",
            "Epoch 319/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4665 - acc: 0.7726 - val_loss: 0.4906 - val_acc: 0.7917\n",
            "Epoch 320/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4663 - acc: 0.7708 - val_loss: 0.4906 - val_acc: 0.7917\n",
            "Epoch 321/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4663 - acc: 0.7743 - val_loss: 0.4906 - val_acc: 0.7917\n",
            "Epoch 322/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4661 - acc: 0.7760 - val_loss: 0.4906 - val_acc: 0.7917\n",
            "Epoch 323/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4660 - acc: 0.7760 - val_loss: 0.4907 - val_acc: 0.7917\n",
            "Epoch 324/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4658 - acc: 0.7760 - val_loss: 0.4907 - val_acc: 0.7865\n",
            "Epoch 325/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4657 - acc: 0.7760 - val_loss: 0.4907 - val_acc: 0.7865\n",
            "Epoch 326/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4657 - acc: 0.7760 - val_loss: 0.4907 - val_acc: 0.7865\n",
            "Epoch 327/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4654 - acc: 0.7760 - val_loss: 0.4907 - val_acc: 0.7865\n",
            "Epoch 328/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4653 - acc: 0.7760 - val_loss: 0.4907 - val_acc: 0.7865\n",
            "Epoch 329/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4652 - acc: 0.7760 - val_loss: 0.4907 - val_acc: 0.7865\n",
            "Epoch 330/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4651 - acc: 0.7760 - val_loss: 0.4907 - val_acc: 0.7865\n",
            "Epoch 331/1500\n",
            "576/576 [==============================] - 0s 59us/step - loss: 0.4650 - acc: 0.7760 - val_loss: 0.4907 - val_acc: 0.7865\n",
            "Epoch 332/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4649 - acc: 0.7760 - val_loss: 0.4907 - val_acc: 0.7865\n",
            "Epoch 333/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4647 - acc: 0.7760 - val_loss: 0.4907 - val_acc: 0.7865\n",
            "Epoch 334/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4646 - acc: 0.7760 - val_loss: 0.4907 - val_acc: 0.7865\n",
            "Epoch 335/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4645 - acc: 0.7760 - val_loss: 0.4908 - val_acc: 0.7865\n",
            "Epoch 336/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4643 - acc: 0.7743 - val_loss: 0.4908 - val_acc: 0.7865\n",
            "Epoch 337/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4642 - acc: 0.7760 - val_loss: 0.4908 - val_acc: 0.7865\n",
            "Epoch 338/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4641 - acc: 0.7760 - val_loss: 0.4908 - val_acc: 0.7865\n",
            "Epoch 339/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4640 - acc: 0.7760 - val_loss: 0.4908 - val_acc: 0.7865\n",
            "Epoch 340/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4638 - acc: 0.7760 - val_loss: 0.4908 - val_acc: 0.7865\n",
            "Epoch 341/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4637 - acc: 0.7760 - val_loss: 0.4908 - val_acc: 0.7865\n",
            "Epoch 342/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4636 - acc: 0.7760 - val_loss: 0.4909 - val_acc: 0.7865\n",
            "Epoch 343/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4635 - acc: 0.7760 - val_loss: 0.4909 - val_acc: 0.7865\n",
            "Epoch 344/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4633 - acc: 0.7760 - val_loss: 0.4909 - val_acc: 0.7865\n",
            "Epoch 345/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4632 - acc: 0.7760 - val_loss: 0.4909 - val_acc: 0.7865\n",
            "Epoch 346/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4630 - acc: 0.7778 - val_loss: 0.4910 - val_acc: 0.7865\n",
            "Epoch 347/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4629 - acc: 0.7778 - val_loss: 0.4910 - val_acc: 0.7865\n",
            "Epoch 348/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4628 - acc: 0.7778 - val_loss: 0.4910 - val_acc: 0.7865\n",
            "Epoch 349/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4626 - acc: 0.7778 - val_loss: 0.4910 - val_acc: 0.7865\n",
            "Epoch 350/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4626 - acc: 0.7778 - val_loss: 0.4910 - val_acc: 0.7865\n",
            "Epoch 351/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4624 - acc: 0.7778 - val_loss: 0.4911 - val_acc: 0.7865\n",
            "Epoch 352/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4623 - acc: 0.7778 - val_loss: 0.4911 - val_acc: 0.7865\n",
            "Epoch 353/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4621 - acc: 0.7778 - val_loss: 0.4911 - val_acc: 0.7812\n",
            "Epoch 354/1500\n",
            "576/576 [==============================] - 0s 62us/step - loss: 0.4620 - acc: 0.7760 - val_loss: 0.4911 - val_acc: 0.7812\n",
            "Epoch 355/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4619 - acc: 0.7760 - val_loss: 0.4912 - val_acc: 0.7812\n",
            "Epoch 356/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4618 - acc: 0.7760 - val_loss: 0.4912 - val_acc: 0.7812\n",
            "Epoch 357/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4617 - acc: 0.7760 - val_loss: 0.4912 - val_acc: 0.7812\n",
            "Epoch 358/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4615 - acc: 0.7743 - val_loss: 0.4912 - val_acc: 0.7812\n",
            "Epoch 359/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4614 - acc: 0.7743 - val_loss: 0.4913 - val_acc: 0.7812\n",
            "Epoch 360/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4613 - acc: 0.7743 - val_loss: 0.4913 - val_acc: 0.7812\n",
            "Epoch 361/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4611 - acc: 0.7743 - val_loss: 0.4913 - val_acc: 0.7760\n",
            "Epoch 362/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4610 - acc: 0.7743 - val_loss: 0.4913 - val_acc: 0.7760\n",
            "Epoch 363/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4609 - acc: 0.7726 - val_loss: 0.4914 - val_acc: 0.7760\n",
            "Epoch 364/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4608 - acc: 0.7743 - val_loss: 0.4914 - val_acc: 0.7812\n",
            "Epoch 365/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4607 - acc: 0.7760 - val_loss: 0.4914 - val_acc: 0.7812\n",
            "Epoch 366/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4606 - acc: 0.7743 - val_loss: 0.4915 - val_acc: 0.7812\n",
            "Epoch 367/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4606 - acc: 0.7726 - val_loss: 0.4915 - val_acc: 0.7760\n",
            "Epoch 368/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4604 - acc: 0.7726 - val_loss: 0.4915 - val_acc: 0.7760\n",
            "Epoch 369/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4603 - acc: 0.7743 - val_loss: 0.4915 - val_acc: 0.7760\n",
            "Epoch 370/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4602 - acc: 0.7726 - val_loss: 0.4916 - val_acc: 0.7812\n",
            "Epoch 371/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4600 - acc: 0.7726 - val_loss: 0.4916 - val_acc: 0.7760\n",
            "Epoch 372/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4600 - acc: 0.7726 - val_loss: 0.4916 - val_acc: 0.7760\n",
            "Epoch 373/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4599 - acc: 0.7743 - val_loss: 0.4917 - val_acc: 0.7760\n",
            "Epoch 374/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4598 - acc: 0.7726 - val_loss: 0.4917 - val_acc: 0.7812\n",
            "Epoch 375/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4597 - acc: 0.7726 - val_loss: 0.4917 - val_acc: 0.7812\n",
            "Epoch 376/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4596 - acc: 0.7743 - val_loss: 0.4918 - val_acc: 0.7812\n",
            "Epoch 377/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4595 - acc: 0.7726 - val_loss: 0.4918 - val_acc: 0.7812\n",
            "Epoch 378/1500\n",
            "576/576 [==============================] - 0s 60us/step - loss: 0.4593 - acc: 0.7726 - val_loss: 0.4918 - val_acc: 0.7812\n",
            "Epoch 379/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4592 - acc: 0.7726 - val_loss: 0.4919 - val_acc: 0.7812\n",
            "Epoch 380/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4591 - acc: 0.7726 - val_loss: 0.4919 - val_acc: 0.7812\n",
            "Epoch 381/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4590 - acc: 0.7726 - val_loss: 0.4919 - val_acc: 0.7812\n",
            "Epoch 382/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4589 - acc: 0.7726 - val_loss: 0.4920 - val_acc: 0.7812\n",
            "Epoch 383/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4588 - acc: 0.7743 - val_loss: 0.4920 - val_acc: 0.7812\n",
            "Epoch 384/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4587 - acc: 0.7726 - val_loss: 0.4920 - val_acc: 0.7812\n",
            "Epoch 385/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4586 - acc: 0.7726 - val_loss: 0.4921 - val_acc: 0.7812\n",
            "Epoch 386/1500\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4585 - acc: 0.7708 - val_loss: 0.4921 - val_acc: 0.7812\n",
            "Epoch 387/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4583 - acc: 0.7708 - val_loss: 0.4921 - val_acc: 0.7812\n",
            "Epoch 388/1500\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4582 - acc: 0.7691 - val_loss: 0.4922 - val_acc: 0.7812\n",
            "Epoch 389/1500\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4581 - acc: 0.7691 - val_loss: 0.4922 - val_acc: 0.7812\n",
            "Epoch 390/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4580 - acc: 0.7691 - val_loss: 0.4922 - val_acc: 0.7812\n",
            "Epoch 391/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4579 - acc: 0.7691 - val_loss: 0.4923 - val_acc: 0.7812\n",
            "Epoch 392/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4578 - acc: 0.7708 - val_loss: 0.4923 - val_acc: 0.7812\n",
            "Epoch 393/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4576 - acc: 0.7691 - val_loss: 0.4923 - val_acc: 0.7812\n",
            "Epoch 394/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4575 - acc: 0.7691 - val_loss: 0.4924 - val_acc: 0.7812\n",
            "Epoch 395/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4574 - acc: 0.7708 - val_loss: 0.4924 - val_acc: 0.7812\n",
            "Epoch 396/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4573 - acc: 0.7691 - val_loss: 0.4924 - val_acc: 0.7812\n",
            "Epoch 397/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4572 - acc: 0.7691 - val_loss: 0.4925 - val_acc: 0.7812\n",
            "Epoch 398/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4571 - acc: 0.7691 - val_loss: 0.4925 - val_acc: 0.7812\n",
            "Epoch 399/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4569 - acc: 0.7691 - val_loss: 0.4925 - val_acc: 0.7812\n",
            "Epoch 400/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4569 - acc: 0.7691 - val_loss: 0.4926 - val_acc: 0.7812\n",
            "Epoch 401/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4567 - acc: 0.7708 - val_loss: 0.4926 - val_acc: 0.7812\n",
            "Epoch 402/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4566 - acc: 0.7708 - val_loss: 0.4926 - val_acc: 0.7812\n",
            "Epoch 403/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4565 - acc: 0.7691 - val_loss: 0.4927 - val_acc: 0.7812\n",
            "Epoch 404/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4563 - acc: 0.7691 - val_loss: 0.4927 - val_acc: 0.7812\n",
            "Epoch 405/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4563 - acc: 0.7708 - val_loss: 0.4928 - val_acc: 0.7812\n",
            "Epoch 406/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4562 - acc: 0.7726 - val_loss: 0.4928 - val_acc: 0.7812\n",
            "Epoch 407/1500\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4560 - acc: 0.7708 - val_loss: 0.4929 - val_acc: 0.7812\n",
            "Epoch 408/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4559 - acc: 0.7726 - val_loss: 0.4929 - val_acc: 0.7812\n",
            "Epoch 409/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4558 - acc: 0.7691 - val_loss: 0.4930 - val_acc: 0.7812\n",
            "Epoch 410/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4557 - acc: 0.7691 - val_loss: 0.4930 - val_acc: 0.7812\n",
            "Epoch 411/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4556 - acc: 0.7691 - val_loss: 0.4931 - val_acc: 0.7812\n",
            "Epoch 412/1500\n",
            "576/576 [==============================] - 0s 39us/step - loss: 0.4555 - acc: 0.7691 - val_loss: 0.4931 - val_acc: 0.7812\n",
            "Epoch 413/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4554 - acc: 0.7674 - val_loss: 0.4931 - val_acc: 0.7812\n",
            "Epoch 414/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4553 - acc: 0.7691 - val_loss: 0.4932 - val_acc: 0.7812\n",
            "Epoch 415/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4552 - acc: 0.7674 - val_loss: 0.4932 - val_acc: 0.7812\n",
            "Epoch 416/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4551 - acc: 0.7691 - val_loss: 0.4933 - val_acc: 0.7812\n",
            "Epoch 417/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4550 - acc: 0.7674 - val_loss: 0.4933 - val_acc: 0.7812\n",
            "Epoch 418/1500\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.4549 - acc: 0.7691 - val_loss: 0.4934 - val_acc: 0.7812\n",
            "Epoch 419/1500\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4547 - acc: 0.7674 - val_loss: 0.4934 - val_acc: 0.7812\n",
            "Epoch 420/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4547 - acc: 0.7674 - val_loss: 0.4935 - val_acc: 0.7812\n",
            "Epoch 421/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4546 - acc: 0.7691 - val_loss: 0.4935 - val_acc: 0.7812\n",
            "Epoch 422/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4545 - acc: 0.7691 - val_loss: 0.4936 - val_acc: 0.7812\n",
            "Epoch 423/1500\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.4544 - acc: 0.7691 - val_loss: 0.4936 - val_acc: 0.7812\n",
            "Epoch 424/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4542 - acc: 0.7691 - val_loss: 0.4937 - val_acc: 0.7812\n",
            "Epoch 425/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4542 - acc: 0.7691 - val_loss: 0.4937 - val_acc: 0.7812\n",
            "Epoch 426/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4540 - acc: 0.7691 - val_loss: 0.4938 - val_acc: 0.7812\n",
            "Epoch 427/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4539 - acc: 0.7691 - val_loss: 0.4938 - val_acc: 0.7812\n",
            "Epoch 428/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4539 - acc: 0.7691 - val_loss: 0.4939 - val_acc: 0.7812\n",
            "Epoch 429/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4538 - acc: 0.7691 - val_loss: 0.4939 - val_acc: 0.7812\n",
            "Epoch 430/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4537 - acc: 0.7691 - val_loss: 0.4940 - val_acc: 0.7812\n",
            "Epoch 431/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4535 - acc: 0.7708 - val_loss: 0.4941 - val_acc: 0.7812\n",
            "Epoch 432/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4535 - acc: 0.7708 - val_loss: 0.4941 - val_acc: 0.7812\n",
            "Epoch 433/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4534 - acc: 0.7708 - val_loss: 0.4942 - val_acc: 0.7812\n",
            "Epoch 434/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4533 - acc: 0.7708 - val_loss: 0.4942 - val_acc: 0.7812\n",
            "Epoch 435/1500\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4532 - acc: 0.7708 - val_loss: 0.4943 - val_acc: 0.7812\n",
            "Epoch 436/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4531 - acc: 0.7708 - val_loss: 0.4943 - val_acc: 0.7812\n",
            "Epoch 437/1500\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4530 - acc: 0.7708 - val_loss: 0.4944 - val_acc: 0.7812\n",
            "Epoch 438/1500\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4529 - acc: 0.7708 - val_loss: 0.4944 - val_acc: 0.7812\n",
            "Epoch 439/1500\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4528 - acc: 0.7726 - val_loss: 0.4945 - val_acc: 0.7812\n",
            "Epoch 440/1500\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4527 - acc: 0.7743 - val_loss: 0.4945 - val_acc: 0.7812\n",
            "Epoch 441/1500\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4526 - acc: 0.7743 - val_loss: 0.4946 - val_acc: 0.7812\n",
            "Epoch 442/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4525 - acc: 0.7743 - val_loss: 0.4947 - val_acc: 0.7812\n",
            "Epoch 443/1500\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4524 - acc: 0.7760 - val_loss: 0.4947 - val_acc: 0.7812\n",
            "Epoch 444/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4524 - acc: 0.7743 - val_loss: 0.4948 - val_acc: 0.7812\n",
            "Epoch 445/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4522 - acc: 0.7743 - val_loss: 0.4948 - val_acc: 0.7812\n",
            "Epoch 446/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4521 - acc: 0.7760 - val_loss: 0.4949 - val_acc: 0.7812\n",
            "Epoch 447/1500\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4520 - acc: 0.7778 - val_loss: 0.4949 - val_acc: 0.7812\n",
            "Epoch 448/1500\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4519 - acc: 0.7778 - val_loss: 0.4950 - val_acc: 0.7812\n",
            "Epoch 449/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4518 - acc: 0.7778 - val_loss: 0.4950 - val_acc: 0.7812\n",
            "Epoch 450/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4517 - acc: 0.7778 - val_loss: 0.4951 - val_acc: 0.7812\n",
            "Epoch 451/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4516 - acc: 0.7778 - val_loss: 0.4951 - val_acc: 0.7812\n",
            "Epoch 452/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4516 - acc: 0.7778 - val_loss: 0.4952 - val_acc: 0.7812\n",
            "Epoch 453/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4515 - acc: 0.7778 - val_loss: 0.4952 - val_acc: 0.7812\n",
            "Epoch 454/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4514 - acc: 0.7778 - val_loss: 0.4952 - val_acc: 0.7812\n",
            "Epoch 455/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4512 - acc: 0.7778 - val_loss: 0.4953 - val_acc: 0.7812\n",
            "Epoch 456/1500\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4512 - acc: 0.7778 - val_loss: 0.4953 - val_acc: 0.7812\n",
            "Epoch 457/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4511 - acc: 0.7778 - val_loss: 0.4954 - val_acc: 0.7812\n",
            "Epoch 458/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4510 - acc: 0.7795 - val_loss: 0.4954 - val_acc: 0.7812\n",
            "Epoch 459/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4509 - acc: 0.7778 - val_loss: 0.4955 - val_acc: 0.7812\n",
            "Epoch 460/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4508 - acc: 0.7778 - val_loss: 0.4955 - val_acc: 0.7812\n",
            "Epoch 461/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4507 - acc: 0.7778 - val_loss: 0.4956 - val_acc: 0.7812\n",
            "Epoch 462/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4506 - acc: 0.7778 - val_loss: 0.4956 - val_acc: 0.7812\n",
            "Epoch 463/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4505 - acc: 0.7795 - val_loss: 0.4957 - val_acc: 0.7812\n",
            "Epoch 464/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4505 - acc: 0.7795 - val_loss: 0.4957 - val_acc: 0.7812\n",
            "Epoch 465/1500\n",
            "576/576 [==============================] - 0s 66us/step - loss: 0.4503 - acc: 0.7795 - val_loss: 0.4958 - val_acc: 0.7812\n",
            "Epoch 466/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4503 - acc: 0.7778 - val_loss: 0.4958 - val_acc: 0.7812\n",
            "Epoch 467/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4502 - acc: 0.7795 - val_loss: 0.4959 - val_acc: 0.7812\n",
            "Epoch 468/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4501 - acc: 0.7795 - val_loss: 0.4959 - val_acc: 0.7812\n",
            "Epoch 469/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4500 - acc: 0.7795 - val_loss: 0.4960 - val_acc: 0.7812\n",
            "Epoch 470/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4500 - acc: 0.7812 - val_loss: 0.4960 - val_acc: 0.7812\n",
            "Epoch 471/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4498 - acc: 0.7795 - val_loss: 0.4961 - val_acc: 0.7812\n",
            "Epoch 472/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4497 - acc: 0.7795 - val_loss: 0.4961 - val_acc: 0.7812\n",
            "Epoch 473/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4497 - acc: 0.7812 - val_loss: 0.4961 - val_acc: 0.7812\n",
            "Epoch 474/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4496 - acc: 0.7812 - val_loss: 0.4962 - val_acc: 0.7812\n",
            "Epoch 475/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4494 - acc: 0.7812 - val_loss: 0.4962 - val_acc: 0.7812\n",
            "Epoch 476/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4494 - acc: 0.7812 - val_loss: 0.4963 - val_acc: 0.7812\n",
            "Epoch 477/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4492 - acc: 0.7812 - val_loss: 0.4963 - val_acc: 0.7812\n",
            "Epoch 478/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4491 - acc: 0.7812 - val_loss: 0.4964 - val_acc: 0.7812\n",
            "Epoch 479/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4491 - acc: 0.7812 - val_loss: 0.4964 - val_acc: 0.7812\n",
            "Epoch 480/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4490 - acc: 0.7812 - val_loss: 0.4965 - val_acc: 0.7812\n",
            "Epoch 481/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4489 - acc: 0.7812 - val_loss: 0.4965 - val_acc: 0.7812\n",
            "Epoch 482/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4488 - acc: 0.7812 - val_loss: 0.4966 - val_acc: 0.7812\n",
            "Epoch 483/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4487 - acc: 0.7812 - val_loss: 0.4966 - val_acc: 0.7812\n",
            "Epoch 484/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4487 - acc: 0.7830 - val_loss: 0.4967 - val_acc: 0.7812\n",
            "Epoch 485/1500\n",
            "576/576 [==============================] - 0s 62us/step - loss: 0.4486 - acc: 0.7830 - val_loss: 0.4967 - val_acc: 0.7812\n",
            "Epoch 486/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4485 - acc: 0.7812 - val_loss: 0.4968 - val_acc: 0.7812\n",
            "Epoch 487/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4483 - acc: 0.7812 - val_loss: 0.4968 - val_acc: 0.7812\n",
            "Epoch 488/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4483 - acc: 0.7812 - val_loss: 0.4969 - val_acc: 0.7812\n",
            "Epoch 489/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4482 - acc: 0.7812 - val_loss: 0.4969 - val_acc: 0.7812\n",
            "Epoch 490/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4482 - acc: 0.7830 - val_loss: 0.4970 - val_acc: 0.7812\n",
            "Epoch 491/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4480 - acc: 0.7830 - val_loss: 0.4970 - val_acc: 0.7812\n",
            "Epoch 492/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4479 - acc: 0.7847 - val_loss: 0.4971 - val_acc: 0.7812\n",
            "Epoch 493/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4478 - acc: 0.7847 - val_loss: 0.4971 - val_acc: 0.7812\n",
            "Epoch 494/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4478 - acc: 0.7830 - val_loss: 0.4972 - val_acc: 0.7812\n",
            "Epoch 495/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4477 - acc: 0.7830 - val_loss: 0.4972 - val_acc: 0.7812\n",
            "Epoch 496/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4476 - acc: 0.7847 - val_loss: 0.4973 - val_acc: 0.7812\n",
            "Epoch 497/1500\n",
            "576/576 [==============================] - 0s 59us/step - loss: 0.4475 - acc: 0.7847 - val_loss: 0.4973 - val_acc: 0.7812\n",
            "Epoch 498/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4474 - acc: 0.7847 - val_loss: 0.4974 - val_acc: 0.7812\n",
            "Epoch 499/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4473 - acc: 0.7847 - val_loss: 0.4974 - val_acc: 0.7812\n",
            "Epoch 500/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4472 - acc: 0.7847 - val_loss: 0.4975 - val_acc: 0.7812\n",
            "Epoch 501/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4471 - acc: 0.7847 - val_loss: 0.4975 - val_acc: 0.7812\n",
            "Epoch 502/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4471 - acc: 0.7847 - val_loss: 0.4976 - val_acc: 0.7760\n",
            "Epoch 503/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4470 - acc: 0.7847 - val_loss: 0.4976 - val_acc: 0.7760\n",
            "Epoch 504/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4469 - acc: 0.7847 - val_loss: 0.4976 - val_acc: 0.7760\n",
            "Epoch 505/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4468 - acc: 0.7847 - val_loss: 0.4977 - val_acc: 0.7760\n",
            "Epoch 506/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4467 - acc: 0.7847 - val_loss: 0.4977 - val_acc: 0.7760\n",
            "Epoch 507/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4466 - acc: 0.7847 - val_loss: 0.4978 - val_acc: 0.7760\n",
            "Epoch 508/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4465 - acc: 0.7847 - val_loss: 0.4979 - val_acc: 0.7760\n",
            "Epoch 509/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4464 - acc: 0.7847 - val_loss: 0.4979 - val_acc: 0.7760\n",
            "Epoch 510/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4464 - acc: 0.7847 - val_loss: 0.4980 - val_acc: 0.7760\n",
            "Epoch 511/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4463 - acc: 0.7847 - val_loss: 0.4981 - val_acc: 0.7760\n",
            "Epoch 512/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4462 - acc: 0.7847 - val_loss: 0.4981 - val_acc: 0.7760\n",
            "Epoch 513/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4461 - acc: 0.7847 - val_loss: 0.4982 - val_acc: 0.7760\n",
            "Epoch 514/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4459 - acc: 0.7847 - val_loss: 0.4982 - val_acc: 0.7760\n",
            "Epoch 515/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4459 - acc: 0.7847 - val_loss: 0.4983 - val_acc: 0.7760\n",
            "Epoch 516/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4458 - acc: 0.7847 - val_loss: 0.4984 - val_acc: 0.7760\n",
            "Epoch 517/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4457 - acc: 0.7847 - val_loss: 0.4984 - val_acc: 0.7760\n",
            "Epoch 518/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4456 - acc: 0.7847 - val_loss: 0.4985 - val_acc: 0.7760\n",
            "Epoch 519/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4456 - acc: 0.7847 - val_loss: 0.4986 - val_acc: 0.7708\n",
            "Epoch 520/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4455 - acc: 0.7847 - val_loss: 0.4986 - val_acc: 0.7656\n",
            "Epoch 521/1500\n",
            "576/576 [==============================] - 0s 62us/step - loss: 0.4454 - acc: 0.7847 - val_loss: 0.4987 - val_acc: 0.7656\n",
            "Epoch 522/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4453 - acc: 0.7847 - val_loss: 0.4988 - val_acc: 0.7656\n",
            "Epoch 523/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4452 - acc: 0.7847 - val_loss: 0.4988 - val_acc: 0.7656\n",
            "Epoch 524/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4451 - acc: 0.7847 - val_loss: 0.4989 - val_acc: 0.7656\n",
            "Epoch 525/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4450 - acc: 0.7847 - val_loss: 0.4990 - val_acc: 0.7656\n",
            "Epoch 526/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4449 - acc: 0.7847 - val_loss: 0.4990 - val_acc: 0.7656\n",
            "Epoch 527/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4448 - acc: 0.7847 - val_loss: 0.4991 - val_acc: 0.7656\n",
            "Epoch 528/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4447 - acc: 0.7847 - val_loss: 0.4991 - val_acc: 0.7656\n",
            "Epoch 529/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4447 - acc: 0.7847 - val_loss: 0.4992 - val_acc: 0.7656\n",
            "Epoch 530/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4446 - acc: 0.7847 - val_loss: 0.4992 - val_acc: 0.7656\n",
            "Epoch 531/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4445 - acc: 0.7847 - val_loss: 0.4993 - val_acc: 0.7656\n",
            "Epoch 532/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4444 - acc: 0.7847 - val_loss: 0.4993 - val_acc: 0.7656\n",
            "Epoch 533/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4443 - acc: 0.7847 - val_loss: 0.4994 - val_acc: 0.7656\n",
            "Epoch 534/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4443 - acc: 0.7865 - val_loss: 0.4995 - val_acc: 0.7656\n",
            "Epoch 535/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4442 - acc: 0.7865 - val_loss: 0.4995 - val_acc: 0.7656\n",
            "Epoch 536/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4441 - acc: 0.7847 - val_loss: 0.4996 - val_acc: 0.7656\n",
            "Epoch 537/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4440 - acc: 0.7865 - val_loss: 0.4996 - val_acc: 0.7656\n",
            "Epoch 538/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4440 - acc: 0.7847 - val_loss: 0.4997 - val_acc: 0.7656\n",
            "Epoch 539/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4439 - acc: 0.7865 - val_loss: 0.4997 - val_acc: 0.7656\n",
            "Epoch 540/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4438 - acc: 0.7865 - val_loss: 0.4998 - val_acc: 0.7656\n",
            "Epoch 541/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4437 - acc: 0.7865 - val_loss: 0.4998 - val_acc: 0.7656\n",
            "Epoch 542/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4436 - acc: 0.7865 - val_loss: 0.4999 - val_acc: 0.7656\n",
            "Epoch 543/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4436 - acc: 0.7882 - val_loss: 0.4999 - val_acc: 0.7656\n",
            "Epoch 544/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4434 - acc: 0.7882 - val_loss: 0.5000 - val_acc: 0.7656\n",
            "Epoch 545/1500\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4434 - acc: 0.7882 - val_loss: 0.5000 - val_acc: 0.7656\n",
            "Epoch 546/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4433 - acc: 0.7865 - val_loss: 0.5001 - val_acc: 0.7656\n",
            "Epoch 547/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4432 - acc: 0.7882 - val_loss: 0.5002 - val_acc: 0.7656\n",
            "Epoch 548/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4431 - acc: 0.7865 - val_loss: 0.5002 - val_acc: 0.7656\n",
            "Epoch 549/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4431 - acc: 0.7882 - val_loss: 0.5003 - val_acc: 0.7656\n",
            "Epoch 550/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4430 - acc: 0.7882 - val_loss: 0.5003 - val_acc: 0.7656\n",
            "Epoch 551/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4429 - acc: 0.7865 - val_loss: 0.5004 - val_acc: 0.7656\n",
            "Epoch 552/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4428 - acc: 0.7865 - val_loss: 0.5004 - val_acc: 0.7656\n",
            "Epoch 553/1500\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.4427 - acc: 0.7882 - val_loss: 0.5005 - val_acc: 0.7656\n",
            "Epoch 554/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4427 - acc: 0.7882 - val_loss: 0.5005 - val_acc: 0.7656\n",
            "Epoch 555/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4426 - acc: 0.7899 - val_loss: 0.5006 - val_acc: 0.7656\n",
            "Epoch 556/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4425 - acc: 0.7882 - val_loss: 0.5006 - val_acc: 0.7656\n",
            "Epoch 557/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4425 - acc: 0.7882 - val_loss: 0.5007 - val_acc: 0.7656\n",
            "Epoch 558/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4424 - acc: 0.7882 - val_loss: 0.5008 - val_acc: 0.7604\n",
            "Epoch 559/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4423 - acc: 0.7899 - val_loss: 0.5008 - val_acc: 0.7604\n",
            "Epoch 560/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4422 - acc: 0.7899 - val_loss: 0.5009 - val_acc: 0.7604\n",
            "Epoch 561/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4421 - acc: 0.7882 - val_loss: 0.5009 - val_acc: 0.7604\n",
            "Epoch 562/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4421 - acc: 0.7882 - val_loss: 0.5010 - val_acc: 0.7604\n",
            "Epoch 563/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4421 - acc: 0.7899 - val_loss: 0.5010 - val_acc: 0.7604\n",
            "Epoch 564/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4419 - acc: 0.7899 - val_loss: 0.5011 - val_acc: 0.7604\n",
            "Epoch 565/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4418 - acc: 0.7917 - val_loss: 0.5012 - val_acc: 0.7604\n",
            "Epoch 566/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4418 - acc: 0.7917 - val_loss: 0.5012 - val_acc: 0.7604\n",
            "Epoch 567/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4417 - acc: 0.7899 - val_loss: 0.5013 - val_acc: 0.7604\n",
            "Epoch 568/1500\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4417 - acc: 0.7899 - val_loss: 0.5013 - val_acc: 0.7604\n",
            "Epoch 569/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4416 - acc: 0.7899 - val_loss: 0.5014 - val_acc: 0.7604\n",
            "Epoch 570/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4415 - acc: 0.7899 - val_loss: 0.5015 - val_acc: 0.7604\n",
            "Epoch 571/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4414 - acc: 0.7917 - val_loss: 0.5015 - val_acc: 0.7604\n",
            "Epoch 572/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4413 - acc: 0.7899 - val_loss: 0.5016 - val_acc: 0.7604\n",
            "Epoch 573/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4413 - acc: 0.7917 - val_loss: 0.5016 - val_acc: 0.7656\n",
            "Epoch 574/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4413 - acc: 0.7917 - val_loss: 0.5017 - val_acc: 0.7656\n",
            "Epoch 575/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4411 - acc: 0.7917 - val_loss: 0.5017 - val_acc: 0.7656\n",
            "Epoch 576/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4411 - acc: 0.7917 - val_loss: 0.5018 - val_acc: 0.7656\n",
            "Epoch 577/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4410 - acc: 0.7917 - val_loss: 0.5019 - val_acc: 0.7656\n",
            "Epoch 578/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4409 - acc: 0.7917 - val_loss: 0.5019 - val_acc: 0.7656\n",
            "Epoch 579/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4409 - acc: 0.7934 - val_loss: 0.5020 - val_acc: 0.7656\n",
            "Epoch 580/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4408 - acc: 0.7934 - val_loss: 0.5020 - val_acc: 0.7656\n",
            "Epoch 581/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4408 - acc: 0.7917 - val_loss: 0.5021 - val_acc: 0.7656\n",
            "Epoch 582/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4407 - acc: 0.7934 - val_loss: 0.5021 - val_acc: 0.7708\n",
            "Epoch 583/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4406 - acc: 0.7917 - val_loss: 0.5022 - val_acc: 0.7708\n",
            "Epoch 584/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4406 - acc: 0.7934 - val_loss: 0.5022 - val_acc: 0.7708\n",
            "Epoch 585/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4405 - acc: 0.7934 - val_loss: 0.5023 - val_acc: 0.7708\n",
            "Epoch 586/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4404 - acc: 0.7917 - val_loss: 0.5024 - val_acc: 0.7708\n",
            "Epoch 587/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4404 - acc: 0.7917 - val_loss: 0.5024 - val_acc: 0.7708\n",
            "Epoch 588/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4403 - acc: 0.7934 - val_loss: 0.5025 - val_acc: 0.7708\n",
            "Epoch 589/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4402 - acc: 0.7917 - val_loss: 0.5025 - val_acc: 0.7708\n",
            "Epoch 590/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4402 - acc: 0.7917 - val_loss: 0.5026 - val_acc: 0.7760\n",
            "Epoch 591/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4401 - acc: 0.7934 - val_loss: 0.5026 - val_acc: 0.7760\n",
            "Epoch 592/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4401 - acc: 0.7917 - val_loss: 0.5027 - val_acc: 0.7760\n",
            "Epoch 593/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4400 - acc: 0.7934 - val_loss: 0.5027 - val_acc: 0.7760\n",
            "Epoch 594/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4400 - acc: 0.7899 - val_loss: 0.5028 - val_acc: 0.7760\n",
            "Epoch 595/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4399 - acc: 0.7934 - val_loss: 0.5029 - val_acc: 0.7760\n",
            "Epoch 596/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4398 - acc: 0.7899 - val_loss: 0.5029 - val_acc: 0.7760\n",
            "Epoch 597/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4397 - acc: 0.7899 - val_loss: 0.5030 - val_acc: 0.7760\n",
            "Epoch 598/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4396 - acc: 0.7899 - val_loss: 0.5030 - val_acc: 0.7760\n",
            "Epoch 599/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4396 - acc: 0.7917 - val_loss: 0.5031 - val_acc: 0.7760\n",
            "Epoch 600/1500\n",
            "576/576 [==============================] - 0s 61us/step - loss: 0.4395 - acc: 0.7917 - val_loss: 0.5031 - val_acc: 0.7760\n",
            "Epoch 601/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4395 - acc: 0.7917 - val_loss: 0.5032 - val_acc: 0.7760\n",
            "Epoch 602/1500\n",
            "576/576 [==============================] - 0s 59us/step - loss: 0.4394 - acc: 0.7917 - val_loss: 0.5032 - val_acc: 0.7760\n",
            "Epoch 603/1500\n",
            "576/576 [==============================] - 0s 62us/step - loss: 0.4393 - acc: 0.7917 - val_loss: 0.5033 - val_acc: 0.7760\n",
            "Epoch 604/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4393 - acc: 0.7934 - val_loss: 0.5033 - val_acc: 0.7760\n",
            "Epoch 605/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4392 - acc: 0.7934 - val_loss: 0.5034 - val_acc: 0.7760\n",
            "Epoch 606/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4391 - acc: 0.7917 - val_loss: 0.5034 - val_acc: 0.7708\n",
            "Epoch 607/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4391 - acc: 0.7951 - val_loss: 0.5035 - val_acc: 0.7708\n",
            "Epoch 608/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4390 - acc: 0.7951 - val_loss: 0.5035 - val_acc: 0.7708\n",
            "Epoch 609/1500\n",
            "576/576 [==============================] - 0s 64us/step - loss: 0.4390 - acc: 0.7951 - val_loss: 0.5036 - val_acc: 0.7708\n",
            "Epoch 610/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4389 - acc: 0.7951 - val_loss: 0.5036 - val_acc: 0.7708\n",
            "Epoch 611/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4388 - acc: 0.7951 - val_loss: 0.5037 - val_acc: 0.7708\n",
            "Epoch 612/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4388 - acc: 0.7951 - val_loss: 0.5037 - val_acc: 0.7708\n",
            "Epoch 613/1500\n",
            "576/576 [==============================] - 0s 63us/step - loss: 0.4387 - acc: 0.7951 - val_loss: 0.5038 - val_acc: 0.7708\n",
            "Epoch 614/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4386 - acc: 0.7934 - val_loss: 0.5039 - val_acc: 0.7708\n",
            "Epoch 615/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4386 - acc: 0.7934 - val_loss: 0.5039 - val_acc: 0.7708\n",
            "Epoch 616/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4385 - acc: 0.7951 - val_loss: 0.5039 - val_acc: 0.7708\n",
            "Epoch 617/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4384 - acc: 0.7951 - val_loss: 0.5040 - val_acc: 0.7708\n",
            "Epoch 618/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4384 - acc: 0.7934 - val_loss: 0.5040 - val_acc: 0.7708\n",
            "Epoch 619/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4383 - acc: 0.7934 - val_loss: 0.5041 - val_acc: 0.7708\n",
            "Epoch 620/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4383 - acc: 0.7951 - val_loss: 0.5041 - val_acc: 0.7708\n",
            "Epoch 621/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4382 - acc: 0.7934 - val_loss: 0.5042 - val_acc: 0.7708\n",
            "Epoch 622/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4381 - acc: 0.7951 - val_loss: 0.5042 - val_acc: 0.7708\n",
            "Epoch 623/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4381 - acc: 0.7934 - val_loss: 0.5043 - val_acc: 0.7708\n",
            "Epoch 624/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4380 - acc: 0.7951 - val_loss: 0.5043 - val_acc: 0.7708\n",
            "Epoch 625/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4379 - acc: 0.7951 - val_loss: 0.5044 - val_acc: 0.7656\n",
            "Epoch 626/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4379 - acc: 0.7951 - val_loss: 0.5044 - val_acc: 0.7656\n",
            "Epoch 627/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4378 - acc: 0.7934 - val_loss: 0.5045 - val_acc: 0.7708\n",
            "Epoch 628/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4378 - acc: 0.7934 - val_loss: 0.5045 - val_acc: 0.7760\n",
            "Epoch 629/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4377 - acc: 0.7951 - val_loss: 0.5046 - val_acc: 0.7760\n",
            "Epoch 630/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4377 - acc: 0.7951 - val_loss: 0.5046 - val_acc: 0.7760\n",
            "Epoch 631/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4376 - acc: 0.7934 - val_loss: 0.5046 - val_acc: 0.7760\n",
            "Epoch 632/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4375 - acc: 0.7969 - val_loss: 0.5047 - val_acc: 0.7760\n",
            "Epoch 633/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4375 - acc: 0.7969 - val_loss: 0.5048 - val_acc: 0.7708\n",
            "Epoch 634/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4374 - acc: 0.7951 - val_loss: 0.5048 - val_acc: 0.7708\n",
            "Epoch 635/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4373 - acc: 0.7969 - val_loss: 0.5049 - val_acc: 0.7708\n",
            "Epoch 636/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4373 - acc: 0.7969 - val_loss: 0.5049 - val_acc: 0.7760\n",
            "Epoch 637/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4372 - acc: 0.7951 - val_loss: 0.5049 - val_acc: 0.7760\n",
            "Epoch 638/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4372 - acc: 0.7951 - val_loss: 0.5050 - val_acc: 0.7760\n",
            "Epoch 639/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4371 - acc: 0.7969 - val_loss: 0.5050 - val_acc: 0.7760\n",
            "Epoch 640/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4371 - acc: 0.7986 - val_loss: 0.5051 - val_acc: 0.7760\n",
            "Epoch 641/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4371 - acc: 0.7969 - val_loss: 0.5051 - val_acc: 0.7760\n",
            "Epoch 642/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4370 - acc: 0.7951 - val_loss: 0.5052 - val_acc: 0.7760\n",
            "Epoch 643/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4369 - acc: 0.7969 - val_loss: 0.5052 - val_acc: 0.7760\n",
            "Epoch 644/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4368 - acc: 0.7969 - val_loss: 0.5053 - val_acc: 0.7760\n",
            "Epoch 645/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4368 - acc: 0.7951 - val_loss: 0.5053 - val_acc: 0.7760\n",
            "Epoch 646/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4367 - acc: 0.7969 - val_loss: 0.5054 - val_acc: 0.7760\n",
            "Epoch 647/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4367 - acc: 0.7986 - val_loss: 0.5054 - val_acc: 0.7760\n",
            "Epoch 648/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4366 - acc: 0.7969 - val_loss: 0.5055 - val_acc: 0.7760\n",
            "Epoch 649/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4366 - acc: 0.7969 - val_loss: 0.5055 - val_acc: 0.7760\n",
            "Epoch 650/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4366 - acc: 0.7969 - val_loss: 0.5056 - val_acc: 0.7760\n",
            "Epoch 651/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4364 - acc: 0.7951 - val_loss: 0.5056 - val_acc: 0.7760\n",
            "Epoch 652/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4365 - acc: 0.7951 - val_loss: 0.5056 - val_acc: 0.7760\n",
            "Epoch 653/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4364 - acc: 0.7951 - val_loss: 0.5057 - val_acc: 0.7760\n",
            "Epoch 654/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4363 - acc: 0.7934 - val_loss: 0.5057 - val_acc: 0.7760\n",
            "Epoch 655/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4363 - acc: 0.7951 - val_loss: 0.5058 - val_acc: 0.7760\n",
            "Epoch 656/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4362 - acc: 0.7951 - val_loss: 0.5058 - val_acc: 0.7760\n",
            "Epoch 657/1500\n",
            "576/576 [==============================] - 0s 60us/step - loss: 0.4362 - acc: 0.7951 - val_loss: 0.5059 - val_acc: 0.7760\n",
            "Epoch 658/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4361 - acc: 0.7934 - val_loss: 0.5059 - val_acc: 0.7760\n",
            "Epoch 659/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4360 - acc: 0.7951 - val_loss: 0.5060 - val_acc: 0.7760\n",
            "Epoch 660/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4360 - acc: 0.7934 - val_loss: 0.5060 - val_acc: 0.7760\n",
            "Epoch 661/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4360 - acc: 0.7934 - val_loss: 0.5060 - val_acc: 0.7760\n",
            "Epoch 662/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4359 - acc: 0.7934 - val_loss: 0.5061 - val_acc: 0.7760\n",
            "Epoch 663/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4359 - acc: 0.7934 - val_loss: 0.5061 - val_acc: 0.7760\n",
            "Epoch 664/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4358 - acc: 0.7934 - val_loss: 0.5062 - val_acc: 0.7760\n",
            "Epoch 665/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4357 - acc: 0.7934 - val_loss: 0.5062 - val_acc: 0.7760\n",
            "Epoch 666/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4357 - acc: 0.7934 - val_loss: 0.5063 - val_acc: 0.7760\n",
            "Epoch 667/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4357 - acc: 0.7899 - val_loss: 0.5063 - val_acc: 0.7760\n",
            "Epoch 668/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4356 - acc: 0.7934 - val_loss: 0.5063 - val_acc: 0.7760\n",
            "Epoch 669/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4355 - acc: 0.7934 - val_loss: 0.5064 - val_acc: 0.7760\n",
            "Epoch 670/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4355 - acc: 0.7934 - val_loss: 0.5064 - val_acc: 0.7760\n",
            "Epoch 671/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4354 - acc: 0.7934 - val_loss: 0.5065 - val_acc: 0.7760\n",
            "Epoch 672/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4354 - acc: 0.7951 - val_loss: 0.5065 - val_acc: 0.7760\n",
            "Epoch 673/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4353 - acc: 0.7934 - val_loss: 0.5066 - val_acc: 0.7760\n",
            "Epoch 674/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4353 - acc: 0.7934 - val_loss: 0.5066 - val_acc: 0.7760\n",
            "Epoch 675/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4352 - acc: 0.7934 - val_loss: 0.5067 - val_acc: 0.7760\n",
            "Epoch 676/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4352 - acc: 0.7934 - val_loss: 0.5067 - val_acc: 0.7760\n",
            "Epoch 677/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4351 - acc: 0.7934 - val_loss: 0.5068 - val_acc: 0.7760\n",
            "Epoch 678/1500\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4351 - acc: 0.7917 - val_loss: 0.5068 - val_acc: 0.7760\n",
            "Epoch 679/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4350 - acc: 0.7899 - val_loss: 0.5069 - val_acc: 0.7760\n",
            "Epoch 680/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4349 - acc: 0.7899 - val_loss: 0.5069 - val_acc: 0.7760\n",
            "Epoch 681/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4349 - acc: 0.7934 - val_loss: 0.5070 - val_acc: 0.7760\n",
            "Epoch 682/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4348 - acc: 0.7934 - val_loss: 0.5070 - val_acc: 0.7760\n",
            "Epoch 683/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4348 - acc: 0.7917 - val_loss: 0.5070 - val_acc: 0.7760\n",
            "Epoch 684/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4347 - acc: 0.7934 - val_loss: 0.5071 - val_acc: 0.7760\n",
            "Epoch 685/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4347 - acc: 0.7917 - val_loss: 0.5071 - val_acc: 0.7760\n",
            "Epoch 686/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4346 - acc: 0.7951 - val_loss: 0.5072 - val_acc: 0.7760\n",
            "Epoch 687/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4345 - acc: 0.7951 - val_loss: 0.5072 - val_acc: 0.7760\n",
            "Epoch 688/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4345 - acc: 0.7934 - val_loss: 0.5073 - val_acc: 0.7760\n",
            "Epoch 689/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4344 - acc: 0.7934 - val_loss: 0.5073 - val_acc: 0.7760\n",
            "Epoch 690/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4344 - acc: 0.7934 - val_loss: 0.5074 - val_acc: 0.7760\n",
            "Epoch 691/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4343 - acc: 0.7951 - val_loss: 0.5074 - val_acc: 0.7760\n",
            "Epoch 692/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4342 - acc: 0.7917 - val_loss: 0.5074 - val_acc: 0.7760\n",
            "Epoch 693/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4342 - acc: 0.7917 - val_loss: 0.5075 - val_acc: 0.7812\n",
            "Epoch 694/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4341 - acc: 0.7917 - val_loss: 0.5075 - val_acc: 0.7812\n",
            "Epoch 695/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4341 - acc: 0.7934 - val_loss: 0.5076 - val_acc: 0.7812\n",
            "Epoch 696/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4341 - acc: 0.7934 - val_loss: 0.5076 - val_acc: 0.7812\n",
            "Epoch 697/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4340 - acc: 0.7934 - val_loss: 0.5077 - val_acc: 0.7812\n",
            "Epoch 698/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4340 - acc: 0.7899 - val_loss: 0.5077 - val_acc: 0.7812\n",
            "Epoch 699/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4340 - acc: 0.7934 - val_loss: 0.5078 - val_acc: 0.7812\n",
            "Epoch 700/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4339 - acc: 0.7917 - val_loss: 0.5078 - val_acc: 0.7812\n",
            "Epoch 701/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4339 - acc: 0.7917 - val_loss: 0.5078 - val_acc: 0.7812\n",
            "Epoch 702/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4338 - acc: 0.7917 - val_loss: 0.5078 - val_acc: 0.7812\n",
            "Epoch 703/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4337 - acc: 0.7917 - val_loss: 0.5079 - val_acc: 0.7812\n",
            "Epoch 704/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4337 - acc: 0.7899 - val_loss: 0.5079 - val_acc: 0.7812\n",
            "Epoch 705/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4337 - acc: 0.7917 - val_loss: 0.5079 - val_acc: 0.7812\n",
            "Epoch 706/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4336 - acc: 0.7917 - val_loss: 0.5080 - val_acc: 0.7812\n",
            "Epoch 707/1500\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.4335 - acc: 0.7917 - val_loss: 0.5080 - val_acc: 0.7812\n",
            "Epoch 708/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4335 - acc: 0.7917 - val_loss: 0.5081 - val_acc: 0.7812\n",
            "Epoch 709/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4334 - acc: 0.7917 - val_loss: 0.5081 - val_acc: 0.7812\n",
            "Epoch 710/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4334 - acc: 0.7917 - val_loss: 0.5082 - val_acc: 0.7812\n",
            "Epoch 711/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4333 - acc: 0.7917 - val_loss: 0.5082 - val_acc: 0.7812\n",
            "Epoch 712/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4333 - acc: 0.7917 - val_loss: 0.5083 - val_acc: 0.7812\n",
            "Epoch 713/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4332 - acc: 0.7917 - val_loss: 0.5083 - val_acc: 0.7812\n",
            "Epoch 714/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4332 - acc: 0.7934 - val_loss: 0.5083 - val_acc: 0.7812\n",
            "Epoch 715/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4331 - acc: 0.7917 - val_loss: 0.5084 - val_acc: 0.7812\n",
            "Epoch 716/1500\n",
            "576/576 [==============================] - 0s 68us/step - loss: 0.4331 - acc: 0.7934 - val_loss: 0.5084 - val_acc: 0.7812\n",
            "Epoch 717/1500\n",
            "576/576 [==============================] - 0s 64us/step - loss: 0.4330 - acc: 0.7934 - val_loss: 0.5085 - val_acc: 0.7812\n",
            "Epoch 718/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4329 - acc: 0.7934 - val_loss: 0.5085 - val_acc: 0.7812\n",
            "Epoch 719/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4329 - acc: 0.7951 - val_loss: 0.5086 - val_acc: 0.7812\n",
            "Epoch 720/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4329 - acc: 0.7934 - val_loss: 0.5086 - val_acc: 0.7812\n",
            "Epoch 721/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4327 - acc: 0.7917 - val_loss: 0.5087 - val_acc: 0.7812\n",
            "Epoch 722/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4327 - acc: 0.7951 - val_loss: 0.5087 - val_acc: 0.7812\n",
            "Epoch 723/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4327 - acc: 0.7986 - val_loss: 0.5088 - val_acc: 0.7812\n",
            "Epoch 724/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4326 - acc: 0.7986 - val_loss: 0.5088 - val_acc: 0.7812\n",
            "Epoch 725/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4325 - acc: 0.7951 - val_loss: 0.5088 - val_acc: 0.7812\n",
            "Epoch 726/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4325 - acc: 0.7969 - val_loss: 0.5089 - val_acc: 0.7812\n",
            "Epoch 727/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4325 - acc: 0.7986 - val_loss: 0.5089 - val_acc: 0.7812\n",
            "Epoch 728/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4323 - acc: 0.7986 - val_loss: 0.5090 - val_acc: 0.7812\n",
            "Epoch 729/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4324 - acc: 0.7986 - val_loss: 0.5090 - val_acc: 0.7812\n",
            "Epoch 730/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4322 - acc: 0.7986 - val_loss: 0.5090 - val_acc: 0.7812\n",
            "Epoch 731/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4322 - acc: 0.7986 - val_loss: 0.5091 - val_acc: 0.7812\n",
            "Epoch 732/1500\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.4321 - acc: 0.7986 - val_loss: 0.5091 - val_acc: 0.7812\n",
            "Epoch 733/1500\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.4321 - acc: 0.7986 - val_loss: 0.5091 - val_acc: 0.7812\n",
            "Epoch 734/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4321 - acc: 0.7986 - val_loss: 0.5092 - val_acc: 0.7812\n",
            "Epoch 735/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4320 - acc: 0.7986 - val_loss: 0.5092 - val_acc: 0.7812\n",
            "Epoch 736/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4320 - acc: 0.7986 - val_loss: 0.5092 - val_acc: 0.7812\n",
            "Epoch 737/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4319 - acc: 0.7986 - val_loss: 0.5093 - val_acc: 0.7812\n",
            "Epoch 738/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4319 - acc: 0.7986 - val_loss: 0.5093 - val_acc: 0.7812\n",
            "Epoch 739/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4318 - acc: 0.7986 - val_loss: 0.5093 - val_acc: 0.7812\n",
            "Epoch 740/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4318 - acc: 0.7986 - val_loss: 0.5094 - val_acc: 0.7812\n",
            "Epoch 741/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4317 - acc: 0.7986 - val_loss: 0.5094 - val_acc: 0.7812\n",
            "Epoch 742/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4317 - acc: 0.7986 - val_loss: 0.5095 - val_acc: 0.7812\n",
            "Epoch 743/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4316 - acc: 0.7986 - val_loss: 0.5095 - val_acc: 0.7812\n",
            "Epoch 744/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4316 - acc: 0.7969 - val_loss: 0.5095 - val_acc: 0.7812\n",
            "Epoch 745/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4315 - acc: 0.7986 - val_loss: 0.5096 - val_acc: 0.7812\n",
            "Epoch 746/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4315 - acc: 0.7986 - val_loss: 0.5096 - val_acc: 0.7812\n",
            "Epoch 747/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4315 - acc: 0.7969 - val_loss: 0.5096 - val_acc: 0.7812\n",
            "Epoch 748/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4314 - acc: 0.7986 - val_loss: 0.5097 - val_acc: 0.7812\n",
            "Epoch 749/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4314 - acc: 0.7986 - val_loss: 0.5097 - val_acc: 0.7812\n",
            "Epoch 750/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4313 - acc: 0.7986 - val_loss: 0.5097 - val_acc: 0.7812\n",
            "Epoch 751/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4313 - acc: 0.7969 - val_loss: 0.5098 - val_acc: 0.7812\n",
            "Epoch 752/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4312 - acc: 0.7986 - val_loss: 0.5098 - val_acc: 0.7812\n",
            "Epoch 753/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4312 - acc: 0.7969 - val_loss: 0.5098 - val_acc: 0.7812\n",
            "Epoch 754/1500\n",
            "576/576 [==============================] - 0s 59us/step - loss: 0.4311 - acc: 0.7986 - val_loss: 0.5099 - val_acc: 0.7760\n",
            "Epoch 755/1500\n",
            "576/576 [==============================] - 0s 63us/step - loss: 0.4311 - acc: 0.7986 - val_loss: 0.5099 - val_acc: 0.7760\n",
            "Epoch 756/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4311 - acc: 0.7969 - val_loss: 0.5100 - val_acc: 0.7760\n",
            "Epoch 757/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4310 - acc: 0.7986 - val_loss: 0.5100 - val_acc: 0.7760\n",
            "Epoch 758/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4309 - acc: 0.7969 - val_loss: 0.5100 - val_acc: 0.7760\n",
            "Epoch 759/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4309 - acc: 0.7969 - val_loss: 0.5101 - val_acc: 0.7760\n",
            "Epoch 760/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4309 - acc: 0.7969 - val_loss: 0.5101 - val_acc: 0.7760\n",
            "Epoch 761/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4308 - acc: 0.7986 - val_loss: 0.5101 - val_acc: 0.7760\n",
            "Epoch 762/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4307 - acc: 0.7986 - val_loss: 0.5102 - val_acc: 0.7760\n",
            "Epoch 763/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4307 - acc: 0.7986 - val_loss: 0.5102 - val_acc: 0.7760\n",
            "Epoch 764/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4306 - acc: 0.7986 - val_loss: 0.5103 - val_acc: 0.7760\n",
            "Epoch 765/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4306 - acc: 0.7986 - val_loss: 0.5103 - val_acc: 0.7760\n",
            "Epoch 766/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4305 - acc: 0.7986 - val_loss: 0.5104 - val_acc: 0.7760\n",
            "Epoch 767/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4305 - acc: 0.7986 - val_loss: 0.5104 - val_acc: 0.7760\n",
            "Epoch 768/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4304 - acc: 0.7969 - val_loss: 0.5104 - val_acc: 0.7760\n",
            "Epoch 769/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4304 - acc: 0.7969 - val_loss: 0.5105 - val_acc: 0.7760\n",
            "Epoch 770/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4303 - acc: 0.7969 - val_loss: 0.5105 - val_acc: 0.7760\n",
            "Epoch 771/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4303 - acc: 0.7969 - val_loss: 0.5105 - val_acc: 0.7760\n",
            "Epoch 772/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4302 - acc: 0.7969 - val_loss: 0.5106 - val_acc: 0.7760\n",
            "Epoch 773/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4302 - acc: 0.7969 - val_loss: 0.5106 - val_acc: 0.7760\n",
            "Epoch 774/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4301 - acc: 0.7969 - val_loss: 0.5106 - val_acc: 0.7760\n",
            "Epoch 775/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4301 - acc: 0.7969 - val_loss: 0.5107 - val_acc: 0.7760\n",
            "Epoch 776/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4300 - acc: 0.7969 - val_loss: 0.5107 - val_acc: 0.7760\n",
            "Epoch 777/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4299 - acc: 0.7969 - val_loss: 0.5107 - val_acc: 0.7760\n",
            "Epoch 778/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4300 - acc: 0.7969 - val_loss: 0.5108 - val_acc: 0.7760\n",
            "Epoch 779/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4299 - acc: 0.7969 - val_loss: 0.5108 - val_acc: 0.7760\n",
            "Epoch 780/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4298 - acc: 0.7986 - val_loss: 0.5109 - val_acc: 0.7760\n",
            "Epoch 781/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4298 - acc: 0.7969 - val_loss: 0.5109 - val_acc: 0.7760\n",
            "Epoch 782/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4298 - acc: 0.7969 - val_loss: 0.5109 - val_acc: 0.7708\n",
            "Epoch 783/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4297 - acc: 0.7969 - val_loss: 0.5109 - val_acc: 0.7708\n",
            "Epoch 784/1500\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.4296 - acc: 0.7951 - val_loss: 0.5110 - val_acc: 0.7708\n",
            "Epoch 785/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4296 - acc: 0.7951 - val_loss: 0.5110 - val_acc: 0.7708\n",
            "Epoch 786/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4296 - acc: 0.7951 - val_loss: 0.5110 - val_acc: 0.7708\n",
            "Epoch 787/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4296 - acc: 0.7969 - val_loss: 0.5111 - val_acc: 0.7708\n",
            "Epoch 788/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4294 - acc: 0.7969 - val_loss: 0.5111 - val_acc: 0.7708\n",
            "Epoch 789/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4295 - acc: 0.7951 - val_loss: 0.5112 - val_acc: 0.7708\n",
            "Epoch 790/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4294 - acc: 0.7951 - val_loss: 0.5112 - val_acc: 0.7708\n",
            "Epoch 791/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4293 - acc: 0.7951 - val_loss: 0.5112 - val_acc: 0.7708\n",
            "Epoch 792/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4293 - acc: 0.7951 - val_loss: 0.5113 - val_acc: 0.7708\n",
            "Epoch 793/1500\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.4293 - acc: 0.7969 - val_loss: 0.5113 - val_acc: 0.7760\n",
            "Epoch 794/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4292 - acc: 0.7951 - val_loss: 0.5113 - val_acc: 0.7760\n",
            "Epoch 795/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4291 - acc: 0.7951 - val_loss: 0.5114 - val_acc: 0.7760\n",
            "Epoch 796/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4291 - acc: 0.7951 - val_loss: 0.5114 - val_acc: 0.7760\n",
            "Epoch 797/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4291 - acc: 0.7951 - val_loss: 0.5115 - val_acc: 0.7760\n",
            "Epoch 798/1500\n",
            "576/576 [==============================] - 0s 68us/step - loss: 0.4291 - acc: 0.7951 - val_loss: 0.5115 - val_acc: 0.7760\n",
            "Epoch 799/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4290 - acc: 0.7969 - val_loss: 0.5115 - val_acc: 0.7760\n",
            "Epoch 800/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4290 - acc: 0.7969 - val_loss: 0.5116 - val_acc: 0.7760\n",
            "Epoch 801/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4289 - acc: 0.7969 - val_loss: 0.5116 - val_acc: 0.7760\n",
            "Epoch 802/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4289 - acc: 0.7969 - val_loss: 0.5116 - val_acc: 0.7760\n",
            "Epoch 803/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4289 - acc: 0.7969 - val_loss: 0.5117 - val_acc: 0.7760\n",
            "Epoch 804/1500\n",
            "576/576 [==============================] - 0s 59us/step - loss: 0.4288 - acc: 0.7969 - val_loss: 0.5117 - val_acc: 0.7760\n",
            "Epoch 805/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4288 - acc: 0.7969 - val_loss: 0.5118 - val_acc: 0.7760\n",
            "Epoch 806/1500\n",
            "576/576 [==============================] - 0s 59us/step - loss: 0.4287 - acc: 0.7969 - val_loss: 0.5118 - val_acc: 0.7760\n",
            "Epoch 807/1500\n",
            "576/576 [==============================] - 0s 59us/step - loss: 0.4287 - acc: 0.7969 - val_loss: 0.5118 - val_acc: 0.7760\n",
            "Epoch 808/1500\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.4286 - acc: 0.7969 - val_loss: 0.5118 - val_acc: 0.7760\n",
            "Epoch 809/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4286 - acc: 0.7951 - val_loss: 0.5119 - val_acc: 0.7760\n",
            "Epoch 810/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4286 - acc: 0.7951 - val_loss: 0.5119 - val_acc: 0.7760\n",
            "Epoch 811/1500\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4285 - acc: 0.7986 - val_loss: 0.5119 - val_acc: 0.7760\n",
            "Epoch 812/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4285 - acc: 0.7951 - val_loss: 0.5119 - val_acc: 0.7760\n",
            "Epoch 813/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4285 - acc: 0.7951 - val_loss: 0.5120 - val_acc: 0.7760\n",
            "Epoch 814/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4284 - acc: 0.7969 - val_loss: 0.5120 - val_acc: 0.7760\n",
            "Epoch 815/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4284 - acc: 0.7951 - val_loss: 0.5120 - val_acc: 0.7760\n",
            "Epoch 816/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4283 - acc: 0.7951 - val_loss: 0.5121 - val_acc: 0.7760\n",
            "Epoch 817/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4283 - acc: 0.7969 - val_loss: 0.5121 - val_acc: 0.7760\n",
            "Epoch 818/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4282 - acc: 0.7969 - val_loss: 0.5121 - val_acc: 0.7760\n",
            "Epoch 819/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4282 - acc: 0.7951 - val_loss: 0.5122 - val_acc: 0.7760\n",
            "Epoch 820/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4281 - acc: 0.7934 - val_loss: 0.5122 - val_acc: 0.7760\n",
            "Epoch 821/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4281 - acc: 0.7951 - val_loss: 0.5122 - val_acc: 0.7760\n",
            "Epoch 822/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4280 - acc: 0.7969 - val_loss: 0.5123 - val_acc: 0.7760\n",
            "Epoch 823/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4280 - acc: 0.7969 - val_loss: 0.5123 - val_acc: 0.7760\n",
            "Epoch 824/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4281 - acc: 0.7951 - val_loss: 0.5123 - val_acc: 0.7760\n",
            "Epoch 825/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4280 - acc: 0.7951 - val_loss: 0.5124 - val_acc: 0.7760\n",
            "Epoch 826/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4279 - acc: 0.7951 - val_loss: 0.5124 - val_acc: 0.7760\n",
            "Epoch 827/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4279 - acc: 0.7934 - val_loss: 0.5125 - val_acc: 0.7760\n",
            "Epoch 828/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4279 - acc: 0.7969 - val_loss: 0.5125 - val_acc: 0.7760\n",
            "Epoch 829/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4278 - acc: 0.7934 - val_loss: 0.5125 - val_acc: 0.7760\n",
            "Epoch 830/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4278 - acc: 0.7934 - val_loss: 0.5126 - val_acc: 0.7760\n",
            "Epoch 831/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4277 - acc: 0.7951 - val_loss: 0.5126 - val_acc: 0.7760\n",
            "Epoch 832/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4277 - acc: 0.7934 - val_loss: 0.5126 - val_acc: 0.7760\n",
            "Epoch 833/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4277 - acc: 0.7951 - val_loss: 0.5127 - val_acc: 0.7760\n",
            "Epoch 834/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4276 - acc: 0.7934 - val_loss: 0.5127 - val_acc: 0.7760\n",
            "Epoch 835/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4275 - acc: 0.7951 - val_loss: 0.5127 - val_acc: 0.7760\n",
            "Epoch 836/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4275 - acc: 0.7934 - val_loss: 0.5128 - val_acc: 0.7760\n",
            "Epoch 837/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4275 - acc: 0.7934 - val_loss: 0.5128 - val_acc: 0.7708\n",
            "Epoch 838/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4275 - acc: 0.7934 - val_loss: 0.5128 - val_acc: 0.7708\n",
            "Epoch 839/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4274 - acc: 0.7934 - val_loss: 0.5128 - val_acc: 0.7708\n",
            "Epoch 840/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4273 - acc: 0.7934 - val_loss: 0.5129 - val_acc: 0.7708\n",
            "Epoch 841/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4273 - acc: 0.7951 - val_loss: 0.5129 - val_acc: 0.7708\n",
            "Epoch 842/1500\n",
            "576/576 [==============================] - 0s 66us/step - loss: 0.4272 - acc: 0.7951 - val_loss: 0.5129 - val_acc: 0.7708\n",
            "Epoch 843/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4272 - acc: 0.7934 - val_loss: 0.5130 - val_acc: 0.7708\n",
            "Epoch 844/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4272 - acc: 0.7934 - val_loss: 0.5130 - val_acc: 0.7708\n",
            "Epoch 845/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4271 - acc: 0.7934 - val_loss: 0.5130 - val_acc: 0.7708\n",
            "Epoch 846/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4271 - acc: 0.7934 - val_loss: 0.5131 - val_acc: 0.7708\n",
            "Epoch 847/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4271 - acc: 0.7934 - val_loss: 0.5131 - val_acc: 0.7708\n",
            "Epoch 848/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4271 - acc: 0.7934 - val_loss: 0.5131 - val_acc: 0.7708\n",
            "Epoch 849/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4270 - acc: 0.7934 - val_loss: 0.5131 - val_acc: 0.7708\n",
            "Epoch 850/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4269 - acc: 0.7934 - val_loss: 0.5132 - val_acc: 0.7708\n",
            "Epoch 851/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4269 - acc: 0.7951 - val_loss: 0.5132 - val_acc: 0.7708\n",
            "Epoch 852/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4269 - acc: 0.7969 - val_loss: 0.5132 - val_acc: 0.7708\n",
            "Epoch 853/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4269 - acc: 0.7951 - val_loss: 0.5133 - val_acc: 0.7708\n",
            "Epoch 854/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4268 - acc: 0.7951 - val_loss: 0.5133 - val_acc: 0.7708\n",
            "Epoch 855/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4268 - acc: 0.7951 - val_loss: 0.5133 - val_acc: 0.7708\n",
            "Epoch 856/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4267 - acc: 0.7934 - val_loss: 0.5134 - val_acc: 0.7656\n",
            "Epoch 857/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4267 - acc: 0.7951 - val_loss: 0.5134 - val_acc: 0.7656\n",
            "Epoch 858/1500\n",
            "576/576 [==============================] - 0s 63us/step - loss: 0.4266 - acc: 0.7951 - val_loss: 0.5134 - val_acc: 0.7656\n",
            "Epoch 859/1500\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4266 - acc: 0.7951 - val_loss: 0.5134 - val_acc: 0.7656\n",
            "Epoch 860/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4266 - acc: 0.7951 - val_loss: 0.5135 - val_acc: 0.7656\n",
            "Epoch 861/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4266 - acc: 0.7969 - val_loss: 0.5135 - val_acc: 0.7656\n",
            "Epoch 862/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4265 - acc: 0.7969 - val_loss: 0.5135 - val_acc: 0.7656\n",
            "Epoch 863/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4265 - acc: 0.7969 - val_loss: 0.5135 - val_acc: 0.7656\n",
            "Epoch 864/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4265 - acc: 0.7969 - val_loss: 0.5135 - val_acc: 0.7656\n",
            "Epoch 865/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4265 - acc: 0.7986 - val_loss: 0.5136 - val_acc: 0.7656\n",
            "Epoch 866/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4264 - acc: 0.7986 - val_loss: 0.5136 - val_acc: 0.7656\n",
            "Epoch 867/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4264 - acc: 0.7986 - val_loss: 0.5136 - val_acc: 0.7656\n",
            "Epoch 868/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4263 - acc: 0.7969 - val_loss: 0.5137 - val_acc: 0.7656\n",
            "Epoch 869/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4263 - acc: 0.7986 - val_loss: 0.5137 - val_acc: 0.7656\n",
            "Epoch 870/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4263 - acc: 0.7986 - val_loss: 0.5137 - val_acc: 0.7656\n",
            "Epoch 871/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4262 - acc: 0.7969 - val_loss: 0.5137 - val_acc: 0.7656\n",
            "Epoch 872/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4263 - acc: 0.7969 - val_loss: 0.5138 - val_acc: 0.7656\n",
            "Epoch 873/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4262 - acc: 0.7969 - val_loss: 0.5138 - val_acc: 0.7656\n",
            "Epoch 874/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4261 - acc: 0.7986 - val_loss: 0.5138 - val_acc: 0.7656\n",
            "Epoch 875/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4261 - acc: 0.7986 - val_loss: 0.5139 - val_acc: 0.7656\n",
            "Epoch 876/1500\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4261 - acc: 0.7986 - val_loss: 0.5139 - val_acc: 0.7656\n",
            "Epoch 877/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4261 - acc: 0.8003 - val_loss: 0.5140 - val_acc: 0.7656\n",
            "Epoch 878/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4261 - acc: 0.7969 - val_loss: 0.5140 - val_acc: 0.7656\n",
            "Epoch 879/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4260 - acc: 0.8003 - val_loss: 0.5140 - val_acc: 0.7656\n",
            "Epoch 880/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4260 - acc: 0.7986 - val_loss: 0.5140 - val_acc: 0.7656\n",
            "Epoch 881/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4260 - acc: 0.8003 - val_loss: 0.5141 - val_acc: 0.7656\n",
            "Epoch 882/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4259 - acc: 0.7986 - val_loss: 0.5141 - val_acc: 0.7656\n",
            "Epoch 883/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4259 - acc: 0.8003 - val_loss: 0.5141 - val_acc: 0.7656\n",
            "Epoch 884/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4258 - acc: 0.8003 - val_loss: 0.5141 - val_acc: 0.7656\n",
            "Epoch 885/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4258 - acc: 0.8003 - val_loss: 0.5141 - val_acc: 0.7656\n",
            "Epoch 886/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4257 - acc: 0.8003 - val_loss: 0.5142 - val_acc: 0.7604\n",
            "Epoch 887/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4257 - acc: 0.8003 - val_loss: 0.5142 - val_acc: 0.7604\n",
            "Epoch 888/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4257 - acc: 0.8003 - val_loss: 0.5142 - val_acc: 0.7604\n",
            "Epoch 889/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4257 - acc: 0.8003 - val_loss: 0.5143 - val_acc: 0.7604\n",
            "Epoch 890/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4256 - acc: 0.8003 - val_loss: 0.5143 - val_acc: 0.7604\n",
            "Epoch 891/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4256 - acc: 0.8021 - val_loss: 0.5143 - val_acc: 0.7604\n",
            "Epoch 892/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4256 - acc: 0.8003 - val_loss: 0.5143 - val_acc: 0.7604\n",
            "Epoch 893/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4255 - acc: 0.7986 - val_loss: 0.5144 - val_acc: 0.7604\n",
            "Epoch 894/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4255 - acc: 0.8021 - val_loss: 0.5144 - val_acc: 0.7604\n",
            "Epoch 895/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4255 - acc: 0.8021 - val_loss: 0.5144 - val_acc: 0.7604\n",
            "Epoch 896/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4255 - acc: 0.8021 - val_loss: 0.5144 - val_acc: 0.7604\n",
            "Epoch 897/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4255 - acc: 0.8021 - val_loss: 0.5145 - val_acc: 0.7604\n",
            "Epoch 898/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4255 - acc: 0.8003 - val_loss: 0.5145 - val_acc: 0.7656\n",
            "Epoch 899/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4254 - acc: 0.8003 - val_loss: 0.5145 - val_acc: 0.7656\n",
            "Epoch 900/1500\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4254 - acc: 0.8003 - val_loss: 0.5145 - val_acc: 0.7656\n",
            "Epoch 901/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4253 - acc: 0.8003 - val_loss: 0.5145 - val_acc: 0.7656\n",
            "Epoch 902/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4253 - acc: 0.8003 - val_loss: 0.5146 - val_acc: 0.7656\n",
            "Epoch 903/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4253 - acc: 0.8021 - val_loss: 0.5146 - val_acc: 0.7656\n",
            "Epoch 904/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4253 - acc: 0.8021 - val_loss: 0.5146 - val_acc: 0.7656\n",
            "Epoch 905/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4252 - acc: 0.8021 - val_loss: 0.5146 - val_acc: 0.7656\n",
            "Epoch 906/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4252 - acc: 0.8003 - val_loss: 0.5147 - val_acc: 0.7656\n",
            "Epoch 907/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4252 - acc: 0.7986 - val_loss: 0.5147 - val_acc: 0.7656\n",
            "Epoch 908/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4251 - acc: 0.8003 - val_loss: 0.5147 - val_acc: 0.7656\n",
            "Epoch 909/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4251 - acc: 0.8003 - val_loss: 0.5147 - val_acc: 0.7656\n",
            "Epoch 910/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4251 - acc: 0.7986 - val_loss: 0.5148 - val_acc: 0.7656\n",
            "Epoch 911/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4251 - acc: 0.8003 - val_loss: 0.5148 - val_acc: 0.7656\n",
            "Epoch 912/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4250 - acc: 0.7986 - val_loss: 0.5148 - val_acc: 0.7656\n",
            "Epoch 913/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4251 - acc: 0.7986 - val_loss: 0.5148 - val_acc: 0.7656\n",
            "Epoch 914/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4250 - acc: 0.7969 - val_loss: 0.5149 - val_acc: 0.7656\n",
            "Epoch 915/1500\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4250 - acc: 0.8003 - val_loss: 0.5149 - val_acc: 0.7656\n",
            "Epoch 916/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4249 - acc: 0.7986 - val_loss: 0.5149 - val_acc: 0.7656\n",
            "Epoch 917/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4249 - acc: 0.7986 - val_loss: 0.5149 - val_acc: 0.7656\n",
            "Epoch 918/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4248 - acc: 0.7986 - val_loss: 0.5150 - val_acc: 0.7656\n",
            "Epoch 919/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4249 - acc: 0.7986 - val_loss: 0.5150 - val_acc: 0.7656\n",
            "Epoch 920/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4248 - acc: 0.8003 - val_loss: 0.5150 - val_acc: 0.7656\n",
            "Epoch 921/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4248 - acc: 0.7986 - val_loss: 0.5150 - val_acc: 0.7656\n",
            "Epoch 922/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4248 - acc: 0.7986 - val_loss: 0.5150 - val_acc: 0.7656\n",
            "Epoch 923/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4247 - acc: 0.8003 - val_loss: 0.5151 - val_acc: 0.7656\n",
            "Epoch 924/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4247 - acc: 0.8003 - val_loss: 0.5151 - val_acc: 0.7656\n",
            "Epoch 925/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4247 - acc: 0.7986 - val_loss: 0.5151 - val_acc: 0.7656\n",
            "Epoch 926/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4247 - acc: 0.7986 - val_loss: 0.5152 - val_acc: 0.7656\n",
            "Epoch 927/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4246 - acc: 0.8003 - val_loss: 0.5152 - val_acc: 0.7656\n",
            "Epoch 928/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4246 - acc: 0.7986 - val_loss: 0.5152 - val_acc: 0.7656\n",
            "Epoch 929/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4245 - acc: 0.7986 - val_loss: 0.5152 - val_acc: 0.7656\n",
            "Epoch 930/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4245 - acc: 0.7986 - val_loss: 0.5153 - val_acc: 0.7656\n",
            "Epoch 931/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4245 - acc: 0.8003 - val_loss: 0.5153 - val_acc: 0.7656\n",
            "Epoch 932/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4245 - acc: 0.8003 - val_loss: 0.5153 - val_acc: 0.7656\n",
            "Epoch 933/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4245 - acc: 0.8003 - val_loss: 0.5154 - val_acc: 0.7656\n",
            "Epoch 934/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4244 - acc: 0.7986 - val_loss: 0.5154 - val_acc: 0.7656\n",
            "Epoch 935/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4244 - acc: 0.7986 - val_loss: 0.5154 - val_acc: 0.7656\n",
            "Epoch 936/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4243 - acc: 0.7986 - val_loss: 0.5154 - val_acc: 0.7656\n",
            "Epoch 937/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4243 - acc: 0.8003 - val_loss: 0.5155 - val_acc: 0.7656\n",
            "Epoch 938/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4243 - acc: 0.7986 - val_loss: 0.5155 - val_acc: 0.7656\n",
            "Epoch 939/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4243 - acc: 0.8003 - val_loss: 0.5155 - val_acc: 0.7656\n",
            "Epoch 940/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4242 - acc: 0.7986 - val_loss: 0.5156 - val_acc: 0.7656\n",
            "Epoch 941/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4242 - acc: 0.8003 - val_loss: 0.5156 - val_acc: 0.7656\n",
            "Epoch 942/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4242 - acc: 0.8003 - val_loss: 0.5156 - val_acc: 0.7656\n",
            "Epoch 943/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4242 - acc: 0.8003 - val_loss: 0.5156 - val_acc: 0.7656\n",
            "Epoch 944/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4242 - acc: 0.8003 - val_loss: 0.5157 - val_acc: 0.7656\n",
            "Epoch 945/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4242 - acc: 0.7986 - val_loss: 0.5157 - val_acc: 0.7656\n",
            "Epoch 946/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4241 - acc: 0.7986 - val_loss: 0.5157 - val_acc: 0.7656\n",
            "Epoch 947/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4241 - acc: 0.7986 - val_loss: 0.5158 - val_acc: 0.7656\n",
            "Epoch 948/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4241 - acc: 0.7969 - val_loss: 0.5158 - val_acc: 0.7656\n",
            "Epoch 949/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4240 - acc: 0.8003 - val_loss: 0.5158 - val_acc: 0.7656\n",
            "Epoch 950/1500\n",
            "576/576 [==============================] - 0s 74us/step - loss: 0.4240 - acc: 0.7986 - val_loss: 0.5159 - val_acc: 0.7656\n",
            "Epoch 951/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4240 - acc: 0.7986 - val_loss: 0.5159 - val_acc: 0.7656\n",
            "Epoch 952/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4239 - acc: 0.7986 - val_loss: 0.5159 - val_acc: 0.7656\n",
            "Epoch 953/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4239 - acc: 0.7986 - val_loss: 0.5159 - val_acc: 0.7656\n",
            "Epoch 954/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4239 - acc: 0.7986 - val_loss: 0.5160 - val_acc: 0.7656\n",
            "Epoch 955/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4239 - acc: 0.7986 - val_loss: 0.5160 - val_acc: 0.7656\n",
            "Epoch 956/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4238 - acc: 0.7986 - val_loss: 0.5160 - val_acc: 0.7656\n",
            "Epoch 957/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4238 - acc: 0.8003 - val_loss: 0.5161 - val_acc: 0.7656\n",
            "Epoch 958/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4237 - acc: 0.7986 - val_loss: 0.5161 - val_acc: 0.7656\n",
            "Epoch 959/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4238 - acc: 0.7986 - val_loss: 0.5161 - val_acc: 0.7656\n",
            "Epoch 960/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4237 - acc: 0.7986 - val_loss: 0.5161 - val_acc: 0.7656\n",
            "Epoch 961/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4237 - acc: 0.7986 - val_loss: 0.5162 - val_acc: 0.7656\n",
            "Epoch 962/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4237 - acc: 0.8003 - val_loss: 0.5162 - val_acc: 0.7656\n",
            "Epoch 963/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4237 - acc: 0.7969 - val_loss: 0.5162 - val_acc: 0.7656\n",
            "Epoch 964/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4236 - acc: 0.8003 - val_loss: 0.5162 - val_acc: 0.7656\n",
            "Epoch 965/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4236 - acc: 0.7986 - val_loss: 0.5163 - val_acc: 0.7656\n",
            "Epoch 966/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4236 - acc: 0.8003 - val_loss: 0.5163 - val_acc: 0.7656\n",
            "Epoch 967/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4235 - acc: 0.7986 - val_loss: 0.5163 - val_acc: 0.7656\n",
            "Epoch 968/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4235 - acc: 0.7986 - val_loss: 0.5163 - val_acc: 0.7656\n",
            "Epoch 969/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4235 - acc: 0.7986 - val_loss: 0.5163 - val_acc: 0.7656\n",
            "Epoch 970/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4234 - acc: 0.7986 - val_loss: 0.5164 - val_acc: 0.7656\n",
            "Epoch 971/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4234 - acc: 0.7969 - val_loss: 0.5164 - val_acc: 0.7656\n",
            "Epoch 972/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4234 - acc: 0.7986 - val_loss: 0.5164 - val_acc: 0.7656\n",
            "Epoch 973/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4233 - acc: 0.7969 - val_loss: 0.5164 - val_acc: 0.7656\n",
            "Epoch 974/1500\n",
            "576/576 [==============================] - 0s 63us/step - loss: 0.4234 - acc: 0.7986 - val_loss: 0.5165 - val_acc: 0.7656\n",
            "Epoch 975/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4234 - acc: 0.7986 - val_loss: 0.5165 - val_acc: 0.7656\n",
            "Epoch 976/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4233 - acc: 0.7986 - val_loss: 0.5165 - val_acc: 0.7656\n",
            "Epoch 977/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4233 - acc: 0.7986 - val_loss: 0.5166 - val_acc: 0.7656\n",
            "Epoch 978/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4233 - acc: 0.7969 - val_loss: 0.5166 - val_acc: 0.7656\n",
            "Epoch 979/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4232 - acc: 0.7986 - val_loss: 0.5166 - val_acc: 0.7656\n",
            "Epoch 980/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4231 - acc: 0.7986 - val_loss: 0.5166 - val_acc: 0.7656\n",
            "Epoch 981/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4232 - acc: 0.7986 - val_loss: 0.5167 - val_acc: 0.7656\n",
            "Epoch 982/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4231 - acc: 0.7986 - val_loss: 0.5167 - val_acc: 0.7656\n",
            "Epoch 983/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4231 - acc: 0.8003 - val_loss: 0.5167 - val_acc: 0.7656\n",
            "Epoch 984/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4231 - acc: 0.7986 - val_loss: 0.5167 - val_acc: 0.7656\n",
            "Epoch 985/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4231 - acc: 0.7969 - val_loss: 0.5167 - val_acc: 0.7656\n",
            "Epoch 986/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4230 - acc: 0.7969 - val_loss: 0.5168 - val_acc: 0.7656\n",
            "Epoch 987/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4230 - acc: 0.7986 - val_loss: 0.5168 - val_acc: 0.7656\n",
            "Epoch 988/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4230 - acc: 0.7969 - val_loss: 0.5168 - val_acc: 0.7656\n",
            "Epoch 989/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4230 - acc: 0.7969 - val_loss: 0.5168 - val_acc: 0.7656\n",
            "Epoch 990/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4229 - acc: 0.7986 - val_loss: 0.5169 - val_acc: 0.7656\n",
            "Epoch 991/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4229 - acc: 0.8003 - val_loss: 0.5169 - val_acc: 0.7604\n",
            "Epoch 992/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4229 - acc: 0.7986 - val_loss: 0.5169 - val_acc: 0.7656\n",
            "Epoch 993/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4228 - acc: 0.7969 - val_loss: 0.5169 - val_acc: 0.7604\n",
            "Epoch 994/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4228 - acc: 0.7969 - val_loss: 0.5170 - val_acc: 0.7604\n",
            "Epoch 995/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4228 - acc: 0.7969 - val_loss: 0.5170 - val_acc: 0.7604\n",
            "Epoch 996/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4228 - acc: 0.7986 - val_loss: 0.5170 - val_acc: 0.7604\n",
            "Epoch 997/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4227 - acc: 0.7986 - val_loss: 0.5170 - val_acc: 0.7604\n",
            "Epoch 998/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4227 - acc: 0.7986 - val_loss: 0.5171 - val_acc: 0.7604\n",
            "Epoch 999/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4227 - acc: 0.7969 - val_loss: 0.5171 - val_acc: 0.7604\n",
            "Epoch 1000/1500\n",
            "576/576 [==============================] - 0s 67us/step - loss: 0.4227 - acc: 0.7969 - val_loss: 0.5171 - val_acc: 0.7604\n",
            "Epoch 1001/1500\n",
            "576/576 [==============================] - 0s 60us/step - loss: 0.4226 - acc: 0.7951 - val_loss: 0.5171 - val_acc: 0.7604\n",
            "Epoch 1002/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4226 - acc: 0.7986 - val_loss: 0.5172 - val_acc: 0.7604\n",
            "Epoch 1003/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4226 - acc: 0.7969 - val_loss: 0.5172 - val_acc: 0.7604\n",
            "Epoch 1004/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4226 - acc: 0.7969 - val_loss: 0.5172 - val_acc: 0.7604\n",
            "Epoch 1005/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4225 - acc: 0.7951 - val_loss: 0.5172 - val_acc: 0.7604\n",
            "Epoch 1006/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4225 - acc: 0.7969 - val_loss: 0.5172 - val_acc: 0.7604\n",
            "Epoch 1007/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4225 - acc: 0.8003 - val_loss: 0.5173 - val_acc: 0.7604\n",
            "Epoch 1008/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4225 - acc: 0.7951 - val_loss: 0.5173 - val_acc: 0.7604\n",
            "Epoch 1009/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4224 - acc: 0.7951 - val_loss: 0.5173 - val_acc: 0.7604\n",
            "Epoch 1010/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4224 - acc: 0.7986 - val_loss: 0.5173 - val_acc: 0.7604\n",
            "Epoch 1011/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4224 - acc: 0.7951 - val_loss: 0.5173 - val_acc: 0.7604\n",
            "Epoch 1012/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4224 - acc: 0.7951 - val_loss: 0.5174 - val_acc: 0.7604\n",
            "Epoch 1013/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4223 - acc: 0.7986 - val_loss: 0.5174 - val_acc: 0.7604\n",
            "Epoch 1014/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4223 - acc: 0.7986 - val_loss: 0.5174 - val_acc: 0.7604\n",
            "Epoch 1015/1500\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.4224 - acc: 0.7986 - val_loss: 0.5174 - val_acc: 0.7604\n",
            "Epoch 1016/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4223 - acc: 0.7969 - val_loss: 0.5174 - val_acc: 0.7604\n",
            "Epoch 1017/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4222 - acc: 0.7969 - val_loss: 0.5174 - val_acc: 0.7604\n",
            "Epoch 1018/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4222 - acc: 0.7951 - val_loss: 0.5175 - val_acc: 0.7604\n",
            "Epoch 1019/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4222 - acc: 0.7969 - val_loss: 0.5175 - val_acc: 0.7604\n",
            "Epoch 1020/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4222 - acc: 0.7986 - val_loss: 0.5175 - val_acc: 0.7604\n",
            "Epoch 1021/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4221 - acc: 0.7969 - val_loss: 0.5175 - val_acc: 0.7604\n",
            "Epoch 1022/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4221 - acc: 0.7986 - val_loss: 0.5175 - val_acc: 0.7604\n",
            "Epoch 1023/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4221 - acc: 0.7969 - val_loss: 0.5176 - val_acc: 0.7604\n",
            "Epoch 1024/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4221 - acc: 0.8003 - val_loss: 0.5176 - val_acc: 0.7604\n",
            "Epoch 1025/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4221 - acc: 0.7986 - val_loss: 0.5176 - val_acc: 0.7604\n",
            "Epoch 1026/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4220 - acc: 0.7986 - val_loss: 0.5176 - val_acc: 0.7604\n",
            "Epoch 1027/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4220 - acc: 0.7986 - val_loss: 0.5177 - val_acc: 0.7604\n",
            "Epoch 1028/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4220 - acc: 0.7986 - val_loss: 0.5177 - val_acc: 0.7604\n",
            "Epoch 1029/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4220 - acc: 0.7986 - val_loss: 0.5177 - val_acc: 0.7604\n",
            "Epoch 1030/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4219 - acc: 0.7969 - val_loss: 0.5177 - val_acc: 0.7604\n",
            "Epoch 1031/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4219 - acc: 0.7986 - val_loss: 0.5177 - val_acc: 0.7604\n",
            "Epoch 1032/1500\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.4219 - acc: 0.8003 - val_loss: 0.5177 - val_acc: 0.7604\n",
            "Epoch 1033/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4218 - acc: 0.7986 - val_loss: 0.5178 - val_acc: 0.7604\n",
            "Epoch 1034/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4218 - acc: 0.7986 - val_loss: 0.5178 - val_acc: 0.7604\n",
            "Epoch 1035/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4218 - acc: 0.8021 - val_loss: 0.5178 - val_acc: 0.7604\n",
            "Epoch 1036/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4217 - acc: 0.8003 - val_loss: 0.5178 - val_acc: 0.7604\n",
            "Epoch 1037/1500\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4218 - acc: 0.8021 - val_loss: 0.5178 - val_acc: 0.7604\n",
            "Epoch 1038/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4217 - acc: 0.8003 - val_loss: 0.5178 - val_acc: 0.7604\n",
            "Epoch 1039/1500\n",
            "576/576 [==============================] - 0s 59us/step - loss: 0.4217 - acc: 0.7969 - val_loss: 0.5179 - val_acc: 0.7604\n",
            "Epoch 1040/1500\n",
            "576/576 [==============================] - 0s 59us/step - loss: 0.4216 - acc: 0.7986 - val_loss: 0.5179 - val_acc: 0.7604\n",
            "Epoch 1041/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4216 - acc: 0.7986 - val_loss: 0.5179 - val_acc: 0.7604\n",
            "Epoch 1042/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4216 - acc: 0.8003 - val_loss: 0.5179 - val_acc: 0.7604\n",
            "Epoch 1043/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4216 - acc: 0.7986 - val_loss: 0.5180 - val_acc: 0.7604\n",
            "Epoch 1044/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4216 - acc: 0.7969 - val_loss: 0.5180 - val_acc: 0.7604\n",
            "Epoch 1045/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4215 - acc: 0.7986 - val_loss: 0.5181 - val_acc: 0.7604\n",
            "Epoch 1046/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4215 - acc: 0.7969 - val_loss: 0.5181 - val_acc: 0.7604\n",
            "Epoch 1047/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4215 - acc: 0.7986 - val_loss: 0.5181 - val_acc: 0.7604\n",
            "Epoch 1048/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4215 - acc: 0.8003 - val_loss: 0.5181 - val_acc: 0.7604\n",
            "Epoch 1049/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4214 - acc: 0.7986 - val_loss: 0.5181 - val_acc: 0.7604\n",
            "Epoch 1050/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4214 - acc: 0.7986 - val_loss: 0.5182 - val_acc: 0.7604\n",
            "Epoch 1051/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4214 - acc: 0.7986 - val_loss: 0.5182 - val_acc: 0.7604\n",
            "Epoch 1052/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4213 - acc: 0.8003 - val_loss: 0.5182 - val_acc: 0.7604\n",
            "Epoch 1053/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4213 - acc: 0.7986 - val_loss: 0.5182 - val_acc: 0.7604\n",
            "Epoch 1054/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4213 - acc: 0.8003 - val_loss: 0.5183 - val_acc: 0.7604\n",
            "Epoch 1055/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4213 - acc: 0.7986 - val_loss: 0.5183 - val_acc: 0.7604\n",
            "Epoch 1056/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4213 - acc: 0.8021 - val_loss: 0.5183 - val_acc: 0.7604\n",
            "Epoch 1057/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4212 - acc: 0.7986 - val_loss: 0.5183 - val_acc: 0.7604\n",
            "Epoch 1058/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4212 - acc: 0.7986 - val_loss: 0.5184 - val_acc: 0.7604\n",
            "Epoch 1059/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4212 - acc: 0.8003 - val_loss: 0.5184 - val_acc: 0.7604\n",
            "Epoch 1060/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4212 - acc: 0.8003 - val_loss: 0.5184 - val_acc: 0.7604\n",
            "Epoch 1061/1500\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4212 - acc: 0.7969 - val_loss: 0.5185 - val_acc: 0.7604\n",
            "Epoch 1062/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4211 - acc: 0.8003 - val_loss: 0.5185 - val_acc: 0.7604\n",
            "Epoch 1063/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4211 - acc: 0.7986 - val_loss: 0.5185 - val_acc: 0.7604\n",
            "Epoch 1064/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4211 - acc: 0.8003 - val_loss: 0.5185 - val_acc: 0.7604\n",
            "Epoch 1065/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4210 - acc: 0.7986 - val_loss: 0.5186 - val_acc: 0.7604\n",
            "Epoch 1066/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4211 - acc: 0.7986 - val_loss: 0.5186 - val_acc: 0.7604\n",
            "Epoch 1067/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4210 - acc: 0.8003 - val_loss: 0.5186 - val_acc: 0.7604\n",
            "Epoch 1068/1500\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4209 - acc: 0.7986 - val_loss: 0.5186 - val_acc: 0.7604\n",
            "Epoch 1069/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4210 - acc: 0.8003 - val_loss: 0.5186 - val_acc: 0.7604\n",
            "Epoch 1070/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4209 - acc: 0.8003 - val_loss: 0.5187 - val_acc: 0.7604\n",
            "Epoch 1071/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4208 - acc: 0.7986 - val_loss: 0.5187 - val_acc: 0.7604\n",
            "Epoch 1072/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4208 - acc: 0.8003 - val_loss: 0.5187 - val_acc: 0.7604\n",
            "Epoch 1073/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4208 - acc: 0.7986 - val_loss: 0.5188 - val_acc: 0.7604\n",
            "Epoch 1074/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4208 - acc: 0.7986 - val_loss: 0.5188 - val_acc: 0.7604\n",
            "Epoch 1075/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4208 - acc: 0.7986 - val_loss: 0.5188 - val_acc: 0.7604\n",
            "Epoch 1076/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4208 - acc: 0.7986 - val_loss: 0.5188 - val_acc: 0.7604\n",
            "Epoch 1077/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4208 - acc: 0.8003 - val_loss: 0.5189 - val_acc: 0.7604\n",
            "Epoch 1078/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4207 - acc: 0.8003 - val_loss: 0.5189 - val_acc: 0.7604\n",
            "Epoch 1079/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4207 - acc: 0.8021 - val_loss: 0.5189 - val_acc: 0.7604\n",
            "Epoch 1080/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4207 - acc: 0.8003 - val_loss: 0.5190 - val_acc: 0.7604\n",
            "Epoch 1081/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4206 - acc: 0.7986 - val_loss: 0.5190 - val_acc: 0.7604\n",
            "Epoch 1082/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4206 - acc: 0.7986 - val_loss: 0.5190 - val_acc: 0.7604\n",
            "Epoch 1083/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4206 - acc: 0.8003 - val_loss: 0.5191 - val_acc: 0.7604\n",
            "Epoch 1084/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4206 - acc: 0.8003 - val_loss: 0.5191 - val_acc: 0.7604\n",
            "Epoch 1085/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4206 - acc: 0.8003 - val_loss: 0.5191 - val_acc: 0.7604\n",
            "Epoch 1086/1500\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4205 - acc: 0.7986 - val_loss: 0.5191 - val_acc: 0.7604\n",
            "Epoch 1087/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4205 - acc: 0.8021 - val_loss: 0.5191 - val_acc: 0.7604\n",
            "Epoch 1088/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4204 - acc: 0.8003 - val_loss: 0.5192 - val_acc: 0.7604\n",
            "Epoch 1089/1500\n",
            "576/576 [==============================] - 0s 63us/step - loss: 0.4205 - acc: 0.8003 - val_loss: 0.5192 - val_acc: 0.7604\n",
            "Epoch 1090/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4204 - acc: 0.8021 - val_loss: 0.5192 - val_acc: 0.7604\n",
            "Epoch 1091/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4204 - acc: 0.8003 - val_loss: 0.5192 - val_acc: 0.7604\n",
            "Epoch 1092/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4204 - acc: 0.8021 - val_loss: 0.5193 - val_acc: 0.7604\n",
            "Epoch 1093/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4203 - acc: 0.8021 - val_loss: 0.5193 - val_acc: 0.7604\n",
            "Epoch 1094/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4204 - acc: 0.7986 - val_loss: 0.5193 - val_acc: 0.7604\n",
            "Epoch 1095/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4202 - acc: 0.8003 - val_loss: 0.5193 - val_acc: 0.7604\n",
            "Epoch 1096/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4203 - acc: 0.8003 - val_loss: 0.5193 - val_acc: 0.7604\n",
            "Epoch 1097/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4203 - acc: 0.8003 - val_loss: 0.5194 - val_acc: 0.7604\n",
            "Epoch 1098/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4202 - acc: 0.8003 - val_loss: 0.5194 - val_acc: 0.7604\n",
            "Epoch 1099/1500\n",
            "576/576 [==============================] - 0s 59us/step - loss: 0.4202 - acc: 0.8003 - val_loss: 0.5194 - val_acc: 0.7604\n",
            "Epoch 1100/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4201 - acc: 0.7986 - val_loss: 0.5195 - val_acc: 0.7604\n",
            "Epoch 1101/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4202 - acc: 0.8003 - val_loss: 0.5195 - val_acc: 0.7604\n",
            "Epoch 1102/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4202 - acc: 0.8003 - val_loss: 0.5195 - val_acc: 0.7604\n",
            "Epoch 1103/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4201 - acc: 0.8003 - val_loss: 0.5195 - val_acc: 0.7604\n",
            "Epoch 1104/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4201 - acc: 0.8021 - val_loss: 0.5196 - val_acc: 0.7604\n",
            "Epoch 1105/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4200 - acc: 0.8021 - val_loss: 0.5196 - val_acc: 0.7604\n",
            "Epoch 1106/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4200 - acc: 0.8021 - val_loss: 0.5196 - val_acc: 0.7604\n",
            "Epoch 1107/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4200 - acc: 0.8003 - val_loss: 0.5196 - val_acc: 0.7604\n",
            "Epoch 1108/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4199 - acc: 0.8021 - val_loss: 0.5197 - val_acc: 0.7604\n",
            "Epoch 1109/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4199 - acc: 0.8021 - val_loss: 0.5197 - val_acc: 0.7604\n",
            "Epoch 1110/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4199 - acc: 0.8021 - val_loss: 0.5197 - val_acc: 0.7604\n",
            "Epoch 1111/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4199 - acc: 0.8003 - val_loss: 0.5198 - val_acc: 0.7604\n",
            "Epoch 1112/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4199 - acc: 0.7986 - val_loss: 0.5198 - val_acc: 0.7604\n",
            "Epoch 1113/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4198 - acc: 0.7986 - val_loss: 0.5198 - val_acc: 0.7604\n",
            "Epoch 1114/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4197 - acc: 0.8021 - val_loss: 0.5199 - val_acc: 0.7604\n",
            "Epoch 1115/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4198 - acc: 0.7986 - val_loss: 0.5199 - val_acc: 0.7604\n",
            "Epoch 1116/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4197 - acc: 0.8003 - val_loss: 0.5199 - val_acc: 0.7604\n",
            "Epoch 1117/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4197 - acc: 0.7986 - val_loss: 0.5200 - val_acc: 0.7604\n",
            "Epoch 1118/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4197 - acc: 0.7986 - val_loss: 0.5200 - val_acc: 0.7604\n",
            "Epoch 1119/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4197 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7604\n",
            "Epoch 1120/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4196 - acc: 0.8003 - val_loss: 0.5201 - val_acc: 0.7604\n",
            "Epoch 1121/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4196 - acc: 0.8003 - val_loss: 0.5201 - val_acc: 0.7656\n",
            "Epoch 1122/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4196 - acc: 0.8003 - val_loss: 0.5202 - val_acc: 0.7604\n",
            "Epoch 1123/1500\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.4195 - acc: 0.8003 - val_loss: 0.5202 - val_acc: 0.7656\n",
            "Epoch 1124/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4195 - acc: 0.7986 - val_loss: 0.5202 - val_acc: 0.7656\n",
            "Epoch 1125/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4195 - acc: 0.8003 - val_loss: 0.5203 - val_acc: 0.7656\n",
            "Epoch 1126/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4195 - acc: 0.8003 - val_loss: 0.5203 - val_acc: 0.7656\n",
            "Epoch 1127/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4194 - acc: 0.8003 - val_loss: 0.5203 - val_acc: 0.7656\n",
            "Epoch 1128/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4194 - acc: 0.7986 - val_loss: 0.5204 - val_acc: 0.7656\n",
            "Epoch 1129/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4193 - acc: 0.8003 - val_loss: 0.5204 - val_acc: 0.7656\n",
            "Epoch 1130/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4193 - acc: 0.8003 - val_loss: 0.5204 - val_acc: 0.7656\n",
            "Epoch 1131/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4193 - acc: 0.8003 - val_loss: 0.5205 - val_acc: 0.7656\n",
            "Epoch 1132/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4193 - acc: 0.8003 - val_loss: 0.5205 - val_acc: 0.7656\n",
            "Epoch 1133/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4192 - acc: 0.8003 - val_loss: 0.5205 - val_acc: 0.7656\n",
            "Epoch 1134/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4192 - acc: 0.8003 - val_loss: 0.5206 - val_acc: 0.7656\n",
            "Epoch 1135/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4192 - acc: 0.8003 - val_loss: 0.5206 - val_acc: 0.7656\n",
            "Epoch 1136/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4191 - acc: 0.8038 - val_loss: 0.5206 - val_acc: 0.7656\n",
            "Epoch 1137/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4191 - acc: 0.8003 - val_loss: 0.5206 - val_acc: 0.7656\n",
            "Epoch 1138/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4191 - acc: 0.8021 - val_loss: 0.5207 - val_acc: 0.7656\n",
            "Epoch 1139/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4190 - acc: 0.8021 - val_loss: 0.5207 - val_acc: 0.7656\n",
            "Epoch 1140/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4190 - acc: 0.8021 - val_loss: 0.5207 - val_acc: 0.7656\n",
            "Epoch 1141/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4190 - acc: 0.7986 - val_loss: 0.5207 - val_acc: 0.7656\n",
            "Epoch 1142/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4189 - acc: 0.8021 - val_loss: 0.5208 - val_acc: 0.7656\n",
            "Epoch 1143/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4189 - acc: 0.8021 - val_loss: 0.5208 - val_acc: 0.7656\n",
            "Epoch 1144/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4189 - acc: 0.8021 - val_loss: 0.5208 - val_acc: 0.7656\n",
            "Epoch 1145/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4189 - acc: 0.8038 - val_loss: 0.5209 - val_acc: 0.7656\n",
            "Epoch 1146/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4189 - acc: 0.8021 - val_loss: 0.5209 - val_acc: 0.7656\n",
            "Epoch 1147/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4188 - acc: 0.8021 - val_loss: 0.5209 - val_acc: 0.7656\n",
            "Epoch 1148/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4188 - acc: 0.8021 - val_loss: 0.5209 - val_acc: 0.7656\n",
            "Epoch 1149/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4187 - acc: 0.8003 - val_loss: 0.5210 - val_acc: 0.7656\n",
            "Epoch 1150/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4187 - acc: 0.8021 - val_loss: 0.5210 - val_acc: 0.7656\n",
            "Epoch 1151/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4187 - acc: 0.8021 - val_loss: 0.5210 - val_acc: 0.7656\n",
            "Epoch 1152/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4186 - acc: 0.8021 - val_loss: 0.5210 - val_acc: 0.7656\n",
            "Epoch 1153/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4187 - acc: 0.8038 - val_loss: 0.5211 - val_acc: 0.7656\n",
            "Epoch 1154/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4186 - acc: 0.8021 - val_loss: 0.5211 - val_acc: 0.7656\n",
            "Epoch 1155/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4186 - acc: 0.8021 - val_loss: 0.5211 - val_acc: 0.7656\n",
            "Epoch 1156/1500\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4185 - acc: 0.8021 - val_loss: 0.5212 - val_acc: 0.7656\n",
            "Epoch 1157/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4185 - acc: 0.8021 - val_loss: 0.5212 - val_acc: 0.7656\n",
            "Epoch 1158/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4185 - acc: 0.8021 - val_loss: 0.5212 - val_acc: 0.7656\n",
            "Epoch 1159/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4184 - acc: 0.8038 - val_loss: 0.5213 - val_acc: 0.7656\n",
            "Epoch 1160/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4185 - acc: 0.8021 - val_loss: 0.5213 - val_acc: 0.7656\n",
            "Epoch 1161/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4184 - acc: 0.8021 - val_loss: 0.5213 - val_acc: 0.7656\n",
            "Epoch 1162/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4184 - acc: 0.8021 - val_loss: 0.5214 - val_acc: 0.7656\n",
            "Epoch 1163/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4184 - acc: 0.8021 - val_loss: 0.5214 - val_acc: 0.7656\n",
            "Epoch 1164/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4183 - acc: 0.8021 - val_loss: 0.5214 - val_acc: 0.7656\n",
            "Epoch 1165/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4183 - acc: 0.8021 - val_loss: 0.5214 - val_acc: 0.7656\n",
            "Epoch 1166/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4183 - acc: 0.8038 - val_loss: 0.5215 - val_acc: 0.7656\n",
            "Epoch 1167/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4183 - acc: 0.8021 - val_loss: 0.5215 - val_acc: 0.7656\n",
            "Epoch 1168/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4182 - acc: 0.8021 - val_loss: 0.5215 - val_acc: 0.7656\n",
            "Epoch 1169/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4182 - acc: 0.8021 - val_loss: 0.5215 - val_acc: 0.7656\n",
            "Epoch 1170/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4182 - acc: 0.8021 - val_loss: 0.5216 - val_acc: 0.7656\n",
            "Epoch 1171/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4182 - acc: 0.8021 - val_loss: 0.5216 - val_acc: 0.7656\n",
            "Epoch 1172/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4182 - acc: 0.8021 - val_loss: 0.5216 - val_acc: 0.7656\n",
            "Epoch 1173/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4181 - acc: 0.8021 - val_loss: 0.5217 - val_acc: 0.7656\n",
            "Epoch 1174/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4181 - acc: 0.8021 - val_loss: 0.5217 - val_acc: 0.7656\n",
            "Epoch 1175/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4180 - acc: 0.8021 - val_loss: 0.5217 - val_acc: 0.7656\n",
            "Epoch 1176/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4180 - acc: 0.8021 - val_loss: 0.5218 - val_acc: 0.7656\n",
            "Epoch 1177/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4180 - acc: 0.8021 - val_loss: 0.5218 - val_acc: 0.7656\n",
            "Epoch 1178/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4180 - acc: 0.8021 - val_loss: 0.5218 - val_acc: 0.7656\n",
            "Epoch 1179/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4179 - acc: 0.8021 - val_loss: 0.5219 - val_acc: 0.7656\n",
            "Epoch 1180/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4179 - acc: 0.8021 - val_loss: 0.5219 - val_acc: 0.7656\n",
            "Epoch 1181/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4179 - acc: 0.8021 - val_loss: 0.5219 - val_acc: 0.7656\n",
            "Epoch 1182/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4179 - acc: 0.8021 - val_loss: 0.5219 - val_acc: 0.7656\n",
            "Epoch 1183/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4179 - acc: 0.8003 - val_loss: 0.5220 - val_acc: 0.7656\n",
            "Epoch 1184/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4178 - acc: 0.8021 - val_loss: 0.5220 - val_acc: 0.7656\n",
            "Epoch 1185/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4178 - acc: 0.8038 - val_loss: 0.5220 - val_acc: 0.7656\n",
            "Epoch 1186/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4178 - acc: 0.8003 - val_loss: 0.5221 - val_acc: 0.7656\n",
            "Epoch 1187/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4177 - acc: 0.8003 - val_loss: 0.5221 - val_acc: 0.7656\n",
            "Epoch 1188/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4177 - acc: 0.8003 - val_loss: 0.5221 - val_acc: 0.7656\n",
            "Epoch 1189/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4176 - acc: 0.8003 - val_loss: 0.5222 - val_acc: 0.7656\n",
            "Epoch 1190/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4177 - acc: 0.8003 - val_loss: 0.5222 - val_acc: 0.7656\n",
            "Epoch 1191/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4176 - acc: 0.8038 - val_loss: 0.5222 - val_acc: 0.7656\n",
            "Epoch 1192/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4176 - acc: 0.8021 - val_loss: 0.5223 - val_acc: 0.7656\n",
            "Epoch 1193/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4176 - acc: 0.8003 - val_loss: 0.5223 - val_acc: 0.7656\n",
            "Epoch 1194/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4176 - acc: 0.8003 - val_loss: 0.5223 - val_acc: 0.7656\n",
            "Epoch 1195/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4175 - acc: 0.8038 - val_loss: 0.5223 - val_acc: 0.7656\n",
            "Epoch 1196/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4175 - acc: 0.8021 - val_loss: 0.5224 - val_acc: 0.7656\n",
            "Epoch 1197/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4175 - acc: 0.8038 - val_loss: 0.5224 - val_acc: 0.7656\n",
            "Epoch 1198/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4175 - acc: 0.8003 - val_loss: 0.5225 - val_acc: 0.7656\n",
            "Epoch 1199/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4175 - acc: 0.8021 - val_loss: 0.5225 - val_acc: 0.7656\n",
            "Epoch 1200/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4174 - acc: 0.8003 - val_loss: 0.5226 - val_acc: 0.7656\n",
            "Epoch 1201/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4174 - acc: 0.8021 - val_loss: 0.5226 - val_acc: 0.7656\n",
            "Epoch 1202/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4174 - acc: 0.8003 - val_loss: 0.5226 - val_acc: 0.7656\n",
            "Epoch 1203/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4174 - acc: 0.8021 - val_loss: 0.5227 - val_acc: 0.7656\n",
            "Epoch 1204/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4173 - acc: 0.8021 - val_loss: 0.5227 - val_acc: 0.7656\n",
            "Epoch 1205/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4173 - acc: 0.8038 - val_loss: 0.5227 - val_acc: 0.7656\n",
            "Epoch 1206/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4172 - acc: 0.8021 - val_loss: 0.5228 - val_acc: 0.7656\n",
            "Epoch 1207/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4173 - acc: 0.8003 - val_loss: 0.5228 - val_acc: 0.7656\n",
            "Epoch 1208/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4172 - acc: 0.8038 - val_loss: 0.5228 - val_acc: 0.7656\n",
            "Epoch 1209/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4172 - acc: 0.8021 - val_loss: 0.5229 - val_acc: 0.7656\n",
            "Epoch 1210/1500\n",
            "576/576 [==============================] - 0s 62us/step - loss: 0.4172 - acc: 0.8021 - val_loss: 0.5229 - val_acc: 0.7656\n",
            "Epoch 1211/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4172 - acc: 0.8021 - val_loss: 0.5229 - val_acc: 0.7656\n",
            "Epoch 1212/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4172 - acc: 0.8021 - val_loss: 0.5230 - val_acc: 0.7656\n",
            "Epoch 1213/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4171 - acc: 0.8021 - val_loss: 0.5230 - val_acc: 0.7656\n",
            "Epoch 1214/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4171 - acc: 0.8021 - val_loss: 0.5230 - val_acc: 0.7656\n",
            "Epoch 1215/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4171 - acc: 0.8021 - val_loss: 0.5230 - val_acc: 0.7656\n",
            "Epoch 1216/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4170 - acc: 0.8021 - val_loss: 0.5231 - val_acc: 0.7656\n",
            "Epoch 1217/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4170 - acc: 0.8003 - val_loss: 0.5231 - val_acc: 0.7656\n",
            "Epoch 1218/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4170 - acc: 0.8003 - val_loss: 0.5232 - val_acc: 0.7656\n",
            "Epoch 1219/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4169 - acc: 0.8021 - val_loss: 0.5232 - val_acc: 0.7656\n",
            "Epoch 1220/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4169 - acc: 0.8003 - val_loss: 0.5232 - val_acc: 0.7656\n",
            "Epoch 1221/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4169 - acc: 0.8003 - val_loss: 0.5233 - val_acc: 0.7656\n",
            "Epoch 1222/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4168 - acc: 0.8003 - val_loss: 0.5233 - val_acc: 0.7656\n",
            "Epoch 1223/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4168 - acc: 0.8003 - val_loss: 0.5234 - val_acc: 0.7656\n",
            "Epoch 1224/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4168 - acc: 0.8003 - val_loss: 0.5234 - val_acc: 0.7656\n",
            "Epoch 1225/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4168 - acc: 0.8003 - val_loss: 0.5235 - val_acc: 0.7656\n",
            "Epoch 1226/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4167 - acc: 0.8003 - val_loss: 0.5235 - val_acc: 0.7656\n",
            "Epoch 1227/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4168 - acc: 0.8003 - val_loss: 0.5236 - val_acc: 0.7656\n",
            "Epoch 1228/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4168 - acc: 0.8003 - val_loss: 0.5236 - val_acc: 0.7656\n",
            "Epoch 1229/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4167 - acc: 0.8003 - val_loss: 0.5236 - val_acc: 0.7656\n",
            "Epoch 1230/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4167 - acc: 0.8003 - val_loss: 0.5237 - val_acc: 0.7656\n",
            "Epoch 1231/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4166 - acc: 0.8003 - val_loss: 0.5237 - val_acc: 0.7656\n",
            "Epoch 1232/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4166 - acc: 0.8003 - val_loss: 0.5238 - val_acc: 0.7656\n",
            "Epoch 1233/1500\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4166 - acc: 0.8003 - val_loss: 0.5238 - val_acc: 0.7656\n",
            "Epoch 1234/1500\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4165 - acc: 0.8003 - val_loss: 0.5239 - val_acc: 0.7656\n",
            "Epoch 1235/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4166 - acc: 0.8003 - val_loss: 0.5239 - val_acc: 0.7656\n",
            "Epoch 1236/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4165 - acc: 0.8021 - val_loss: 0.5240 - val_acc: 0.7656\n",
            "Epoch 1237/1500\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4165 - acc: 0.8003 - val_loss: 0.5240 - val_acc: 0.7656\n",
            "Epoch 1238/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4165 - acc: 0.8003 - val_loss: 0.5241 - val_acc: 0.7656\n",
            "Epoch 1239/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4165 - acc: 0.8003 - val_loss: 0.5241 - val_acc: 0.7656\n",
            "Epoch 1240/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4165 - acc: 0.8003 - val_loss: 0.5241 - val_acc: 0.7656\n",
            "Epoch 1241/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4165 - acc: 0.8003 - val_loss: 0.5242 - val_acc: 0.7656\n",
            "Epoch 1242/1500\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.4164 - acc: 0.8003 - val_loss: 0.5242 - val_acc: 0.7656\n",
            "Epoch 1243/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4164 - acc: 0.8003 - val_loss: 0.5243 - val_acc: 0.7656\n",
            "Epoch 1244/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4164 - acc: 0.8003 - val_loss: 0.5243 - val_acc: 0.7656\n",
            "Epoch 1245/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4164 - acc: 0.8003 - val_loss: 0.5243 - val_acc: 0.7656\n",
            "Epoch 1246/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4163 - acc: 0.8003 - val_loss: 0.5244 - val_acc: 0.7656\n",
            "Epoch 1247/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4163 - acc: 0.8003 - val_loss: 0.5244 - val_acc: 0.7656\n",
            "Epoch 1248/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4163 - acc: 0.8003 - val_loss: 0.5245 - val_acc: 0.7656\n",
            "Epoch 1249/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4163 - acc: 0.8003 - val_loss: 0.5245 - val_acc: 0.7656\n",
            "Epoch 1250/1500\n",
            "576/576 [==============================] - 0s 60us/step - loss: 0.4163 - acc: 0.8003 - val_loss: 0.5246 - val_acc: 0.7656\n",
            "Epoch 1251/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4162 - acc: 0.8003 - val_loss: 0.5246 - val_acc: 0.7656\n",
            "Epoch 1252/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4161 - acc: 0.8003 - val_loss: 0.5246 - val_acc: 0.7656\n",
            "Epoch 1253/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4162 - acc: 0.8003 - val_loss: 0.5247 - val_acc: 0.7656\n",
            "Epoch 1254/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4161 - acc: 0.8003 - val_loss: 0.5247 - val_acc: 0.7656\n",
            "Epoch 1255/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4161 - acc: 0.8003 - val_loss: 0.5248 - val_acc: 0.7656\n",
            "Epoch 1256/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4161 - acc: 0.7986 - val_loss: 0.5248 - val_acc: 0.7656\n",
            "Epoch 1257/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4161 - acc: 0.8003 - val_loss: 0.5249 - val_acc: 0.7656\n",
            "Epoch 1258/1500\n",
            "576/576 [==============================] - 0s 63us/step - loss: 0.4161 - acc: 0.8003 - val_loss: 0.5249 - val_acc: 0.7656\n",
            "Epoch 1259/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4160 - acc: 0.8003 - val_loss: 0.5250 - val_acc: 0.7656\n",
            "Epoch 1260/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4160 - acc: 0.7986 - val_loss: 0.5250 - val_acc: 0.7656\n",
            "Epoch 1261/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4160 - acc: 0.7986 - val_loss: 0.5251 - val_acc: 0.7656\n",
            "Epoch 1262/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4159 - acc: 0.8003 - val_loss: 0.5251 - val_acc: 0.7656\n",
            "Epoch 1263/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4159 - acc: 0.7986 - val_loss: 0.5252 - val_acc: 0.7656\n",
            "Epoch 1264/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4159 - acc: 0.7986 - val_loss: 0.5252 - val_acc: 0.7656\n",
            "Epoch 1265/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4159 - acc: 0.7986 - val_loss: 0.5253 - val_acc: 0.7656\n",
            "Epoch 1266/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4158 - acc: 0.7986 - val_loss: 0.5253 - val_acc: 0.7656\n",
            "Epoch 1267/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4158 - acc: 0.7986 - val_loss: 0.5254 - val_acc: 0.7656\n",
            "Epoch 1268/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4158 - acc: 0.7986 - val_loss: 0.5254 - val_acc: 0.7656\n",
            "Epoch 1269/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4158 - acc: 0.7986 - val_loss: 0.5255 - val_acc: 0.7656\n",
            "Epoch 1270/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4157 - acc: 0.7969 - val_loss: 0.5255 - val_acc: 0.7656\n",
            "Epoch 1271/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4157 - acc: 0.8003 - val_loss: 0.5256 - val_acc: 0.7656\n",
            "Epoch 1272/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4157 - acc: 0.8038 - val_loss: 0.5256 - val_acc: 0.7656\n",
            "Epoch 1273/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4157 - acc: 0.7986 - val_loss: 0.5256 - val_acc: 0.7656\n",
            "Epoch 1274/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4157 - acc: 0.8021 - val_loss: 0.5257 - val_acc: 0.7656\n",
            "Epoch 1275/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4157 - acc: 0.8021 - val_loss: 0.5257 - val_acc: 0.7656\n",
            "Epoch 1276/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4157 - acc: 0.8003 - val_loss: 0.5258 - val_acc: 0.7656\n",
            "Epoch 1277/1500\n",
            "576/576 [==============================] - 0s 78us/step - loss: 0.4156 - acc: 0.8003 - val_loss: 0.5258 - val_acc: 0.7656\n",
            "Epoch 1278/1500\n",
            "576/576 [==============================] - 0s 59us/step - loss: 0.4156 - acc: 0.8021 - val_loss: 0.5259 - val_acc: 0.7656\n",
            "Epoch 1279/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4156 - acc: 0.8003 - val_loss: 0.5259 - val_acc: 0.7656\n",
            "Epoch 1280/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4155 - acc: 0.8021 - val_loss: 0.5260 - val_acc: 0.7656\n",
            "Epoch 1281/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4155 - acc: 0.8021 - val_loss: 0.5260 - val_acc: 0.7656\n",
            "Epoch 1282/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4155 - acc: 0.8021 - val_loss: 0.5261 - val_acc: 0.7656\n",
            "Epoch 1283/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4155 - acc: 0.8021 - val_loss: 0.5261 - val_acc: 0.7656\n",
            "Epoch 1284/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4155 - acc: 0.8021 - val_loss: 0.5261 - val_acc: 0.7656\n",
            "Epoch 1285/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4155 - acc: 0.8021 - val_loss: 0.5262 - val_acc: 0.7656\n",
            "Epoch 1286/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4154 - acc: 0.8021 - val_loss: 0.5262 - val_acc: 0.7656\n",
            "Epoch 1287/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4154 - acc: 0.8021 - val_loss: 0.5263 - val_acc: 0.7656\n",
            "Epoch 1288/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4153 - acc: 0.8021 - val_loss: 0.5263 - val_acc: 0.7656\n",
            "Epoch 1289/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4154 - acc: 0.8021 - val_loss: 0.5264 - val_acc: 0.7656\n",
            "Epoch 1290/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4153 - acc: 0.8021 - val_loss: 0.5264 - val_acc: 0.7656\n",
            "Epoch 1291/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4153 - acc: 0.8021 - val_loss: 0.5264 - val_acc: 0.7656\n",
            "Epoch 1292/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4152 - acc: 0.8021 - val_loss: 0.5265 - val_acc: 0.7656\n",
            "Epoch 1293/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4153 - acc: 0.8021 - val_loss: 0.5265 - val_acc: 0.7656\n",
            "Epoch 1294/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4152 - acc: 0.8021 - val_loss: 0.5266 - val_acc: 0.7656\n",
            "Epoch 1295/1500\n",
            "576/576 [==============================] - 0s 67us/step - loss: 0.4152 - acc: 0.8021 - val_loss: 0.5266 - val_acc: 0.7656\n",
            "Epoch 1296/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4152 - acc: 0.8021 - val_loss: 0.5267 - val_acc: 0.7656\n",
            "Epoch 1297/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4152 - acc: 0.8038 - val_loss: 0.5267 - val_acc: 0.7656\n",
            "Epoch 1298/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4152 - acc: 0.8021 - val_loss: 0.5268 - val_acc: 0.7656\n",
            "Epoch 1299/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4152 - acc: 0.8021 - val_loss: 0.5268 - val_acc: 0.7656\n",
            "Epoch 1300/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4151 - acc: 0.8038 - val_loss: 0.5269 - val_acc: 0.7656\n",
            "Epoch 1301/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4151 - acc: 0.8021 - val_loss: 0.5269 - val_acc: 0.7656\n",
            "Epoch 1302/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4150 - acc: 0.8021 - val_loss: 0.5269 - val_acc: 0.7656\n",
            "Epoch 1303/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4150 - acc: 0.8021 - val_loss: 0.5270 - val_acc: 0.7656\n",
            "Epoch 1304/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4150 - acc: 0.8021 - val_loss: 0.5271 - val_acc: 0.7656\n",
            "Epoch 1305/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4150 - acc: 0.8021 - val_loss: 0.5271 - val_acc: 0.7656\n",
            "Epoch 1306/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4150 - acc: 0.8003 - val_loss: 0.5272 - val_acc: 0.7656\n",
            "Epoch 1307/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4150 - acc: 0.8021 - val_loss: 0.5272 - val_acc: 0.7656\n",
            "Epoch 1308/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4150 - acc: 0.8021 - val_loss: 0.5273 - val_acc: 0.7656\n",
            "Epoch 1309/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4150 - acc: 0.8021 - val_loss: 0.5273 - val_acc: 0.7656\n",
            "Epoch 1310/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4149 - acc: 0.8021 - val_loss: 0.5274 - val_acc: 0.7656\n",
            "Epoch 1311/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4149 - acc: 0.8038 - val_loss: 0.5274 - val_acc: 0.7656\n",
            "Epoch 1312/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4148 - acc: 0.8021 - val_loss: 0.5275 - val_acc: 0.7656\n",
            "Epoch 1313/1500\n",
            "576/576 [==============================] - 0s 61us/step - loss: 0.4149 - acc: 0.8021 - val_loss: 0.5275 - val_acc: 0.7656\n",
            "Epoch 1314/1500\n",
            "576/576 [==============================] - 0s 64us/step - loss: 0.4148 - acc: 0.8021 - val_loss: 0.5276 - val_acc: 0.7656\n",
            "Epoch 1315/1500\n",
            "576/576 [==============================] - 0s 60us/step - loss: 0.4148 - acc: 0.8021 - val_loss: 0.5276 - val_acc: 0.7656\n",
            "Epoch 1316/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4148 - acc: 0.8021 - val_loss: 0.5276 - val_acc: 0.7656\n",
            "Epoch 1317/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4148 - acc: 0.8021 - val_loss: 0.5277 - val_acc: 0.7656\n",
            "Epoch 1318/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4147 - acc: 0.8021 - val_loss: 0.5277 - val_acc: 0.7656\n",
            "Epoch 1319/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4148 - acc: 0.8021 - val_loss: 0.5277 - val_acc: 0.7656\n",
            "Epoch 1320/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4148 - acc: 0.8021 - val_loss: 0.5278 - val_acc: 0.7656\n",
            "Epoch 1321/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4147 - acc: 0.8021 - val_loss: 0.5278 - val_acc: 0.7656\n",
            "Epoch 1322/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4147 - acc: 0.8021 - val_loss: 0.5279 - val_acc: 0.7656\n",
            "Epoch 1323/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4146 - acc: 0.8021 - val_loss: 0.5279 - val_acc: 0.7656\n",
            "Epoch 1324/1500\n",
            "576/576 [==============================] - 0s 60us/step - loss: 0.4147 - acc: 0.8021 - val_loss: 0.5279 - val_acc: 0.7656\n",
            "Epoch 1325/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4146 - acc: 0.8021 - val_loss: 0.5280 - val_acc: 0.7656\n",
            "Epoch 1326/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4146 - acc: 0.8021 - val_loss: 0.5280 - val_acc: 0.7656\n",
            "Epoch 1327/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4146 - acc: 0.8021 - val_loss: 0.5281 - val_acc: 0.7656\n",
            "Epoch 1328/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4145 - acc: 0.8021 - val_loss: 0.5281 - val_acc: 0.7656\n",
            "Epoch 1329/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4145 - acc: 0.8021 - val_loss: 0.5281 - val_acc: 0.7656\n",
            "Epoch 1330/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4146 - acc: 0.8021 - val_loss: 0.5282 - val_acc: 0.7656\n",
            "Epoch 1331/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4145 - acc: 0.8021 - val_loss: 0.5282 - val_acc: 0.7656\n",
            "Epoch 1332/1500\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4145 - acc: 0.8021 - val_loss: 0.5282 - val_acc: 0.7656\n",
            "Epoch 1333/1500\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.4144 - acc: 0.8021 - val_loss: 0.5283 - val_acc: 0.7656\n",
            "Epoch 1334/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4144 - acc: 0.8021 - val_loss: 0.5283 - val_acc: 0.7656\n",
            "Epoch 1335/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4145 - acc: 0.8003 - val_loss: 0.5283 - val_acc: 0.7656\n",
            "Epoch 1336/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4144 - acc: 0.8038 - val_loss: 0.5284 - val_acc: 0.7656\n",
            "Epoch 1337/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4144 - acc: 0.8021 - val_loss: 0.5284 - val_acc: 0.7656\n",
            "Epoch 1338/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4144 - acc: 0.8021 - val_loss: 0.5284 - val_acc: 0.7656\n",
            "Epoch 1339/1500\n",
            "576/576 [==============================] - 0s 59us/step - loss: 0.4144 - acc: 0.8021 - val_loss: 0.5285 - val_acc: 0.7656\n",
            "Epoch 1340/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4143 - acc: 0.8021 - val_loss: 0.5285 - val_acc: 0.7604\n",
            "Epoch 1341/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4143 - acc: 0.8021 - val_loss: 0.5286 - val_acc: 0.7604\n",
            "Epoch 1342/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4143 - acc: 0.8038 - val_loss: 0.5286 - val_acc: 0.7604\n",
            "Epoch 1343/1500\n",
            "576/576 [==============================] - 0s 66us/step - loss: 0.4143 - acc: 0.8038 - val_loss: 0.5287 - val_acc: 0.7604\n",
            "Epoch 1344/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4142 - acc: 0.8038 - val_loss: 0.5287 - val_acc: 0.7604\n",
            "Epoch 1345/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4142 - acc: 0.8038 - val_loss: 0.5288 - val_acc: 0.7604\n",
            "Epoch 1346/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4142 - acc: 0.8021 - val_loss: 0.5288 - val_acc: 0.7604\n",
            "Epoch 1347/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4141 - acc: 0.8038 - val_loss: 0.5288 - val_acc: 0.7604\n",
            "Epoch 1348/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4141 - acc: 0.8038 - val_loss: 0.5289 - val_acc: 0.7604\n",
            "Epoch 1349/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4142 - acc: 0.8038 - val_loss: 0.5289 - val_acc: 0.7604\n",
            "Epoch 1350/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4141 - acc: 0.8021 - val_loss: 0.5290 - val_acc: 0.7604\n",
            "Epoch 1351/1500\n",
            "576/576 [==============================] - 0s 61us/step - loss: 0.4141 - acc: 0.8038 - val_loss: 0.5290 - val_acc: 0.7604\n",
            "Epoch 1352/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4141 - acc: 0.8038 - val_loss: 0.5290 - val_acc: 0.7604\n",
            "Epoch 1353/1500\n",
            "576/576 [==============================] - 0s 59us/step - loss: 0.4140 - acc: 0.8038 - val_loss: 0.5291 - val_acc: 0.7604\n",
            "Epoch 1354/1500\n",
            "576/576 [==============================] - 0s 59us/step - loss: 0.4141 - acc: 0.8038 - val_loss: 0.5291 - val_acc: 0.7604\n",
            "Epoch 1355/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4140 - acc: 0.8038 - val_loss: 0.5292 - val_acc: 0.7604\n",
            "Epoch 1356/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4140 - acc: 0.8038 - val_loss: 0.5292 - val_acc: 0.7604\n",
            "Epoch 1357/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4140 - acc: 0.8038 - val_loss: 0.5292 - val_acc: 0.7604\n",
            "Epoch 1358/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4140 - acc: 0.8038 - val_loss: 0.5293 - val_acc: 0.7604\n",
            "Epoch 1359/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4140 - acc: 0.8038 - val_loss: 0.5293 - val_acc: 0.7604\n",
            "Epoch 1360/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4139 - acc: 0.8038 - val_loss: 0.5294 - val_acc: 0.7604\n",
            "Epoch 1361/1500\n",
            "576/576 [==============================] - 0s 62us/step - loss: 0.4139 - acc: 0.8038 - val_loss: 0.5294 - val_acc: 0.7604\n",
            "Epoch 1362/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4139 - acc: 0.8038 - val_loss: 0.5294 - val_acc: 0.7604\n",
            "Epoch 1363/1500\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.4139 - acc: 0.8021 - val_loss: 0.5294 - val_acc: 0.7604\n",
            "Epoch 1364/1500\n",
            "576/576 [==============================] - 0s 61us/step - loss: 0.4138 - acc: 0.8038 - val_loss: 0.5295 - val_acc: 0.7604\n",
            "Epoch 1365/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4139 - acc: 0.8021 - val_loss: 0.5295 - val_acc: 0.7604\n",
            "Epoch 1366/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4138 - acc: 0.8038 - val_loss: 0.5296 - val_acc: 0.7604\n",
            "Epoch 1367/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4138 - acc: 0.8038 - val_loss: 0.5296 - val_acc: 0.7604\n",
            "Epoch 1368/1500\n",
            "576/576 [==============================] - 0s 60us/step - loss: 0.4137 - acc: 0.8038 - val_loss: 0.5296 - val_acc: 0.7604\n",
            "Epoch 1369/1500\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.4137 - acc: 0.8038 - val_loss: 0.5297 - val_acc: 0.7604\n",
            "Epoch 1370/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4137 - acc: 0.8038 - val_loss: 0.5297 - val_acc: 0.7604\n",
            "Epoch 1371/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4137 - acc: 0.8021 - val_loss: 0.5298 - val_acc: 0.7604\n",
            "Epoch 1372/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4137 - acc: 0.8038 - val_loss: 0.5298 - val_acc: 0.7604\n",
            "Epoch 1373/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4138 - acc: 0.8021 - val_loss: 0.5298 - val_acc: 0.7604\n",
            "Epoch 1374/1500\n",
            "576/576 [==============================] - 0s 62us/step - loss: 0.4137 - acc: 0.8021 - val_loss: 0.5298 - val_acc: 0.7604\n",
            "Epoch 1375/1500\n",
            "576/576 [==============================] - 0s 60us/step - loss: 0.4137 - acc: 0.8038 - val_loss: 0.5299 - val_acc: 0.7604\n",
            "Epoch 1376/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4137 - acc: 0.8038 - val_loss: 0.5299 - val_acc: 0.7604\n",
            "Epoch 1377/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4136 - acc: 0.8038 - val_loss: 0.5300 - val_acc: 0.7604\n",
            "Epoch 1378/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4136 - acc: 0.8038 - val_loss: 0.5300 - val_acc: 0.7604\n",
            "Epoch 1379/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4136 - acc: 0.8038 - val_loss: 0.5301 - val_acc: 0.7604\n",
            "Epoch 1380/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4135 - acc: 0.8021 - val_loss: 0.5301 - val_acc: 0.7604\n",
            "Epoch 1381/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4136 - acc: 0.8021 - val_loss: 0.5301 - val_acc: 0.7604\n",
            "Epoch 1382/1500\n",
            "576/576 [==============================] - 0s 62us/step - loss: 0.4135 - acc: 0.8038 - val_loss: 0.5301 - val_acc: 0.7656\n",
            "Epoch 1383/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4135 - acc: 0.8021 - val_loss: 0.5302 - val_acc: 0.7656\n",
            "Epoch 1384/1500\n",
            "576/576 [==============================] - 0s 63us/step - loss: 0.4135 - acc: 0.8038 - val_loss: 0.5302 - val_acc: 0.7656\n",
            "Epoch 1385/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4135 - acc: 0.8038 - val_loss: 0.5302 - val_acc: 0.7656\n",
            "Epoch 1386/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4134 - acc: 0.8021 - val_loss: 0.5303 - val_acc: 0.7656\n",
            "Epoch 1387/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4135 - acc: 0.8038 - val_loss: 0.5303 - val_acc: 0.7656\n",
            "Epoch 1388/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4134 - acc: 0.8021 - val_loss: 0.5303 - val_acc: 0.7656\n",
            "Epoch 1389/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4134 - acc: 0.8003 - val_loss: 0.5304 - val_acc: 0.7656\n",
            "Epoch 1390/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4134 - acc: 0.8038 - val_loss: 0.5304 - val_acc: 0.7656\n",
            "Epoch 1391/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4134 - acc: 0.8038 - val_loss: 0.5304 - val_acc: 0.7656\n",
            "Epoch 1392/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4134 - acc: 0.8038 - val_loss: 0.5305 - val_acc: 0.7656\n",
            "Epoch 1393/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4133 - acc: 0.8038 - val_loss: 0.5305 - val_acc: 0.7656\n",
            "Epoch 1394/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4134 - acc: 0.8038 - val_loss: 0.5306 - val_acc: 0.7656\n",
            "Epoch 1395/1500\n",
            "576/576 [==============================] - 0s 59us/step - loss: 0.4133 - acc: 0.8021 - val_loss: 0.5306 - val_acc: 0.7656\n",
            "Epoch 1396/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4132 - acc: 0.8038 - val_loss: 0.5306 - val_acc: 0.7656\n",
            "Epoch 1397/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4132 - acc: 0.8038 - val_loss: 0.5307 - val_acc: 0.7656\n",
            "Epoch 1398/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4132 - acc: 0.8021 - val_loss: 0.5307 - val_acc: 0.7656\n",
            "Epoch 1399/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4132 - acc: 0.8038 - val_loss: 0.5307 - val_acc: 0.7656\n",
            "Epoch 1400/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4131 - acc: 0.8038 - val_loss: 0.5307 - val_acc: 0.7656\n",
            "Epoch 1401/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4133 - acc: 0.8021 - val_loss: 0.5308 - val_acc: 0.7656\n",
            "Epoch 1402/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4131 - acc: 0.8021 - val_loss: 0.5308 - val_acc: 0.7656\n",
            "Epoch 1403/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4131 - acc: 0.8038 - val_loss: 0.5309 - val_acc: 0.7656\n",
            "Epoch 1404/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4131 - acc: 0.8038 - val_loss: 0.5309 - val_acc: 0.7656\n",
            "Epoch 1405/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4131 - acc: 0.8038 - val_loss: 0.5309 - val_acc: 0.7656\n",
            "Epoch 1406/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4131 - acc: 0.8003 - val_loss: 0.5309 - val_acc: 0.7656\n",
            "Epoch 1407/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4130 - acc: 0.8021 - val_loss: 0.5310 - val_acc: 0.7656\n",
            "Epoch 1408/1500\n",
            "576/576 [==============================] - 0s 60us/step - loss: 0.4130 - acc: 0.8021 - val_loss: 0.5310 - val_acc: 0.7656\n",
            "Epoch 1409/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4130 - acc: 0.8021 - val_loss: 0.5310 - val_acc: 0.7656\n",
            "Epoch 1410/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4130 - acc: 0.8038 - val_loss: 0.5311 - val_acc: 0.7656\n",
            "Epoch 1411/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4129 - acc: 0.8021 - val_loss: 0.5311 - val_acc: 0.7656\n",
            "Epoch 1412/1500\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4129 - acc: 0.8021 - val_loss: 0.5311 - val_acc: 0.7656\n",
            "Epoch 1413/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4129 - acc: 0.8003 - val_loss: 0.5312 - val_acc: 0.7656\n",
            "Epoch 1414/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4129 - acc: 0.8003 - val_loss: 0.5312 - val_acc: 0.7656\n",
            "Epoch 1415/1500\n",
            "576/576 [==============================] - 0s 58us/step - loss: 0.4129 - acc: 0.8021 - val_loss: 0.5312 - val_acc: 0.7656\n",
            "Epoch 1416/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4129 - acc: 0.8021 - val_loss: 0.5313 - val_acc: 0.7656\n",
            "Epoch 1417/1500\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4129 - acc: 0.8003 - val_loss: 0.5313 - val_acc: 0.7656\n",
            "Epoch 1418/1500\n",
            "576/576 [==============================] - 0s 67us/step - loss: 0.4128 - acc: 0.8038 - val_loss: 0.5313 - val_acc: 0.7656\n",
            "Epoch 1419/1500\n",
            "576/576 [==============================] - 0s 74us/step - loss: 0.4128 - acc: 0.8038 - val_loss: 0.5313 - val_acc: 0.7656\n",
            "Epoch 1420/1500\n",
            "576/576 [==============================] - 0s 60us/step - loss: 0.4128 - acc: 0.8038 - val_loss: 0.5314 - val_acc: 0.7656\n",
            "Epoch 1421/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4128 - acc: 0.8021 - val_loss: 0.5314 - val_acc: 0.7656\n",
            "Epoch 1422/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4127 - acc: 0.8021 - val_loss: 0.5315 - val_acc: 0.7656\n",
            "Epoch 1423/1500\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.4127 - acc: 0.8021 - val_loss: 0.5315 - val_acc: 0.7656\n",
            "Epoch 1424/1500\n",
            "576/576 [==============================] - 0s 54us/step - loss: 0.4127 - acc: 0.8003 - val_loss: 0.5315 - val_acc: 0.7656\n",
            "Epoch 1425/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4126 - acc: 0.8021 - val_loss: 0.5316 - val_acc: 0.7656\n",
            "Epoch 1426/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4126 - acc: 0.8038 - val_loss: 0.5316 - val_acc: 0.7656\n",
            "Epoch 1427/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4126 - acc: 0.8021 - val_loss: 0.5316 - val_acc: 0.7656\n",
            "Epoch 1428/1500\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4126 - acc: 0.8038 - val_loss: 0.5317 - val_acc: 0.7656\n",
            "Epoch 1429/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4126 - acc: 0.8021 - val_loss: 0.5317 - val_acc: 0.7656\n",
            "Epoch 1430/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4126 - acc: 0.8003 - val_loss: 0.5318 - val_acc: 0.7656\n",
            "Epoch 1431/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4126 - acc: 0.8021 - val_loss: 0.5318 - val_acc: 0.7656\n",
            "Epoch 1432/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4125 - acc: 0.8003 - val_loss: 0.5318 - val_acc: 0.7656\n",
            "Epoch 1433/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4125 - acc: 0.8021 - val_loss: 0.5319 - val_acc: 0.7656\n",
            "Epoch 1434/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4124 - acc: 0.8021 - val_loss: 0.5319 - val_acc: 0.7656\n",
            "Epoch 1435/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4125 - acc: 0.8038 - val_loss: 0.5319 - val_acc: 0.7656\n",
            "Epoch 1436/1500\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.4124 - acc: 0.8003 - val_loss: 0.5319 - val_acc: 0.7656\n",
            "Epoch 1437/1500\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.4124 - acc: 0.8021 - val_loss: 0.5320 - val_acc: 0.7656\n",
            "Epoch 1438/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4124 - acc: 0.8056 - val_loss: 0.5320 - val_acc: 0.7656\n",
            "Epoch 1439/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4124 - acc: 0.8003 - val_loss: 0.5320 - val_acc: 0.7656\n",
            "Epoch 1440/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4123 - acc: 0.8021 - val_loss: 0.5321 - val_acc: 0.7656\n",
            "Epoch 1441/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4123 - acc: 0.8038 - val_loss: 0.5321 - val_acc: 0.7656\n",
            "Epoch 1442/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4123 - acc: 0.8038 - val_loss: 0.5321 - val_acc: 0.7656\n",
            "Epoch 1443/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4123 - acc: 0.7986 - val_loss: 0.5322 - val_acc: 0.7656\n",
            "Epoch 1444/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4123 - acc: 0.8003 - val_loss: 0.5322 - val_acc: 0.7656\n",
            "Epoch 1445/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4122 - acc: 0.8038 - val_loss: 0.5323 - val_acc: 0.7656\n",
            "Epoch 1446/1500\n",
            "576/576 [==============================] - 0s 62us/step - loss: 0.4122 - acc: 0.8038 - val_loss: 0.5323 - val_acc: 0.7656\n",
            "Epoch 1447/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4122 - acc: 0.8021 - val_loss: 0.5323 - val_acc: 0.7656\n",
            "Epoch 1448/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4122 - acc: 0.8038 - val_loss: 0.5323 - val_acc: 0.7656\n",
            "Epoch 1449/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4122 - acc: 0.8021 - val_loss: 0.5324 - val_acc: 0.7656\n",
            "Epoch 1450/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4121 - acc: 0.8021 - val_loss: 0.5324 - val_acc: 0.7656\n",
            "Epoch 1451/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4121 - acc: 0.8003 - val_loss: 0.5325 - val_acc: 0.7656\n",
            "Epoch 1452/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4121 - acc: 0.8038 - val_loss: 0.5325 - val_acc: 0.7656\n",
            "Epoch 1453/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4120 - acc: 0.8021 - val_loss: 0.5325 - val_acc: 0.7656\n",
            "Epoch 1454/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4120 - acc: 0.8021 - val_loss: 0.5325 - val_acc: 0.7656\n",
            "Epoch 1455/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4120 - acc: 0.8038 - val_loss: 0.5326 - val_acc: 0.7604\n",
            "Epoch 1456/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4120 - acc: 0.8038 - val_loss: 0.5326 - val_acc: 0.7604\n",
            "Epoch 1457/1500\n",
            "576/576 [==============================] - 0s 52us/step - loss: 0.4120 - acc: 0.8021 - val_loss: 0.5326 - val_acc: 0.7604\n",
            "Epoch 1458/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4120 - acc: 0.8021 - val_loss: 0.5327 - val_acc: 0.7604\n",
            "Epoch 1459/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4120 - acc: 0.8021 - val_loss: 0.5327 - val_acc: 0.7604\n",
            "Epoch 1460/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4119 - acc: 0.8038 - val_loss: 0.5327 - val_acc: 0.7604\n",
            "Epoch 1461/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4118 - acc: 0.8021 - val_loss: 0.5328 - val_acc: 0.7604\n",
            "Epoch 1462/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4119 - acc: 0.8021 - val_loss: 0.5328 - val_acc: 0.7604\n",
            "Epoch 1463/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4118 - acc: 0.8038 - val_loss: 0.5328 - val_acc: 0.7604\n",
            "Epoch 1464/1500\n",
            "576/576 [==============================] - 0s 45us/step - loss: 0.4118 - acc: 0.8021 - val_loss: 0.5329 - val_acc: 0.7604\n",
            "Epoch 1465/1500\n",
            "576/576 [==============================] - 0s 51us/step - loss: 0.4118 - acc: 0.8038 - val_loss: 0.5329 - val_acc: 0.7604\n",
            "Epoch 1466/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4117 - acc: 0.8038 - val_loss: 0.5330 - val_acc: 0.7604\n",
            "Epoch 1467/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4117 - acc: 0.8021 - val_loss: 0.5330 - val_acc: 0.7604\n",
            "Epoch 1468/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4118 - acc: 0.8003 - val_loss: 0.5330 - val_acc: 0.7604\n",
            "Epoch 1469/1500\n",
            "576/576 [==============================] - 0s 47us/step - loss: 0.4117 - acc: 0.8003 - val_loss: 0.5330 - val_acc: 0.7604\n",
            "Epoch 1470/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4117 - acc: 0.8056 - val_loss: 0.5331 - val_acc: 0.7604\n",
            "Epoch 1471/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4117 - acc: 0.8038 - val_loss: 0.5331 - val_acc: 0.7604\n",
            "Epoch 1472/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4116 - acc: 0.8056 - val_loss: 0.5332 - val_acc: 0.7604\n",
            "Epoch 1473/1500\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4116 - acc: 0.8038 - val_loss: 0.5332 - val_acc: 0.7604\n",
            "Epoch 1474/1500\n",
            "576/576 [==============================] - 0s 56us/step - loss: 0.4116 - acc: 0.8021 - val_loss: 0.5332 - val_acc: 0.7604\n",
            "Epoch 1475/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4116 - acc: 0.8038 - val_loss: 0.5333 - val_acc: 0.7604\n",
            "Epoch 1476/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4115 - acc: 0.8021 - val_loss: 0.5333 - val_acc: 0.7604\n",
            "Epoch 1477/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4115 - acc: 0.8038 - val_loss: 0.5333 - val_acc: 0.7604\n",
            "Epoch 1478/1500\n",
            "576/576 [==============================] - 0s 46us/step - loss: 0.4115 - acc: 0.8038 - val_loss: 0.5334 - val_acc: 0.7604\n",
            "Epoch 1479/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4115 - acc: 0.8021 - val_loss: 0.5334 - val_acc: 0.7604\n",
            "Epoch 1480/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4115 - acc: 0.8021 - val_loss: 0.5334 - val_acc: 0.7604\n",
            "Epoch 1481/1500\n",
            "576/576 [==============================] - 0s 43us/step - loss: 0.4114 - acc: 0.8038 - val_loss: 0.5334 - val_acc: 0.7604\n",
            "Epoch 1482/1500\n",
            "576/576 [==============================] - 0s 48us/step - loss: 0.4114 - acc: 0.8038 - val_loss: 0.5335 - val_acc: 0.7604\n",
            "Epoch 1483/1500\n",
            "576/576 [==============================] - 0s 53us/step - loss: 0.4113 - acc: 0.8021 - val_loss: 0.5335 - val_acc: 0.7604\n",
            "Epoch 1484/1500\n",
            "576/576 [==============================] - 0s 59us/step - loss: 0.4113 - acc: 0.8038 - val_loss: 0.5335 - val_acc: 0.7604\n",
            "Epoch 1485/1500\n",
            "576/576 [==============================] - 0s 57us/step - loss: 0.4113 - acc: 0.8038 - val_loss: 0.5336 - val_acc: 0.7604\n",
            "Epoch 1486/1500\n",
            "576/576 [==============================] - 0s 50us/step - loss: 0.4113 - acc: 0.8056 - val_loss: 0.5336 - val_acc: 0.7604\n",
            "Epoch 1487/1500\n",
            "576/576 [==============================] - 0s 49us/step - loss: 0.4113 - acc: 0.8038 - val_loss: 0.5336 - val_acc: 0.7604\n",
            "Epoch 1488/1500\n",
            "576/576 [==============================] - 0s 60us/step - loss: 0.4112 - acc: 0.8038 - val_loss: 0.5336 - val_acc: 0.7604\n",
            "Epoch 1489/1500\n",
            "576/576 [==============================] - 0s 59us/step - loss: 0.4113 - acc: 0.8038 - val_loss: 0.5337 - val_acc: 0.7604\n",
            "Epoch 1490/1500\n",
            "576/576 [==============================] - 0s 44us/step - loss: 0.4112 - acc: 0.8056 - val_loss: 0.5337 - val_acc: 0.7604\n",
            "Epoch 1491/1500\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4112 - acc: 0.8021 - val_loss: 0.5337 - val_acc: 0.7604\n",
            "Epoch 1492/1500\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4112 - acc: 0.8021 - val_loss: 0.5338 - val_acc: 0.7604\n",
            "Epoch 1493/1500\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4112 - acc: 0.8021 - val_loss: 0.5338 - val_acc: 0.7604\n",
            "Epoch 1494/1500\n",
            "576/576 [==============================] - 0s 55us/step - loss: 0.4112 - acc: 0.8056 - val_loss: 0.5338 - val_acc: 0.7604\n",
            "Epoch 1495/1500\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4111 - acc: 0.8021 - val_loss: 0.5339 - val_acc: 0.7604\n",
            "Epoch 1496/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4111 - acc: 0.8056 - val_loss: 0.5339 - val_acc: 0.7604\n",
            "Epoch 1497/1500\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4111 - acc: 0.8038 - val_loss: 0.5339 - val_acc: 0.7604\n",
            "Epoch 1498/1500\n",
            "576/576 [==============================] - 0s 42us/step - loss: 0.4111 - acc: 0.8056 - val_loss: 0.5340 - val_acc: 0.7604\n",
            "Epoch 1499/1500\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4110 - acc: 0.8038 - val_loss: 0.5340 - val_acc: 0.7604\n",
            "Epoch 1500/1500\n",
            "576/576 [==============================] - 0s 41us/step - loss: 0.4110 - acc: 0.8038 - val_loss: 0.5340 - val_acc: 0.7604\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAswAAAF1CAYAAAD8/Lw6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 864x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "accuracy is 0.760\n",
            "roc-auc is 0.805\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAHiCAYAAADSwATnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOIuIiKQYFWcREZEUo+IsIiKSYv4/+O1yvcAPC9sAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}